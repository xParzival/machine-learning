# S04-随机森林
集成学习的另一个类别是bagging方法，而bagging方法中运用最广泛的就是随机森林算法
## bagging算法
![bagging](https://s1.ax1x.com/2020/07/11/UQty4I.png)
如图所示，bagging方法和boosting方法不一样，它的基学习器是并行的，彼此之间没有直接联系。如果每个基学习器都按相同的方式训练，得到的结果是一样的，没有意义，因此bagging采用了随机采样的方式，即每个基学习器都从训练样本中随机采样一定比例的样本进行训练。bagging采用的是有放回采样，假设一个有$N$个样本的训练数据，如果对其进行随机采样，那么在一次采样中每个样本被采集到的概率为${1\over N}$，那么一轮采样中某样本不被采集到的概率为$(1-{1\over N})^N$，而$\lim\limits_{N\to\infty}(1-{1\over N})^N={1\over e}\approx0.368$。因此在训练数据足够多时，每轮采样大约有三分之一的数据没有被采样，这些数据被称为袋外数据(out of bag)
### 组合策略
bagging对所有基分类器的组合策略也比较简单，对于分类问题，通常使用简单投票法，得到最多票数的类别或者类别之一为最终的模型输出。对于回归问题，通常使用简单平均法，对T个弱学习器得到的回归结果进行算术平均得到最终的模型输出
### 袋外估计
因为每轮采样都会有袋外数据，这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。可以用这些袋外数据对最终得到的集成学习器进行验证，计算袋外误差率，这种方法称为袋外估计(OOB estimate)
袋外误差率计算方法：
1. 对每个样本，把以它作为oob样本的基学习器组合起来当作其集成学习器，得到预测结果
2. 计算在该预测结果下的样本误差率即为袋外误差率

数学上可以证明袋外估计是误差的无偏估计，也就是说模型在生成的过程中就可以对误差建立一个无偏估计，因此使用袋外估计使得bagging没有必要进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计，它可以直接在模型内部进行模型效果的评估，这也是bagging方法的优势之一
例如对于一个二分类问题，训练数据为$D$，设bagging方法产生了$T$个基学习器$f_t(x)$，其中每个基学习器分别是在采样到的子样本$D_t$上训练得到的，根据袋外误差率的计算方法
1. 样本$(x,y)$在以其为oob样本的基学习器构成的集成学习器上预测分类
   $$
   \hat{y}=\arg\max_{Y\in\{0,1\}}\sum_{t=1}^TI(Y=f_t(x))I(x\notin D_t)
   $$
2. 计算袋外误差率
   $$
   E_{oob}=\frac{\sum\limits_{D}I(\hat{y}=y)}{|D|}
   $$

### 总结
bagging对于弱学习器没有限制，这和Adaboost一样，但是最常用的一般也是决策树和神经网络
由于Bagging算法每次都进行采样来训练模型，因此泛化能力很强，对于降低模型的方差很有作用。但是同样因为采样的缘故，模型的偏差也会大一些
## 随机森林
随机森林是对传统bagging方法的改进。随机森林的基学习器为决策树，是多颗决策树的组合，因此得名“森林“；随机森林不仅在训练每棵树时对样本有采样，而且特征也只选择一部分而不是全部特征，这也导致随机森林可以得到特征重要性指标
### 特征选择
随机森林的一大特点就是随机选择特征进行基决策树的训练。特征的随机抽样是指，在建每棵树进行每个节点分裂的时候，都随机抽一部分特征，然后在这随机抽的部分特征里面按决策树的分裂条件选择最好的特征。随机选取样本首先可以降低过拟合，其次减少训练的计算复杂度和耗时
假设总共$d$个特征，从中随机选取了$k$维特征，显然参数$k$控制了随机性的引入程度，$k$越大基学习器的性能越好，但是基学习器之间的独立性会大打折扣；$k$越小基学习器之间的相关性会降低，但是基学习器的性能也会下降。在特征特别多的时候一般随机选取$k=\lceil\log_2d\rceil$或者$k=\lceil\log_2d+1\rceil$个特征，特征比较少时可以按一定比例选择特征
随机森林中进行特征重要性(Variable importance measures)的评估思想为：判断每个特征在随机森林中的每颗树上做了多大的贡献，然后取个平均值，最后比一比特征之间的贡献大小。其中关于贡献的计算方式可以是基尼指数或袋外数据错误率
1. 基尼指数：训练基决策树时，比较在利用某个特征分裂结点后基尼指数的变化量，变化大的特征重要度越高。这个方法仅适用于分类问题
   设特征集合为$D$，第$t$颗树的结点集合为$M_t$，随机选择的特征集合为$D_t$，第$m$个结点对应一个特征$d$，结点分裂前后基尼指数变化即为该特征的重要度
   $$
   \text{V}_{td}=\text{Gini}_m(t)-\text{Gini}_{ml}(t)-\text{Gini}_{mr}(t)
   $$
   因为总共有$T$颗树，在每棵树都对该特征$d$计算特征重要度，如果某棵树没有使用该特征则不算在内，因此该特征的重要度为
   $$
   \text{V}_d=\sum_{t=1}^T\text{V}_{td}I(d\in D_t)
   $$
   一般要对特征重要度进行归一化
   $$
   \text{VIM}_d=\frac{V_d}{\sum\limits_{d=1}^D\text{V}_d}
   $$
2. 基于袋外数据：对于第$t$棵树，计算单棵树的袋外误差率$E(t)$；然后对于这棵树用到的某个特征$d$加入随机噪声，计算加入噪声后的袋外误差率$E_d(t)$，显然如果加入噪声后袋外误差率变得很大，则该特征的重要度很大，则特征$d$的特征重要度可表示为
   $$
   \text{VIM}_d=\frac{1}{T}\sum_{t=1}^T[E_d(t)-E(t)]I(d\in D_t)
   $$

从理论上来讲方法二更合理，但是计算复杂度比方法一更高
### 随机森林总结
随机森林相比普通bagging算法做了一些改进，进一步提升了泛化能力
对于随机森林，由于其基决策树并不进行后剪枝，因此单棵树的决策边界会倾向于复杂，叠加起来会更复杂，但是当树的数目达到一定程度，决策边界会越平滑。但是树的数目增加也会导致过拟合风险增加，因此选择合适的树的数目很重要
优点：
1. 可并行：训练可以高度并行化，对于大数据时代的大样本训练速度有优势
2. 高维有效：由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型
3. 特征重要度：在训练后，可以给出各个特征对于输出的重要性
4. 泛化能力：由于采用了随机采样，训练出的模型的方差小，泛化能力强
5. 实现简单：从原理看到随机森林实现非常简单，而且其基决策树一般不剪枝
6. 缺失不敏感：有缺失值也可以得到较好的效果
7. 评估简单：在生成过程中，能够获取到内部生成误差的一种无偏估计

缺点：
1. 噪声影响：随机森林已经被证明在某些噪音较大的分类或者回归问题上会过拟合
2. 不可解释：随机森林是一个黑盒子，无法控制模型内部的运行。只能在不同的参数和随机种子之间进行尝试
3. 随机性：可能有很多相似的决策树，掩盖了真实的结果
4. 单一性：因为随机森林不能给出连续的输出值，因此随机森林主要用于分类，在回归问题上的表现并不好