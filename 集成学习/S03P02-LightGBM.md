# LightGBM
LightGBM由微软提出，主要用于解决GDBT在海量数据中遇到的问题，以便其可以更好更快地用于工业实践中。相比于XGBoost具有训练速度快、内存占用低的特点
根据[ApacheCN中文文档](https://github.com/apachecn/lightgbm-doc-zh)所述，它是分布式的, 高效的, 它具有以下优势:
* 速度和内存使用的优化
  * 减少分割增益的计算量
  * 通过直方图的相减来进行进一步的加速
  * 减少内存的使用 减少并行学习的通信代价

* 稀疏优化
* 准确率优化
  * Leaf-wise (Best-first) 的决策树生长策略
  * 类别特征值的最优分割

* 网络通信的优化
* 并行学习的优化
  * 特征并行
  * 数据并行
  * 投票并行

* GPU 支持可处理大规模数据

## 单边梯度采样
对于XGBoost而言，在构建回归树中最优特征分裂，分裂点查找时需要全量的样本参与增益的计算，而LightGBM针对这一点，提出了利用单边梯度采样算法(Gradient-Based One-Side Sampling, GOSS)来进行样本采用，不使用全量样本进行信息增益的计算,达到减少样本个数目的
观察GBDT的梯度更新公式
$$
f_t(x)=f_{t-1}(x)-\eta\nabla L(y,f_{t-1}(x))
$$
可以看到梯度小的样本目标函数几乎不更新，于是我们可以用梯度信息用来衡量样本是否被充分学习，对样本的梯度进行排序，大梯度的样本保留，小梯度的样本随机抽样，减少的训练样本，加速模型训练，但是在抽样之后会产生新的问题，需要额外的应对策略
### 小梯度加权补偿
直接根据样本的梯度进行抽样，会改变训练集中的标签的分布，为了不改变分布需要做额外的加权补偿
具体方法如下：
1. 在训练过程中，根据损失函数计算得到样本的一阶梯度信息，对其进行降序排列
2. 选择$a\times100\%$的大梯度样本，再从剩余样本中随机选择$b\times100\%$个小梯度样本
3. 为了维持样本标签分布不变，需要对小梯度样本进行加权补偿，即
   $$
   \begin{gathered}
   N=N\times a+N\times b\times w\\
   \Rightarrow w=\frac{1-a}{b}
   \end{gathered}
   $$
   在计算小梯度样本的结构分数增益时，其损失要乘以这个补偿

使用GOSS可以减少大量只具有小梯度的数据实例，这样在计算信息增益的时候只利用剩下的具有高梯度的数据就可以了，相比XGBoost遍历所有特征值节省了不少时间和空间上的开销
#### 结构分数增益的计算方式
以GBDT为例，回归树结点$R$分裂后结构分数增益计算公式可表示为
$$
\text{Gain}=\frac{1}{|R|}\left[\frac{(\sum_{R_l}g_{ti})^2}{|R_l|}+\frac{(\sum_{R_r}g_{ti})^2}{|R_r|}\right]
$$
运用GOSS，设集合$A$为$a\times100\%$的大梯度样本，集合$B$为剩余样本中随机选择的$b\times100\%$个小梯度样本，那么增益就变为
$$
\text{Gain}=\frac{1}{|A\cup B|}\left[\frac{\left(\sum_{A_l}g_{ti}+\cfrac{1-a}{b}\sum_{B_l}g_{ti}\right)^2}{|A_l\cup B_l|}+\frac{\left(\sum_{A_r}g_{ti}+\cfrac{1-a}{b}\sum_{B_r}g_{ti}\right)^2}{|A_r\cup B_r|}\right]
$$
如果是XGBoost，就是以当前模型与真实值的误差作为排序依据，增益中误差小的样本一阶导数和二阶导数都要乘以加权补偿
### 直方图算法
直方图算法的基本思想是：先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点
具体步骤：
1. 直方图优化算法需要在训练前预先将特征值转化为bin value，也就是对每个特征的取值做个分段函数，将所有样本在该特征上划分到某一bin中，最终把特征值从连续值转化成了离散值。特征值对应的bin value在整个训练过程中是不会改变的
2. 将特征离散化后，在训练树时就要构造直方图，每个bin统计结点上对应bin的样本一阶导数之和、二阶导数之和、样本数量
3. 在训练树时，把每个bin的切分点当作切分点，计算分裂增益。此时因为有直方图，计算十分简单

#### 直方图做差加速
对树的一个叶结点进行分裂时，在计算出该叶结点本身直方图以及分裂后一个子结点的直方图后，显然另一个子结点的直方图不需要重新计算，只需要用分裂前的直方图与已经计算出的子结点直方图做差即可，这样直接提升了一倍运算速度
实际上lightGBM会先计算分裂后较小的子结点直方图，然后做差得到较大的子结点直方图，这样进一步提升了运算效率
#### 直方图算法优点总结
1. 内存占用小：直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，极大减小了内存消耗
2. 计算代价更小：预排序算法XGBoost每遍历一个特征值就需要计算一次分裂的增益，而直方图算法LightGBM只需要计算k次，直接将时间复杂度从$O(N\times d)$降低到$O(k\times d)$(d为特征数目)。同时直方图做差加速进一步节省了时间
3. 直接处理类别特征：对于类别特征，不需要进行离散化，可以直接将类别特征值作为bin进行直方图计算。这样同样避免了其他很多算法需要对离散特征进行one-hot等编码的过程，提升了处理效率

### 互斥特征捆绑
高维度的数据往往是稀疏的，这种稀疏性启发我们设计一种无损的方法来减少特征的维度，就是将某些特征合并在一起，称为绑定(bundle)，减少了特征个数。lightGBM的特征绑定算法为Exclusive Feature Bundling(EFB)
通常将互斥特征绑定在一起
* 互斥特征：即在高维稀疏的特征空间中，可能存在多个特征的取值不会同时为非零值，如果取值同时为非零了，那么就说特征之间发生了冲突

比如在风控场景中，不同客群能够获取的信息种类是不一样的，比如对于高净值的客户，他们可提供的信息可能包含征信信息，公积金，房产信息等金融数据，而对于小镇青年，有可能他们无社保信息，无银行流水，但是他们活跃在各大短视频应用中，留下了丰富的行为数据，如果我们构建了同时包含金融属性特征，与行为特征的训练集，那么可以认为这些特征很可能发生互斥，将这一的特征进行合并，可以减少特征的稀疏程度，特征似乎也更具有信息量（虽然特征的含义发生了变化）
另外一个例子，也是风控场景中，会设计不同时间切片的特征，如最近1天，最近3天，最近7天被其他家机构查询的次数，一般来说时间越近，特征越稀疏，如果多个不同类型的最近N天的信息进行合并，也可以大大提高特征的信息量
显然特征绑定要解决两个问题：
1. 哪些特征应该绑在一起
2. 怎么把特征绑在一起

#### 绑定条件
互斥特征的查找就是把数据集中的特征分成几组互斥的特征，这个问题类似于图着色问题。将特征视为无向图中的顶点，将不是相互独立的特征用无向边连接起来，边的权重就是两个相连接的特征的总冲突值，这样需要绑定的特征就是在图着色问题中要涂上同一种颜色的那些点（特征）
此外，通常有很多特征，尽管不是完全相互排斥，但也很少同时取非零值。如果我们的算法可以允许一小部分的冲突，我们可以得到更少的特征包，进一步提高计算效率。一般设定一个最大冲突比率$\gamma$，用来平衡精度和效率
具体解决步骤如下：
1. 构造一个加权无向图，顶点是特征，边有权重，其权重与两个特征间冲突相关，完全互斥或满足最大冲突比率$\gamma$的特征不相连
2. 将特征按无向图的度降序排序，即按相邻的特征数目排序。度越大表示与其他特征的冲突越大
3. 检查排序之后的每个特征，将它与已有的特征捆绑绑定，如果无法绑定则建立新的绑定

这个算法的在维度不是特别多的时候效率可以接受，但是对高维情况耗时特别长。因此lightGBM还提供了一种更加高效的无图的排序策略：如果特征的非零值个数少，那么发生冲突的概率也就小，于是将特征按照非零值个数排序，这和使用图节点的度排序相似，新算法在以前的算法基础上改变了排序策略
#### 特征合并算法
为了降低训练复杂度，需要在同一个捆绑中，将特征进行整合，这个整合算法名为Merge Exclusive Features。算法的关键是确保原始特征能从特征捆绑中区分出来。由于基于直方图的算法将特征值以离散箱的形式存储而不是连续值存储，因此可以利用不同箱中的排他特征来构建特征捆绑。通过增加偏移估计，可以实现的这样的特征捆绑，达到减少特征的目的
例如两个0-1特征，一个特征为是否是男，另一个特征为是否是女，显然可以捆绑在一起，合并为一个特征。但是直接合并为一个特征就不能区分开男和女了，因此对是否是女这个特征加一个偏置1，于是就可以构成1代表男，2代表女的新特征
#### 特征捆绑优点总结
EFB算法可以把很多特征绑到一起，形成更少的稠密特征束，这样可以避免对0特征值的无用的计算。加速计算直方图还可以用一个表记录数据的非0值
### 决策树生长策略
lightGBM对决策树的生长策略进行了优化
#### level-wise策略
xgboost等算法的基决策树采用的都是level-wise的生长策略
该策略遍历一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，实际上很多叶子的分裂增益较低，没必要进行搜索和分裂，因此带来了很多没必要的计算开销
![level-wise](https://s1.ax1x.com/2020/07/30/anGx0S.png)

#### leaf-wise策略
lightGBM使用了leaf-wise树生长策略
从level-wise的分析中发现，很多叶结点的对目标函数的贡献本身就很小，对这些结点还进行分裂是很不划算的，因此leaf-wise采用了一种更简单的方式，每一层只分裂对目标函数贡献最大的叶结点，也就是每一层只分裂一个结点
该策略每次从当前两个叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，Leaf-wise的优点是：在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度；Leaf-wise的缺点是：可能会长出比较深的决策树，产生过拟合。因此LightGBM会在Leaf-wise之上增加一个最大深度的限制，在保证高效率的同时防止过拟合
![leaf-wise](https://s1.ax1x.com/2020/07/30/andKhD.png)

以上都是lightGBM在算法层面的优化，下面介绍lightGBM在工程上的优化
### lightGBM的工程优化
#### 直接支持类别特征
这在直方图算法中可以清晰的看出
#### 支持高效并行
1. 特征并行
   特征并行的主要思想是不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。XGBoost使用的就是这种特征并行方法
   * 传统特征并行
     1. 垂直划分数据（不同的机器有不同的特征集）
     2. 在每个机器本地特征集寻找最佳划分点{特征, 阈值}
     3. 进行各个划分的通信整合并得到最佳划分
     4. 以最佳划分方法对对应机器的数据进行划分，并将数据划分结果传递给其他机器
     5. 其他机器对接受到的数据进一步划分，最后整合

   这种特征并行方法有个很大的缺点：对数据进行垂直划分，每台机器所含数据不同，然后使用不同机器找到不同特征的最优分裂点，划分结果需要通过通信告知每台机器，增加了额外的复杂度
   LightGBM则不进行数据垂直划分，每台机器都有训练集完整数据，在得到最佳划分方案后可在本地执行划分，从而减少了不必要的通信
   * lightBGM的特征并行
     1. 每个机器存储所有数据
     2. 在每个机器本地特征集寻找最佳划分点{特征, 阈值}
     3. 进行各个划分的通信整合并得到最佳划分
     4. 任意一台机器都能执行最佳划分

   该特征并行算法在数据量很大时，每个机器存储所有数据代价高。因此在数据量很大时使用数据并行
2. 数据并行
   传统的数据并行策略主要为水平划分数据，然后本地构建直方图并整合成全局直方图，这种数据划分有一个很大的缺点：通讯开销过大。如果使用点对点通信，一台机器的通讯开销大约为$O(N_{machine}N_{feature}N_{bin})$；如果使用集成的通信，则通讯开销为$O(2N_{feature}N_{bin})$
   * 传统数据并行
     1. 水平划分数据
     2. 各机器以本地数据构建本地直方图
     3. 将本地直方图整合成全局整合图
     4. 在全局直方图中寻找最佳划分，然后执行此划分
   
   LightGBM在数据并行中使用分散规约(Reduce scatter)，把直方图合并的任务也并行化分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量
   * lightGBM的数据并行
     1. 水平划分数据
     2. 各机器以本地数据构建本地直方图
     3. 使用Reduce Scatter并行算子归并来自不同worker的不同特征子集的直方图，然后在局部归并的直方图中找到最优局部分裂信息
     4. 整合为全局最优分裂信息

3. 投票并行
   基于投票的数据并行则进一步优化数据并行中的通信代价，用投票的方式只合并部分特征值的直方图，使通信代价变成常数级别。具体是在每个机器中通过本地的数据选出top k个分裂特征，然后将每个机器选出的k个特征进行汇总，然后利用投票筛选出可能是全局最优分割点的特征，合并直方图的时候只合并这些被选出来的特征，从而降低了通信量。有理论证明，这种voting parallel以很大的概率选出实际最优的特征，因此不用担心top k的问题

### lightGBM总结
lightGBM相比XGBoost，主要有以下优点
1. 速度更快
   * LightGBM 采用了直方图算法将遍历样本转变为遍历直方图，极大的降低了时间复杂度
   * LightGBM 在训练过程中采用单边梯度算法过滤掉梯度小的样本，减少了大量的计算
   * LightGBM 采用了基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要的计算量
   * LightGBM 采用优化后的特征并行、数据并行方法加速计算，当数据量非常大的时候还可以采用投票并行的策略
   * LightGBM 对缓存也进行了优化，增加了缓存命中率

2. 内存占用小
   * LightGBM 采用了直方图算法将存储特征值转变为存储 bin 值，降低了内存消耗
   * LightGBM 在训练过程中采用互斥特征捆绑算法减少了特征数量，降低了内存消耗

但是lightGBM并不是完美的，也有缺陷
* 可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度限制，在保证高效率的同时防止过拟合
* Boosting族是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行权重调整，所以随着迭代不断进行，误差会越来越小，模型的偏差（bias）会不断降低。由于LightGBM是基于偏差的算法，所以会对噪点较为敏感