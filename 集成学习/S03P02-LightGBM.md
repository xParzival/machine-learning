# LightGBM
LightGBM由微软提出，主要用于解决GDBT在海量数据中遇到的问题，以便其可以更好更快地用于工业实践中。相比于XGBoost具有训练速度快、内存占用低的特点
根据[ApacheCN中文文档](https://github.com/apachecn/lightgbm-doc-zh)所述，它是分布式的, 高效的, 它具有以下优势:
* 速度和内存使用的优化
  * 减少分割增益的计算量
  * 通过直方图的相减来进行进一步的加速
  * 减少内存的使用 减少并行学习的通信代价

* 稀疏优化
* 准确率优化
  * Leaf-wise (Best-first) 的决策树生长策略
  * 类别特征值的最优分割

* 网络通信的优化
* 并行学习的优化
  * 特征并行
  * 数据并行
  * 投票并行

* GPU 支持可处理大规模数据

## 单边梯度采样
对于XGBoost而言，在构建回归树中最优特征分裂，分裂点查找时需要全量的样本参与增益的计算，而LightGBM针对这一点，提出了利用单边梯度采样算法(Gradient-Based One-Side Sampling, GOSS)来进行样本采用，不使用全量样本进行信息增益的计算,达到减少样本个数目的
观察GBDT的梯度更新公式
$$
f_t(x)=f_{t-1}(x)-\eta\nabla L(y,f_{t-1}(x))
$$
可以看到梯度小的样本目标函数几乎不更新
同样对于XGBoost的目标函数
$$
Obj(t)=-{1\over2}\sum_{m=1}^M\frac{G_{tm}^2}{H_{tm}+\lambda}+\gamma M
$$
梯度小的样本对目标函数的贡献也非常小
根据上面的分析，我们可以用梯度信息用来衡量样本是否被充分学习，对样本的梯度进行排序，大梯度的样本保留，小梯度的样本随机抽样，减少的训练样本，加速模型训练，但是在抽样之后会产生新的问题，需要额外的应对策略
### 小梯度加权补偿
直接根据样本的梯度进行抽样，会改变训练集中的标签的分布，为了不改变分布需要做额外的加权补偿
具体方法如下：
1. 在训练过程中，根据损失函数计算得到样本的一阶梯度信息，对其进行降序排列
2. 选择$a\times100\%$的大梯度样本，再从剩余样本中随机选择$b\times100\%$个小梯度样本
3. 为了维持样本标签分布不变，需要对小梯度样本进行加权补偿，即
   $$
   \begin{gathered}
   N=N\times a+N\times b\times w\\
   \Rightarrow w=\frac{1-a}{b}
   \end{gathered}
   $$
   在计算小梯度样本的结构分数增益时，

### 结构分数增益的计算方式
在XGBoost中，结点分裂后结构分数增益计算公式为
$$
\text{Gain}={1\over2}\left[\frac{G_l^2}{H_l+\lambda}+\frac{G_r^2}{H_r+\lambda}-\frac{(G_l+G_r)^2}{H_l+H_r+\lambda}\right]-\gamma
$$
