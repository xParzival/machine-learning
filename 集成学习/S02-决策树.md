# S02-决策树
## 概述
### 定义
决策树是树方法的基础，从决策树衍生出了很多复杂的树方法
分类决策树模型是一种描述对实例进行分类的树形结构，由结点和有向边组成。结点又分为内部结点和叶结点，内部结点表示一个特征，叶结点表示一个分类
例如对于二维随机变量$X=(X^{(1)},X^{(2)},X^{(3)})^T$，如果可分为两类，对其进行分类的决策树可能如下图
![DecisionTree](https://s1.ax1x.com/2020/06/28/NgWrcD.png)
其中圆圈就是内部结点，方块就是叶结点。决策树的分类就是从根结点起，对样本的对应特征进行测试，按对应的关系分配到对应的结点，如此递归的对样本进行测试和分配最终到达叶结点，进入所分到的类别
从上面的分析可以看到，决策树能够看成一个if-then的规则集合，样本的特征满足某些条件即决策树上从根结点到叶结点的某条路径，就对应归入叶结点所对应的类别。每一个样本都只对应一条路径或一个if-then规则
### 决策树的学习
#### 决策树的生成
从定义可以知道，决策树的学习就是递归地找到能够最优分割样本的特征及特征值。开始构建根结点，所有训练样本都在根结点，然后找到最优分割特征将样本分割为两个子集，使得两个子集都在当前特征下得到最好的分类；然后在子集上分别采用同样的策略，将子集当作根结点再进行分割；如果某子集已经基本被正确分类，那么就让该子集作为叶结点，如此递归的进行下去直到所有训练样本都分配到相应的叶结点，或者没有合适的特征为止，这样就生成了一颗决策树
如果在一开始特征数量就特别多，那么可以对特征进行筛选，只留下对训练数据有足够分类能力的特征
#### 决策树的剪枝
按上述方法生成的决策树，对训练数据或许效果很好，但是对测试数据未必，极可能发生过拟合，因此需要对决策树进行剪枝，使其有更好的泛化能力。具体地说就是去掉过于细分的叶结点使其退回父结点甚至更高的结点，然后将父结点获更高的结点作为叶结点
总的来说，决策树的生成对应于模型的局部选择，只考虑局部最优，决策树的剪枝对应于模型的全局选择，考虑全局最优
### 特征选择与决策树
在决策树生成过程中，需要选择最优特征对样本进行划分，也就是选择对训练数据具有分类能力的特征。因此下面介绍度量特征对数据分类能力的指标
#### 信息增益
熵是随机变量不确定度的度量，因此可以用熵度量特征的分类能力。设一个样本集共有$K$种分类$Y=\{y_1,y_2,\cdots,y_K\}$，符合分布$P(Y=y_k)=p_k,k=1,2,\cdots,K$，那么不对样本进行划分时，样本自身的不确定度用熵表示
$$
H(Y)=-\sum_{k=1}^Kp_k\log p_k
$$
设某特征$X$取值为$X=\{x_1,x_2,\cdots,x_n\}$且符合分布$Q(X=x_i)=q_i,i=1,2,\cdots,n$。如果该特征对样本的分类能力强，意味着在这个特征取不同的取值时，我们能根据取值更确信样本属于那个分类，或者说在$X$确定时条件概率$P(Y|X)$会更高，分类的不确定性会降低。这个不确定度可用特征$X$确定时分类$Y$条件熵来表示
$$
H(Y|X)=\sum_{i=1}^nq_iH(Y|X=x_i)
$$
很显然，如果在用特征$X$对样本进行分割后，样本的不确定度降低得多，那么该特征对样本的分类能力较强，反之分类能力较弱，于是就可以用特征$X$的熵与用特征$X$分割样本后分类$Y$的条件熵之差作为分类能力的度量，我们称之为信息增益(information gain)，又叫做互信息，表示得知特征$X$的信息使得分类$Y$的信息不确定度减少的成程度
$$
g(Y,X)=H(Y)-H(Y|X)
$$
在决策树生成过程中，根据不同的特征分别计算相应的信息增益，信息增益最大的特征即为当前的最优划分样本特征
下面总结一下决策树生成过程中的信息增益计算方法：设决策树生成过程中某个需要分割的数据集为$D$，$|D|$表示其样本容量；设共有$K$个类别，每个类别用$C_k$表示，$|C_k|$表示每个类别的样本容量；设某特征$X$共有$n$种取值$X=\{x_1,x_2,\cdots,x_n\}$，根据特征$X$的不同取值将$D$划分为$n$个子集$D_1,D_2,\cdots,D_n$，用$D_i$表示对应子集的样本容量，$\sum\limits_{i=1}^n|D_i|=|D|$；记子集$D_i$中属于$C_k$类的样本集为$D_{ik}$，其样本容量为$|D_{ik}|$
1. 计算样本集$D$的熵
   $$
   H(D)=-\sum_{k=1}^K\frac{|D_k|}{|D|}\log\frac{|D_k|}{|D|}
   $$
2. 计算条件熵
   $$
   \begin{gathered}
   H(D|X=x_i)=-\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log\frac{|D_{ik}|}{|D_i|}\\
   \begin{aligned}
   H(D|X)&=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D|X=x_i)\\
   &=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log\frac{|D_{ik}|}{|D_i|}
   \end{aligned}
   \end{gathered}
   $$
3. 计算信息增益
   $$
   \begin{aligned}
   g(D,X)&=H(D)-H(D|X)\\
   &=\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log\frac{|D_{ik}|}{|D_i|}-\sum_{k=1}^K\frac{|D_k|}{|D|}\log\frac{|D_k|}{|D|}
   \end{aligned}
   $$

#### ID3决策树
##### 算法
ID3决策树算法就是根据信息增益指标构建的决策树，其算法步骤如下：
> 输入：训练数据$D$，特征集合$A$，阈值$\varepsilon$
> 输出：决策树$T$
> (1)若$D$中所有样本属于同一类$C_k$，则直接将$D$作为决策树$T$返回，类$C_k$为其类标记
> (2)若$A=\varnothing$，则直接将$D$作为决策树$T$返回，将$D$中样本最多的类$C_k$作为类标记
> (3)不满足(1)、(2)则计算$A$中各特征的信息增益，选择信息增益最大的特征$A_g$，设其信息增益为$g_{max}$
> (4)若$g_{max}\lt\varepsilon$，则直接将$D$作为决策树$T$返回，将$D$中样本最多的类$C_k$作为类标记
> (5)不满足(4)则按$A_g=\{a_i\}$每个可能取值将样本$D$划分为若干子集$D_i$，用$D_i$构建子结点，结点和该子结点构成树$T$并返回
> (6)对每个子结点$D_i$，分别将其作为训练数据，将$A-\{A_g\}$作为特征集，递归进行步骤(1)~(5)，直到得到子树$T_i$
> (7)将$T$与所有子树合并构成最终决策树$T$并返回

##### 算法缺点
1. ID3决策树算法容易过拟合
2. 因为使用的不确定度量是信息增益，由于信息增益存在偏向于选择取值较多的特征的问题。一是当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，因此划分之后的熵更低，导致信息增益容易较大，二是当特征的取值较多时，根据此特征划分样本得到的子集容量小，那么从中估计的经验概率就容易偏离实际概率，划分之后的熵更低，导致信息增益大。这个缺点在样本量不大时体现明显，在样本量极大时几乎没有影响。在样本量不大时，需要对信息增益进行改进
3. ID3决策树算法无法处理连续型特征
4. 只可用于分类，不能用于回归

#### 信息增益率
信息增益率是在信息增益的基础上进行校正，解决信息增益容易偏向选择取值更多的特征的问题
某个特征的信息增益率定义为该特征的信息增益与样本关于该特征的熵之比。令样本$D$关于特征$X$的熵为$H_X(D)$
$$
H_X(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\log\frac{|D_i|}{|D|}
$$
信息增益率为
$$
\begin{aligned}
g_r(D,X)&=\frac{g(D,X)}{H_X(D)}\\
&=\cfrac{\sum\limits_{k=1}^K\cfrac{|D_k|}{|D|}\log\cfrac{|D_k|}{|D|}-\sum\limits_{i=1}^n\cfrac{|D_i|}{|D|}\sum\limits_{k=1}^K\cfrac{|D_{ik}|}{|D_i|}\log\cfrac{|D_{ik}|}{|D_i|}}{\sum\limits_{i=1}^n\cfrac{|D_i|}{|D|}\log\cfrac{|D_i|}{|D|}}
\end{aligned}
$$
信息增益率本质就是给信息增益增加了一个罚项，当特征取值多时，$H_X(D)$较大，于是可以用信息增益比上$H_X(D)$，使其不过于偏向取值多的特征，同时也可以使信息增益率偏向取值少的特征
#### C4.5决策树
C4.5决策树是使用信息增益率为不确定度量的决策树生成算法，其步骤与ID3决策树算法相同，只是将不确定度量换为信息增益率
##### 连续特征处理
为了解决无法处理连续变量的问题，C4.5算法采用以下方法来处理连续特征：
对于某特征$X$，将其在样本中的取值排序得到序列$X=(x_1,x_2,\cdots,x_n)$，然后依次在区间$(x_i,x_{i+1}),i=1,2,\cdots,n$里取一个值（一般取中值$\cfrac{x_i+x_{i+1}}{2}$），按该值划分样本并计算信息增益。这样计算出$n-1$个信息增益，使信息增益最大的那个值即作为分割点取值，如此对每个连续特征进行分割点的选择。实际上这种处理方式相当于对连续特征进行离散化，将连续特征按分割点分为两个部分，变成了二值的离散特征
##### 算法缺点
1. 因为对连续变量进行离散化处理，在样本很大，连续特征取值很多时，算法耗时高，效率低
2. 只可用于分类，不能用于回归

#### 决策树的剪枝
ID3决策树和C4.5决策树生成算法都是递归算法，一直到无法继续才停止，这样生成的决策树很容易出现过拟合，因此要对生成的决策树做简化，这个过程称为剪枝，也就是在生成的决策树基础上裁剪掉一些叶结点或子树
##### 预剪枝
预剪枝就是指在决策树生成过程中就对决策树的某些参数进行限制，不让其一直分裂到完全不能分裂为止，例如限制决策树的最大叶结点数目、最小叶结点样本数、最大树深度和最小分裂样本数等。这样做算法效率更高，但是因为没有从损失函数的角度取考虑，对精度会有影响，为了保证决策树的精确度寻找合适的参数就变得尤为重要
##### 后剪枝
后剪枝就是指在决策树生成后，再对决策树进行剪枝，优化树的结构
在生成决策树时，只考虑了特征对样本划分的效果，都是以最好的分割特征进行划分的，也就是说只考虑了决策树的分类准确度，力求分类准确，但是没有把决策树本身的复杂度考虑进去，从而导致树结构复杂，容易过拟合。因此剪枝就是要把树结构的复杂度考虑进去，在保证准确度足够的同时降低决策树的复杂度
根据上述思想，决策树的剪枝就是优化一个损失函数，这个损失函数的一部分代表决策树犯错误程度，一部分代表决策树的复杂程度
下面介绍一种简单的剪枝方法：决策树犯错误程度显然可以用决策树叶结点的不确定度来表达，也就是熵，决策树容易犯错，意味着叶结点的熵值较大；决策树的复杂度可以用叶结点数目来表达，叶结点数目越多，决策树越复杂。
设决策树$T$的叶结点数目为$|T|$，设叶结点$t$的样本容量为$N_t$，其中属于第$k$类的样本数目为$N_{tk}$，$H_t(T)$为叶结点$t$的经验熵，那么按照上述方法可以写出损失函数
$$
\begin{aligned}
L_\lambda(T)&=L(T)+\lambda|T|\\
&=\sum_{t=1}^{|T|}N_tH_t(T)+\lambda|T|\\
&=-\sum_{t=1}^{|T|}N_t\sum_{k=1}^K\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}+\lambda|T|
\end{aligned}
$$
其中$L(T)$表示决策树的犯错误程度，$\lambda|T|$表示决策树的复杂度。$\lambda\gt0$为常数，可以用来调整我们对决策树复杂度的容忍度。优化这个损失函数就可以综合保证决策树拟合程度和决策树的复杂度，$\lambda$较大意味着选择复杂度较小的树，$\lambda$较小意味着选择复杂度较大的树
##### 剪枝算法
> 输入：生成的树$T$，参数$\lambda$
> 输出：剪枝后的树$T_\lambda$
> (1)计算每个结点的经验熵$H_t(T)$
> (2)递归对决策树$T$的每组叶结点进行剪枝，设一组叶结点剪枝之前的完整决策树为$T_b$，剪枝之后的整体决策树为$T_a$，如果
> $$
> L_\lambda(T_a)\ge L_\lambda(T_b)
> $$
> 则进行剪枝，将这组叶结点的父结点作为新的叶结点，$T_a$作为新的决策树$T$
> (3)返回步骤(2)，直到不能继续，得到剪枝完成损失函数最小的决策树$T_\lambda$

其实比较剪枝前后损失函数的大小，即计算剪枝前后损失函数的差值。因为剪枝前后内部结点的熵值是不变的，所以计算差值时其实不用去计算整体树的损失函数，只用在剪枝的局部计算损失函数的差值就可以了，因此这种剪枝算法可以用动态规划的方式来实现
#### CART决策树
CART决策树全称分类与回归树(Classification and regression tree)，它是可以用于分类也可以用于回归的决策树。CART决策树是二叉树，即每次分割都是将样本分成两个子集，而ID3、C4.5算法都有可能将样本划分为两个以上的子集
##### 回归树
设训练样本$D=\{(X,Y)\}=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$，对于回归树，目标变量$Y$为连续变量。回归树就是将样本按特征划分为多个子空间或者叶结点，每个叶结点对应一个目标取值
设一颗回归树将样本空间划分为$M$个叶结点$R_1,R_2,\cdots,R_M$，每个叶结点$R_m$分别对应取值$r_m$，于是回归树模型可以用指示函数来表达
$$
f(X)=\sum_{m=1}^Mr_mI(X\in R_m)
$$
在回归树中，我们用平方误差$\sum\limits_{x_i\in R_m}(y_i-f(x_i))^2$表达每个叶结点对训练样本的预测误差，每个结点的最优取值就是使平方误差最小的取值，显然$R_m$上的最优取值就是$R_m$上所有样本对应目标值的均值
$$
r_m=\text{avg}(y_i|x_i\in R_m)
$$
下面需要思考如何对样本进行划分并最终得到回归树，本质还是寻找最优划分样本的特征及其取值。类似C4.5算法的处理方式，对某个特征的所有取值排序后依次选择每个取值作为分割点，计算分割后的平方误差，然后取所有取值中使平方误差最小的那个该特征的分割值，然后遍历所有特征，平方误差最小的特征就作为当前划分样本的特征
设$X$有$k$个特征即$k$维，$X=(X^{(1)},X^{(2)},\cdots,X^{(k)})$，对某个特征$X^{(j)}$的取值排序后，对其任意一个取值$s$，都可以将样本划分为两个子集
$$
R_1(j,s)=\{x|x^{(j)}\le s\}\quad R_2(j,s)=\{x|x^{(j)}\gt s\}
$$
那么这两个子集的最优目标值是子集内所有样本目标值的均值
$$
\begin{gathered}
r_1=\text{avg}(y_i|x_i\in R_1(j,s))\\
r_2=\text{avg}(y_i|x_i\in R_2(j,s))
\end{gathered}
$$
先遍历特征$X^{(j)}$的所有取值$s$，计算平方误差，将平方误差最小的取值$s$作为该特征的分割点，该取值对应的平方误差作为该特征的平方误差；再遍历所有特征，将平方误差最小的特征作为回归树当前的最优分割特征，即计算下式
$$
\min_{s,j}\left[\sum_{x_i\in R_1(j,s)}(y_i-r_1)^2+\sum_{x_i\in R_2(j,s)}(y_i-r_2)^2\right]
$$
对初始样本和后续每个子集样本都按此方法划分，直到无法划分为止，这样的回归树一般称为最小二乘回归树
回归树的损失函数并不一定是平方损失函数，实际上理论上可以为任意损失函数，相应的决策树分裂条件也从最小化平方误差变为最小化指定的损失函数
##### 分类树
分类树不用信息增益或信息增益率作为不确定度量，而是用基尼系数(Gini index)
* Gini系数：分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$，则概率分布的Gini系数定义为
  $$
  \text{Gini}(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2
  $$

对于二分类问题，Gini系数为
$$
\text{Gini}(p)=2p(1-p)
$$
Gini系数越大，样本不确定度越大
二分类问题的熵为
$$
H(p)=-p\log p-(1-p)\log(1-p)
$$
二分类问题的分类误差率
$$
\text{error\_rate}=1-\max(p,1-p)
$$
画出Gini系数、熵的一半和分类误差率的图像
![Gini](https://s1.ax1x.com/2020/06/30/N5v2UH.png)
可以看到，熵和Gini系数都可以近似代表分类误差率
设分类树生成过程中某个需要分割的数据集为$D$，$|D|$表示其样本容量
对于离散特征，设特征$X$共有$n$种取值$X=\{x_1,x_2,\cdots,x_n\}$，根据特征$X=x_i$或者$X\ne x_i$将$D$划分为两个子集$D_1,D_2$，用$D_i$表示对应子集的样本容量，$|D_1|+|D_2|=|D|$；对于连续特征应用和C4.5决策树相同的处理方法进行离散化，也可将$D$划分为两个子集$D_1,D_2$
则样本$D$在特征$X$条件下的Gini系数为
$$
\text{Gini}(D,X)=\frac{|D_1|}{|D|}\text{Gini}(D_1)+\frac{|D_2|}{|D|}\text{Gini}(D_2)
$$
对所有特征$X$的所有可能取值计算$\text{Gini}(D,X)$，选其中Gini系数最小的特征及其对应的切分值作为最优切分特征和最优切分点，然后对生成的两个子集递归运用相同的特征选择方式，最终生成分类树，其算法步骤和ID3或C4.5决策树是相同的，只是选择特征的方式不同
CART分类树的停止条件有三个：
1. 结点中样本数目小于设定阈值
2. 结点Gini系数小于设定阈值，即结点中的样本基本属于同一类
3. 没有更多的特征可供选择
