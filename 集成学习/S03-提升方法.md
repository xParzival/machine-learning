# S03-提升方法
提升(boosting)方法是一种集成学习的方法，例如对于决策树这种弱分类器，单个决策树效果可能很差，但是可以对多个决策树进行线性组合，对弱学习器的结果进行整合，以此来提高分类性能
对于分类问题，用比较粗糙的分类规则（弱学习器）分类效果不好，提升方法就从弱学习器出发，把弱学习器学习到的结果进行反复的学习，得到一系列的弱分类器，然后组合起来，具体来说，大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布训练一系列的弱学习器，所以对于提升方法来说主要解决两个基本问题
1. 如何改变训练数据的权值或概率分布
2. 如何将弱分类器组合成强分类器

boosting方法的原理可以用下图表达
![boosting](https://s1.ax1x.com/2020/07/01/NTersU.png)
boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器
显然boosting算法的个体学习器是同质的，而且个体学习器之间存在强依赖关系，一系列个体学习器基本都需要串行生成
boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法
## Adaboost算法
Adaboost算法全称Adaptive boosting（自适应增强），Adaboost解决两个基本问题的思想如下：
1. 每一轮迭代时提高前一轮分类错误的样本权值，降低分类正确的样本权值。这样那些没有被正确分类的样本由于权值增大，会引起更多重视
2. 采用加权多数表决方法确定最终样本的预测分类。加大分类错误率小的弱分类器权值，让其表决时作用增大；减小分类错误率高的分类器权值，让其表决时作用减小

以二分类问题为例。设一个二分类问题的样本集为
$$
D=\{(X,Y)\}=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}
$$
其中$y_i\in\{-1,+1\}$
在第$t$轮迭代也就是第$t$个学习器上的样本权重为
$$
W(t)=(w_{t1},w_{t2},\cdots,w_{tN})
$$
一般初始化第一轮样本权重为$w_{1i}={1\over N},i=1,2,\cdots,N$
$$
W(1)=({1\over N},{1\over N},\cdots,{1\over N})
$$
在第$t$轮用带权值$W(t-1)$的样本训练出一个弱分类器$f_t(x)$，根据迭代的新分类器可以更新样本权重$W(t)$，并且计算该弱学习器的权重$\alpha_t$，最终集成分类器为
$$
f(x)=\alpha_1f_1(x)+\alpha_2f_2(x)+\cdots+\alpha_Tf_T(x)
$$
根据加权多数表决的方法，最终分类结果就观察$f(x)$值的正负，为正那么属于正类，为负那么属于负类，即
$$
g(x)=\text{sign}(f(x))
$$
Adaboost算法是一个加法模型，即基模型的线性组合
### 算法原理
从Adaboost算法的基本思想看，算法的关键就在于如何更新样本权重和计算弱分类器权重
设在第$t$轮迭代时得到的集成分类器为$F_t(x)$，则
$$
F_t(x)=\sum_{s=1}^tf_t(x)
$$
显然有如下递推式
$$
F_{t+1}(x)=F_t(x)+f_{t+1}(x)
$$
最终的集成分类器
$$
f(x)=F_T(x)
$$
Adaboost算法的每一轮迭代都是为了最小化分类错误率，那么可以将分类错误率作为损失函数，但是这个损失函数数学性质差，很难求解最优化问题，因此选用其他的损失函数。Adaboost算法采用的损失函数为指数损失函数，即
$$
L(y,f(x))=\exp(-yf(x))
$$
因此最终得到的强分类器平均损失为
$$
\begin{aligned}
L&={1\over N}\sum_{i=1}^N\exp(-y_if(x_i))\\
&={1\over N}\sum_{i=1}^N\exp\left[-y_i\sum_{t=1}^T\alpha_tf_t(x)\right]\\
&={1\over N}\sum_{i=1}^N\exp\left[-\sum_{t=1}^T\alpha_ty_if_t(x)\right]\\
&={1\over N}\sum_{i=1}^N\prod_{t=1}^T\exp[-\alpha_ty_if_t(x)]
\end{aligned}
$$

但是由于Adaboost算法是个迭代的过程，并没有直接得到$f(x)$，于是算法并没有直接去最小化整体的损失函数，而是采用一种巧妙的思想：在迭代的每一轮，优化当前这一轮集成分类器的损失函数，然后一轮一轮迭代下去，逐步逼近目标。这种方法称为前向分步算法
具体来说，假设在第$t$轮迭代，目标是优化得到集成分类器
$$
F_t(x)=F_{t-1}(x)+f_t(x)=F_{t-1}(x)+\alpha_tf_t(x)
$$
其中$F_{t-1}(x)$是上一轮优化得到的，为已知量，需要求解的即$\alpha_t,f_t(x)$。所以我们的目标就是优化当前集成分类器在样本上的指数损失函数
$$
\begin{aligned}
(\alpha_t,f_t(x))&=\arg\min_{\alpha_t,f_t(x)}\sum_{i=1}^N\exp(-y_iF_t(x_i))\\
&=\arg\min_{\alpha_t,f_t(x)}\sum_{i=1}^N\exp[-y_i(F_{t-1}(x_i)+\alpha_tf_t(x_i))]\\
&=\arg\min_{\alpha_t,f_t(x)}\sum_{i=1}^N\exp[-y_iF_{t-1}(x_i)]\exp[-\alpha_ty_if_t(x_i)]
\end{aligned}
$$
其中$\exp[-y_iF_{t-1}(x_i)]$是上一轮得到的集成学习器对每个样本的损失函数，令$\beta_{ti}=\exp[-y_iF_{t-1}(x_i)]$，则
$$
(\alpha_t,f_t(x))=\arg\min_{\alpha_t,f_t(x)}\sum_{i=1}^N\beta_{ti}\exp[-\alpha_ty_if_t(x_i)]
$$
令$L_t=\sum\limits_{i=1}^N\beta_{ti}\exp[-\alpha_ty_if_t(x_i)],t\ge2$；特别地在$t=1$时有$L_1=\sum\limits_{i=1}^N\exp[-\alpha_1y_if_1(x_i)]$
因为$\beta_{ti}=\exp[-y_iF_{t-1}(x_i)]$是上一轮得到的集成学习器对每个样本的损失函数，于是有如果样本$x_i$在前一轮得到的集成分类器上分类错误那么$\beta_{ti}$就大；如果样本在前一轮得到的集成分类器上分类正确那么$\beta_{ti}$就小，因此$\beta_{ti}$完全可以当作更新样本权重的方法。于是据此我们得到了每一轮样本权重的更新思想：将样本在上一轮集成分类器上的指数损失函数作为本轮新的样本权值
按照Adaboost算法的思想，$f_t(x_i)$是按当前带权重的样本训练得到的个体学习器，因此自然其对应的加权指数损失函数就是最小的，即
$$
\sum_{i=1}^N\beta_{ti}\exp(-y_if_t(x_i))
$$
是最小的。显然在给定$\alpha_t\gt0$时，此式最小与$L_t$最小是等价的
于是综合以上就得出如下结论：以$\beta_{ti}$为第$t$轮的样本权重时，在该权重下的样本上训练得到的个体分类器$f_t(x)$就是使$L_t$最小的$f_t(x)$。这样就求解出$f_t(x)$，并且得到了每一轮样本权重的计算方法
再来求解$\alpha_t$，先对$L_t$做一些变换
$$
\begin{aligned}
L_t&=\sum_{i=1}^N\beta_{ti}\exp[-\alpha_ty_if_t(x_i)]\\
&=\sum_{y_i=f_t(x_i)}\beta_{ti}e^{-\alpha_t}+\sum_{y_i\ne f_t(x_i)}\beta_{ti}e^{\alpha_t}\\
&=e^{-\alpha_t}\sum_{i=1}^N\beta_{ti}-e^{-\alpha_t}\sum_{i=1}^N\beta_{ti}I(y_i\ne f_t(x_i))+e^{\alpha_t}\sum_{i=1}^N\beta_{ti}I(y_i\ne f_t(x_i))\\
&=(e^{\alpha_t}-e^{-\alpha_t})\sum_{i=1}^N\beta_{ti}I(y_i\ne f_t(x_i))+e^{-\alpha_t}\sum_{i=1}^N\beta_{ti}
\end{aligned}
$$
求导并令导数为零可得
$$
\begin{gathered}
\frac{\partial L_t}{\partial\alpha_t}=(e^{\alpha_t}+e^{-\alpha_t})\sum_{i=1}^N\beta_{ti}I(y_i\ne f_t(x_i))-e^{-\alpha_t}\sum_{i=1}^N\beta_{ti}=0\\
\Rightarrow \frac{e^{-\alpha_t}}{e^{\alpha_t}+e^{-\alpha_t}}=\frac{\sum\limits_{i=1}^N\beta_{ti}I(y_i\ne f_t(x_i))}{\sum\limits_{i=1}^N\beta_{ti}}
\end{gathered}
$$
等式右边就相当于以$\beta_{ti}$为样本权重时的分类误差率，令其为$e_t$
$$
e_t=\frac{\sum\limits_{i=1}^N\beta_{ti}I(y_i\ne f_t(x_i))}{\sum\limits_{i=1}^N\beta_{ti}}
$$
则可求得
$$
\alpha_t={1\over2}\ln\frac{1-e_t}{e_t}
$$
这样就得到了第$t$轮弱分类器的权重计算公式
下面再看样本权重的更新
$$
\begin{aligned}
\beta_{ti}&=\exp[-y_iF_{t-1}(x_i)]\\
&=\exp[-y_i(F_{t-2}(x_i)+\alpha_{t-1}f_{t-1}(x_i))]\\
&=\exp[-y_iF_{t-2}(x_i)]\exp(-\alpha_{t-1}y_if_{t-1}(x_i))\\
&=\beta_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]
\end{aligned}
$$
因为样本的权重是为了改变样本的概率分布，所以要对权值做归一化才能作为概率值，于是有
$$
\begin{aligned}
w_{ti}&=\frac{\beta_{ti}}{\sum\limits_{i=1}^N\beta_{ti}}\\
&=\frac{\beta_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}{\sum\limits_{i=1}^N\beta_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}\\
&=\frac{\cfrac{1}{\sum_{j=1}^N\beta_{(t-1)j}}\beta_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}{\cfrac{1}{\sum_{j=1}^N\beta_{(t-1)j}}\sum\limits_{i=1}^N\beta_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}\\
&=\frac{w_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}{\sum\limits_{i=1}^Nw_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}
\end{aligned}
$$
令$Z_t=\sum\limits_{i=1}^Nw_{ti}\exp[-\alpha_ty_if_t(x_i)]$为归一化因子，则样本权重更新公式为
$$
w_{ti}=\frac{w_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}{Z_{t-1}}
$$
初始值在$t=1$时$w_{1i}=\cfrac{1}{N}$
分类误差率$e_t$可以改写为
$$
e_t=\sum\limits_{i=1}^N\frac{\beta_{ti}}{\sum_{j=1}^N\beta_{tj}}I(y_i\ne f_t(x_i))=\sum\limits_{i=1}^Nw_{ti}I(y_i\ne f_t(x_i))
$$
下面证明前向分步算法的有效性，即每一步优化局部损失函数$L_t$就是在优化最终的损失函数$L$。由样本权重更新公式可得
$$
w_{ti}Z_{t-1}=w_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]
$$
对损失函数$L$做一些变换，并将上式带入可得
$$
\begin{aligned}
L&={1\over N}\sum_{i=1}^N\prod_{t=1}^T\exp[-\alpha_ty_if_t(x)]\\
&=\sum_{i=1}^Nw_{1i}\prod_{t=1}^T\exp[-\alpha_ty_if_t(x)]\\
&=\sum_{i=1}^Nw_{1i}\exp[-\alpha_1y_if_1(x)]\prod_{t=2}^T\exp[-\alpha_ty_if_t(x)]\\
&=\sum_{i=1}^Nw_{2i}Z_1\prod_{t=2}^T\exp[-\alpha_ty_if_t(x)]\\
&=Z_1\sum_{i=1}^Nw_{2i}\exp[-\alpha_2y_if_2(x)]\prod_{t=3}^T\exp[-\alpha_ty_if_t(x)]\\
&=\cdots\\
&=\prod_{t=1}^TZ_t
\end{aligned}
$$
显然，要优化损失函数$L$，就是最优化$Z_t$。观察局部损失函数$L_t$和归一化因子$Z_t$，可以发现二者表达式只相差对样本权重的归一化，所以局部损失函数$L_t$和归一化因子$Z_t$其实是等价的，因此最优化$Z_t$等价于最优化$L_t$，于是优化损失函数$L$实际就是在每一轮局部优化$L_t$，因此Adaboost的前向分步算法是有效的
### 算法步骤
根据算法原理的推导总结出二分类Adaboost算法的步骤
> 输入：训练数据$D=\{(x_i,y_i)\}_{i=1}^N$，其中$y_i\in\{-1,+1\}$；个体分类器个数$T$
> 输出：分类器$g(x)=\text{sign}(f(x))$
> (1)初始化样本权重$W(1)=(w_{11},w_{12},\cdots,w_{1N})$，$w_{1i}=\cfrac{1}{N}$
> (2)对$t=1,2,\cdots,T$进行如下迭代
> 1. 根据带权重$W(t)$的训练数据进行训练得到个体分类器$f_t(x)$
> 2. 计算个体分类器$f_t(x)$在带权重$W(t)$的训练数据上的分类误差率
> $$
> e_t=\sum_{i=1}^Nw_{ti}I(y_i\ne f_t(x_i))
> $$
> 3. 利用分类误差率计算$f_t(x)$的权重
> $$
> \alpha_t=\frac{1}{2}\ln\frac{1-e_t}{e_t}
> $$
> 4. 更新样本权重$W(t+1)=(w_{(t+1)1},w_{(t+1)2},\cdots,w_{(t+1)N})$
> $$
> w_{(t+1)i}=\frac{w_{ti}\exp[-\alpha_ty_if_t(x_i)]}{Z_t}
> $$
> 其中$Z_t=\sum\limits_{i=1}^Nw_{ti}\exp[-\alpha_ty_if_t(x_i)]$，为归一化因子
> 
> (3)构建基分类器线性组合
> $$
> f(x)=\sum_{t=1}^T\alpha_tf_t(x)
> $$
> 最终分类器为
> $$
> g(x)=\text{sign}(f(x))=\text{sign}\left(\sum_{t=1}^T\alpha_tf_t(x)\right)
> $$

### Adaboost总结
Adaboost算法的基学习器理论上可以是任何学习器，但是一般常用的是决策树和神经网络。当基学习器是决策树时一般使用CART决策树，而且不仅可以用于分类问题也可以用于回归问题，在回归问题时只需要把分类问题的指数损失函数换成回归问题的损失函数即可，比如线性损失函数，平方损失函数或指数损失函数等
Adaboost算法优缺点：
优点
1. Adaboost作为分类器时，分类精度很高
2. 在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活
3. 作为简单的二元分类器时，构造简单，结果可理解
4. 不容易发生过拟合

缺点
1. 迭代次数也就是弱分类器数目不太好设定，一般要用交叉验证来确定
2. 对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性
3. 训练耗时，因为每一轮都需要训练一个弱分类器

## 提升树
提升树是以CART分类树或回归树为基分类器的提升方法。提升树是统计学习中性能最好的方法之一
提升树可以表达为决策树的加法模型
$$
f(x)=\sum_{t=1}^TT(x;\theta_t)
$$
其中$T(x;\theta_t)$为基分类器决策树，$\theta_t$为决策树的参数
因为是加法模型，所以同样可以用前向分步算法的思想，每一轮迭代优化当前的损失函数。具体来说，设样本$D=\{(x_i,y_i)\}_{i=1}^N$，第$t$轮的训练的基分类器为$T(x;\theta_t)$，集成分类器为$f_t(x)=\sum\limits_{s=1}^tT(x;\theta_s)$，优化本轮的损失函数
$$
L(y,f_t(x))=\sum_{i=1}^NL(y_i,f_t(x_i))
$$
#### 回归问题
回归问题的基分类器为回归树。设第$t$轮回归树有$M$个叶结点，第$m$个叶结点记为$R_m$，对应的输出值为$c_m$
$$
T(x;\theta_t)=\sum_{m=1}^Mc_mI(x\in R_m)
$$
其中参数$\theta_t=\{(R_m,c_m)\}_{m=1}^M$
于是根据前向分步算法在这一轮就是要优化损失函数
$$
L(y,f_t(x))=\sum_{i=1}^NL(y_i,f_{t-1}(x_i)+T(x_i,\theta_t))
$$
对于回归问题，一般用平方损失函数，于是就是优化
$$
\begin{aligned}
L(y,f_t(x))&=\sum_{i=1}^NL[y_i-(f_{t-1}(x_i)+T(x_i,\theta_t))]^2\\
&=\sum_{i=1}^NL[y_i-f_{t-1}(x_i)-T(x_i,\theta_t)]^2\\
&=\sum_{i=1}^NL[r_{ti}-T(x_i,\theta_t)]^2
\end{aligned}
$$
其中$r_{ti}=y_i-f_{t-1}(x_i)$，为上一轮集成学习器在样本上的残差。因此根据损失函数可以看到，本轮的基分类器回归树$T(x,\theta_t)$就是在拟合上一轮集成学习器的残差
这就是回归提升树的基本思路，每一轮用基学习器去拟合上一轮的残差，然后经过迭代一轮一轮减小残差，最终逼近真实值
存在问题：
1. 回归问题可以这样解决，但是在分类问题时，由于目标值为类标记，因此并不能直接计算残差；如果是使用指数损失函数的分类，那么就可以使用Adaboost算法
2. 当损失函数为平方损失函数时可以很简单的得到每一步是优化残差，但是对于一般的损失函数每一步的优化就没有这么简单
#### 梯度提升树
提升树回归问题给我们提供了一种思路，就是每一轮去缩小该轮已得学习器结果与真实目标值的差距，经过多次迭代，使差距越来越小，最终逼近真实值。这个思想类似梯度下降法，函数值沿负梯度方向下降最快，若想求一个函数$f(x)$的极小值，先随机选一个初始点$x_0$，然后按该点负梯度方向寻找下一个点，通过迭代最终收敛于极小值点，表达为
$$
x_t=x_{t-1}-\eta\nabla f(x_{t-1})
$$
将这种思想运用在提升树中，目标是求损失函数的最小值。因为
$$
f_t(x)=f_{t-1}(x)+T(x;\theta_t)
$$
如果将这个迭代过程看作按梯度下降法求$L(y,f(x))$最小值，那么有
$$
f_t(x)=f_{t-1}(x)-\eta\nabla L(y,f_{t-1}(x))
$$
其中$\nabla L(y,f_{t-1}(x))=\cfrac{\partial L(y,f(x))}{\partial f(x)}\bigg|_{f(x)=f_{t-1}(x)}$，取$\eta=1$时可得
$$
T(x;\theta_t)=-\nabla L(y,f_{t-1}(x))
$$
于是就相当于每一轮的基学习器回归树在拟合上一轮的负梯度。这种用基决策树拟合负梯度，按梯度下降法求解损失函数最小值的集成学习器称为梯度提升树(Gradient boosting decision tree,GBDT)，也叫Gradient boosting machine(GBM)
这样无论是分类问题还是回归问题，我们通过其损失函数的负梯度的拟合，就可以用GBDT来解决。区别仅仅在于损失函数不同导致的负梯度不同而已
##### GBDT回归算法步骤
对于回归问题，输出为连续值，可以直接拟合负梯度，总结为如下算法
> 输入：训练数据$D=\{(x_i,y_i)\}_{i=1}^N$，损失函数$L(y,f(x))$，迭代次数$T$
> 输出：$f_T(x)$
> (1)初始化$f_0(x)=\arg\min\limits_c\sum\limits_{i=1}^NL(y_i,c)$
> (2)对$t=1,2,\cdots,T$进行如下迭代：
> 1. 对每个样本计算
> $$
> g_{ti}=\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}\bigg|_{f(x_i)=f_{t-1}(x_i)}
> $$
> 2. 对数据集$\{(x_i,g_{ti})\}_{i=1}^N$拟合回归树$T(x;\theta_t)$，有$M$个叶结点，每个叶结点为$R_{tm}$，这里定义每个叶结点对应的输出值为使损失函数最小的值
> $$
> l_{tm}=\arg\min\limits_l\sum_{x_i\in R_{tm}}L(y_i,f_{t-1}(x_i)+l)
> $$
> 3. 更新集成学习器
> $$
> \begin{aligned}
> f_t(x)&=f_{t-1}(x)+T(x;\theta_t)\\
> &=f_{t-1}(x)+\sum_{m=1}^Ml_{tm}I(x_i\in R_{tm})
> \end{aligned}
> $$
> 
> (3)得到集成学习器$f_T(x)$