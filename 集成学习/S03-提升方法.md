# S03-提升方法
提升(boosting)方法是一种集成学习的方法，例如对于决策树这种弱分类器，单个决策树效果可能很差，但是可以对多个决策树进行线性组合，对弱学习器的结果进行整合，以此来提高分类性能
对于分类问题，用比较粗糙的分类规则（弱学习器）分类效果不好，提升方法就从弱学习器出发，把弱学习器学习到的结果进行反复的学习，得到一系列的弱分类器，然后组合起来，具体来说，大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布训练一系列的弱学习器，所以对于提升方法来说主要解决两个基本问题
1. 如何改变训练数据的权值或概率分布
2. 如何将弱分类器组合成强分类器

boosting方法的原理可以用下图表达
![boosting](https://s1.ax1x.com/2020/07/01/NTersU.png)
boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器
显然boosting算法的个体学习器是同质的，而且个体学习器之间存在强依赖关系，一系列个体学习器基本都需要串行生成
boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法
## Adaboost算法
Adaboost算法全称Adaptive boosting（自适应增强），Adaboost解决两个基本问题的思想如下：
1. 每一轮迭代时提高前一轮分类错误的样本权值，降低分类正确的样本权值。这样那些没有被正确分类的样本由于权值增大，会引起更多重视
2. 采用加权多数表决方法确定最终样本的预测分类。加大分类错误率小的弱分类器权值，让其表决时作用增大；减小分类错误率高的分类器权值，让其表决时作用减小

以二分类问题为例。设一个二分类问题的样本集为
$$
D=\{(X,Y)\}=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}
$$
其中$y_i\in\{-1,+1\}$
在第$t$轮迭代也就是第$t$个学习器上的样本权重为
$$
W(t)=(w_{t1},w_{t2},\cdots,w_{tN})
$$
一般初始化第一轮样本权重为$w_{1i}={1\over N},i=1,2,\cdots,N$
$$
W(1)=({1\over N},{1\over N},\cdots,{1\over N})
$$
在第$t$轮用带权值$W(t-1)$的样本训练出一个弱分类器$f_t(x)$，根据迭代的新分类器可以更新样本权重$W(t)$，并且计算该弱学习器的权重$\alpha_t$，最终集成分类器为
$$
f(x)=\alpha_1f_1(x)+\alpha_2f_2(x)+\cdots+\alpha_Tf_T(x)
$$
根据加权多数表决的方法，最终分类结果就观察$f(x)$值的正负，为正那么属于正类，为负那么属于负类，即
$$
g(x)=\text{sign}(f(x))
$$
Adaboost算法是一个加法模型，即基模型的线性组合
### 算法原理
从Adaboost算法的基本思想看，算法的关键就在于如何更新样本权重和计算弱分类器权重
设在第$t$轮迭代时得到的集成分类器为$F_t(x)$，则
$$
F_t(x)=\sum_{s=1}^tf_t(x)
$$
显然有如下递推式
$$
F_{t+1}(x)=F_t(x)+f_{t+1}(x)
$$
最终的集成分类器
$$
f(x)=F_T(x)
$$
Adaboost算法的每一轮迭代都是为了最小化分类错误率，那么可以将分类错误率作为损失函数，但是这个损失函数数学性质差，很难求解最优化问题，因此选用其他的损失函数。Adaboost算法采用的损失函数为指数损失函数，即
$$
L(y,f(x))=\exp(-yf(x))
$$
因此最终得到的强分类器平均损失为
$$
\begin{aligned}
L&={1\over N}\sum_{i=1}^N\exp(-y_if(x_i))\\
&={1\over N}\sum_{i=1}^N\exp\left[-y_i\sum_{t=1}^T\alpha_tf_t(x)\right]\\
&={1\over N}\sum_{i=1}^N\exp\left[-\sum_{t=1}^T\alpha_ty_if_t(x)\right]\\
&={1\over N}\sum_{i=1}^N\prod_{t=1}^T\exp[-\alpha_ty_if_t(x)]
\end{aligned}
$$

但是由于Adaboost算法是个迭代的过程，并没有直接得到$f(x)$，于是算法并没有直接去最小化整体的损失函数，而是采用一种巧妙的思想：在迭代的每一轮，优化当前这一轮集成分类器的损失函数，然后一轮一轮迭代下去，逐步逼近目标。这种方法称为前向分步算法
具体来说，假设在第$t$轮迭代，目标是优化得到集成分类器
$$
F_t(x)=F_{t-1}(x)+f_t(x)=F_{t-1}(x)+\alpha_tf_t(x)
$$
其中$F_{t-1}(x)$是上一轮优化得到的，为已知量，需要求解的即$\alpha_t,f_t(x)$。所以我们的目标就是优化当前集成分类器在样本上的指数损失函数
$$
\begin{aligned}
(\alpha_t,f_t(x))&=\arg\min_{\alpha_t,f_t(x)}\sum_{i=1}^N\exp(-y_iF_t(x_i))\\
&=\arg\min_{\alpha_t,f_t(x)}\sum_{i=1}^N\exp[-y_i(F_{t-1}(x_i)+\alpha_tf_t(x_i))]\\
&=\arg\min_{\alpha_t,f_t(x)}\sum_{i=1}^N\exp[-y_iF_{t-1}(x_i)]\exp[-\alpha_ty_if_t(x_i)]
\end{aligned}
$$
其中$\exp[-y_iF_{t-1}(x_i)]$是上一轮得到的集成学习器对每个样本的损失函数，令$\beta_{ti}=\exp[-y_iF_{t-1}(x_i)]$，则
$$
(\alpha_t,f_t(x))=\arg\min_{\alpha_t,f_t(x)}\sum_{i=1}^N\beta_{ti}\exp[-\alpha_ty_if_t(x_i)]
$$
令$L_t=\sum\limits_{i=1}^N\beta_{ti}\exp[-\alpha_ty_if_t(x_i)],t\ge2$；特别地在$t=1$时有$L_1=\sum\limits_{i=1}^N\exp[-\alpha_1y_if_1(x_i)]$
因为$\beta_{ti}=\exp[-y_iF_{t-1}(x_i)]$是上一轮得到的集成学习器对每个样本的损失函数，于是有如果样本$x_i$在前一轮得到的集成分类器上分类错误那么$\beta_{ti}$就大；如果样本在前一轮得到的集成分类器上分类正确那么$\beta_{ti}$就小，因此$\beta_{ti}$完全可以当作更新样本权重的方法。于是据此我们得到了每一轮样本权重的更新思想：将样本在上一轮集成分类器上的指数损失函数作为本轮新的样本权值
按照Adaboost算法的思想，$f_t(x_i)$是按当前带权重的样本训练得到的个体学习器，因此自然其对应的加权指数损失函数就是最小的，即
$$
\sum_{i=1}^N\beta_{ti}\exp(-y_if_t(x_i))
$$
是最小的。显然在给定$\alpha_t\gt0$时，此式最小与$L_t$最小是等价的
于是综合以上就得出如下结论：以$\beta_{ti}$为第$t$轮的样本权重时，在该权重下的样本上训练得到的个体分类器$f_t(x)$就是使$L_t$最小的$f_t(x)$。这样就求解出$f_t(x)$，并且得到了每一轮样本权重的计算方法
再来求解$\alpha_t$，先对$L_t$做一些变换
$$
\begin{aligned}
L_t&=\sum_{i=1}^N\beta_{ti}\exp[-\alpha_ty_if_t(x_i)]\\
&=\sum_{y_i=f_t(x_i)}\beta_{ti}e^{-\alpha_t}+\sum_{y_i\ne f_t(x_i)}\beta_{ti}e^{\alpha_t}\\
&=e^{-\alpha_t}\sum_{i=1}^N\beta_{ti}-e^{-\alpha_t}\sum_{i=1}^N\beta_{ti}I(y_i\ne f_t(x_i))+e^{\alpha_t}\sum_{i=1}^N\beta_{ti}I(y_i\ne f_t(x_i))\\
&=(e^{\alpha_t}-e^{-\alpha_t})\sum_{i=1}^N\beta_{ti}I(y_i\ne f_t(x_i))+e^{-\alpha_t}\sum_{i=1}^N\beta_{ti}
\end{aligned}
$$
求导并令导数为零可得
$$
\begin{gathered}
\frac{\partial L_t}{\partial\alpha_t}=(e^{\alpha_t}+e^{-\alpha_t})\sum_{i=1}^N\beta_{ti}I(y_i\ne f_t(x_i))-e^{-\alpha_t}\sum_{i=1}^N\beta_{ti}=0\\
\Rightarrow \frac{e^{-\alpha_t}}{e^{\alpha_t}+e^{-\alpha_t}}=\frac{\sum\limits_{i=1}^N\beta_{ti}I(y_i\ne f_t(x_i))}{\sum\limits_{i=1}^N\beta_{ti}}
\end{gathered}
$$
等式右边就相当于以$\beta_{ti}$为样本权重时的分类误差率，令其为$e_t$
$$
e_t=\frac{\sum\limits_{i=1}^N\beta_{ti}I(y_i\ne f_t(x_i))}{\sum\limits_{i=1}^N\beta_{ti}}
$$
则可求得
$$
\alpha_t={1\over2}\ln\frac{1-e_t}{e_t}
$$
这样就得到了第$t$轮弱分类器的权重计算公式
下面再看样本权重的更新
$$
\begin{aligned}
\beta_{ti}&=\exp[-y_iF_{t-1}(x_i)]\\
&=\exp[-y_i(F_{t-2}(x_i)+\alpha_{t-1}f_{t-1}(x_i))]\\
&=\exp[-y_iF_{t-2}(x_i)]\exp(-\alpha_{t-1}y_if_{t-1}(x_i))\\
&=\beta_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]
\end{aligned}
$$
因为样本的权重是为了改变样本的概率分布，所以要对权值做归一化才能作为概率值，于是有
$$
\begin{aligned}
w_{ti}&=\frac{\beta_{ti}}{\sum\limits_{i=1}^N\beta_{ti}}\\
&=\frac{\beta_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}{\sum\limits_{i=1}^N\beta_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}\\
&=\frac{\cfrac{1}{\sum_{j=1}^N\beta_{(t-1)j}}\beta_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}{\cfrac{1}{\sum_{j=1}^N\beta_{(t-1)j}}\sum\limits_{i=1}^N\beta_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}\\
&=\frac{w_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}{\sum\limits_{i=1}^Nw_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}
\end{aligned}
$$
令$Z_t=\sum\limits_{i=1}^Nw_{ti}\exp[-\alpha_ty_if_t(x_i)]$为归一化因子，则样本权重更新公式为
$$
w_{ti}=\frac{w_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}{Z_{t-1}}
$$
初始值在$t=1$时$w_{1i}=\cfrac{1}{N}$
分类误差率$e_t$可以改写为
$$
e_t=\sum\limits_{i=1}^N\frac{\beta_{ti}}{\sum_{j=1}^N\beta_{tj}}I(y_i\ne f_t(x_i))=\sum\limits_{i=1}^Nw_{ti}I(y_i\ne f_t(x_i))
$$
下面证明前向分步算法的有效性，即每一步优化局部损失函数$L_t$就是在优化最终的损失函数$L$。由样本权重更新公式可得
$$
w_{ti}Z_{t-1}=w_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]
$$
对损失函数$L$做一些变换，并将上式带入可得
$$
\begin{aligned}
L&={1\over N}\sum_{i=1}^N\prod_{t=1}^T\exp[-\alpha_ty_if_t(x)]\\
&=\sum_{i=1}^Nw_{1i}\prod_{t=1}^T\exp[-\alpha_ty_if_t(x)]\\
&=\sum_{i=1}^Nw_{1i}\exp[-\alpha_1y_if_1(x)]\prod_{t=2}^T\exp[-\alpha_ty_if_t(x)]\\
&=\sum_{i=1}^Nw_{2i}Z_1\prod_{t=2}^T\exp[-\alpha_ty_if_t(x)]\\
&=Z_1\sum_{i=1}^Nw_{2i}\exp[-\alpha_2y_if_2(x)]\prod_{t=3}^T\exp[-\alpha_ty_if_t(x)]\\
&=\cdots\\
&=\prod_{t=1}^TZ_t
\end{aligned}
$$
显然，要优化损失函数$L$，就是最优化$Z_t$。观察局部损失函数$L_t$和归一化因子$Z_t$，可以发现二者表达式只相差对样本权重的归一化，所以局部损失函数$L_t$和归一化因子$Z_t$其实是等价的，因此最优化$Z_t$等价于最优化$L_t$，于是优化损失函数$L$实际就是在每一轮局部优化$L_t$，因此Adaboost的前向分步算法是有效的
### 算法步骤
根据算法原理的推导总结出二分类Adaboost算法的步骤
> 输入：训练数据$D=\{(x_i,y_i)\}_{i=1}^N$，其中$y_i\in\{-1,+1\}$；个体分类器个数$T$
> 输出：分类器$g(x)=\text{sign}(f(x))$
> (1)初始化样本权重$W(1)=(w_{11},w_{12},\cdots,w_{1N})$，$w_{1i}=\cfrac{1}{N}$
> (2)对$t=1,2,\cdots,T$进行如下迭代
> 1. 根据带权重$W(t)$的训练数据进行训练得到个体分类器$f_t(x)$
> 2. 计算个体分类器$f_t(x)$在带权重$W(t)$的训练数据上的分类误差率
> $$
> e_t=\sum_{i=1}^Nw_{ti}I(y_i\ne f_t(x_i))
> $$
> 3. 利用分类误差率计算$f_t(x)$的权重
> $$
> \alpha_t=\frac{1}{2}\ln\frac{1-e_t}{e_t}
> $$
> 4. 更新样本权重$W(t+1)=(w_{(t+1)1},w_{(t+1)2},\cdots,w_{(t+1)N})$
> $$
> w_{(t+1)i}=\frac{w_{ti}\exp[-\alpha_ty_if_t(x_i)]}{Z_t}
> $$
> 其中$Z_t=\sum\limits_{i=1}^Nw_{ti}\exp[-\alpha_ty_if_t(x_i)]$，为归一化因子
> 
> (3)构建基分类器线性组合
> $$
> f(x)=\sum_{t=1}^T\alpha_tf_t(x)
> $$
> 最终分类器为
> $$
> g(x)=\text{sign}(f(x))=\text{sign}\left(\sum_{t=1}^T\alpha_tf_t(x)\right)
> $$

### Adaboost总结
Adaboost算法的基学习器理论上可以是任何学习器，但是一般常用的是决策树和神经网络。当基学习器是决策树时一般使用CART决策树，而且不仅可以用于分类问题也可以用于回归问题，在回归问题时只需要把分类问题的指数损失函数换成回归问题的损失函数即可，比如线性损失函数，平方损失函数或指数损失函数等
Adaboost算法优缺点：
优点
1. Adaboost作为分类器时，分类精度很高
2. 在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活
3. 作为简单的二元分类器时，构造简单，结果可理解
4. 不容易发生过拟合

缺点
1. 迭代次数也就是弱分类器数目不太好设定，一般要用交叉验证来确定
2. 对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性
3. 训练耗时，因为每一轮都需要训练一个弱分类器

## 提升树
提升树是以CART分类树或回归树为基分类器的提升方法。提升树是统计学习中性能最好的方法之一
提升树可以表达为决策树的加法模型
$$
f(x)=\sum_{t=1}^TT(x;\theta_t)
$$
其中$T(x;\theta_t)$为基分类器决策树，$\theta_t$为决策树的参数
因为是加法模型，所以同样可以用前向分步算法的思想，每一轮迭代优化当前的损失函数。具体来说，设样本$D=\{(x_i,y_i)\}_{i=1}^N$，第$t$轮的训练的基分类器为$T(x;\theta_t)$，集成分类器为$f_t(x)=\sum\limits_{s=1}^tT(x;\theta_s)$，优化本轮的损失函数
$$
L(y,f_t(x))=\sum_{i=1}^NL(y_i,f_t(x_i))
$$
#### 回归问题
回归问题的基分类器为回归树。设第$t$轮回归树有$M$个叶结点，第$m$个叶结点记为$R_m$，对应的输出值为$c_m$
$$
T(x;\theta_t)=\sum_{m=1}^Mc_mI(x\in R_m)
$$
其中参数$\theta_t=\{(R_m,c_m)\}_{m=1}^M$
于是根据前向分步算法在这一轮就是要优化损失函数
$$
L(y,f_t(x))=\sum_{i=1}^NL(y_i,f_{t-1}(x_i)+T(x_i,\theta_t))
$$
对于回归问题，一般用平方损失函数，于是就是优化
$$
\begin{aligned}
L(y,f_t(x))&=\sum_{i=1}^NL[y_i-(f_{t-1}(x_i)+T(x_i,\theta_t))]^2\\
&=\sum_{i=1}^NL[y_i-f_{t-1}(x_i)-T(x_i,\theta_t)]^2\\
&=\sum_{i=1}^NL[r_{ti}-T(x_i,\theta_t)]^2
\end{aligned}
$$
其中$r_{ti}=y_i-f_{t-1}(x_i)$，为上一轮集成学习器在样本上的残差。因此根据损失函数可以看到，本轮的基分类器回归树$T(x,\theta_t)$就是在拟合上一轮集成学习器的残差
这就是回归提升树的基本思路，每一轮用基学习器去拟合上一轮的残差，然后经过迭代一轮一轮减小残差，最终逼近真实值
存在问题：
1. 回归问题可以这样解决，但是在分类问题时，由于目标值为类标记，因此并不能直接计算残差；如果是使用指数损失函数的分类，那么就可以使用Adaboost算法
2. 当损失函数为平方损失函数时可以很简单的得到每一步是优化残差，但是对于一般的损失函数每一步的优化就没有这么简单
#### 梯度提升树
提升树回归问题给我们提供了一种思路，就是每一轮去缩小该轮已得学习器结果与真实目标值的偏差，经过多次迭代，使偏差越来越小，最终逼近真实值。这个思想类似梯度下降法，函数值沿负梯度方向下降最快，若想求一个函数$f(x)$的极小值，先随机选一个初始点$x_0$，然后按该点负梯度方向寻找下一个点，通过迭代最终收敛于极小值点，表达为
$$
x_t=x_{t-1}-\eta\nabla f(x_{t-1})
$$
将这种思想运用在提升树中，目标是求损失函数的最小值。因为
$$
f_t(x)=f_{t-1}(x)+T(x;\theta_t)
$$
如果将这个迭代过程看作按梯度下降法求$L(y,f(x))$最小值，那么有
$$
f_t(x)=f_{t-1}(x)-\eta\nabla L(y,f_{t-1}(x))
$$
其中$\nabla L(y,f_{t-1}(x))=\cfrac{\partial L(y,f(x))}{\partial f(x)}\bigg|_{f(x)=f_{t-1}(x)}$，取$\eta=1$时可得
$$
T(x;\theta_t)=-\nabla L(y,f_{t-1}(x))
$$
于是就相当于每一轮的基学习器回归树在拟合上一轮的负梯度。这种用基决策树拟合负梯度，按梯度下降法求解损失函数最小值的集成学习器称为梯度提升树(Gradient boosting decision tree,GBDT)，也叫Gradient boosting machine(GBM)
这样无论是分类问题还是回归问题，我们通过其损失函数的负梯度的拟合，就可以用GBDT来解决。区别仅仅在于损失函数不同导致的负梯度不同而已
##### GBDT回归算法
对于回归问题，输出为连续值，可以直接拟合负梯度，总结为如下算法
> 输入：训练数据$D=\{(x_i,y_i)\}_{i=1}^N$，损失函数$L(y,f(x))$，迭代次数$T$
> 输出：集成学习器$f(x)=f_T(x)$
> (1)初始化$f_0(x)=\arg\min\limits_c\sum\limits_{i=1}^NL(y_i,c)$
> (2)对$t=1,2,\cdots,T$进行如下迭代：
> 1. 对每个样本计算负梯度
> $$
> g_{ti}=-\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}\bigg|_{f(x_i)=f_{t-1}(x_i)}
> $$
> 2. 对数据集$\{(x_i,g_{ti})\}_{i=1}^N$拟合回归树$T(x;\theta_t)$，有$M$个叶结点，每个叶结点为$R_{tm}$，这里定义每个叶结点对应的输出值为使损失函数最小的值
> $$
> l_{tm}=\arg\min\limits_l\sum_{x_i\in R_{tm}}L(y_i,f_{t-1}(x_i)+l)
> $$
> 3. 更新集成学习器
> $$
> \begin{aligned}
> f_t(x)&=f_{t-1}(x)+T(x;\theta_t)\\
> &=f_{t-1}(x)+\sum_{m=1}^Ml_{tm}I(x_i\in R_{tm})
> \end{aligned}
> $$
> 
> (3)得到集成学习器$f(x)=f_T(x)$

##### GBDT分类算法
对于分类问题，输出为类标记而不是连续值，不能直接使用负梯度来拟合，需要做一些处理。一般我们考虑使用类似于逻辑回归的对数似然损失函数的方法，即用类别的预测概率值和真实概率值的差来拟合损失
对于二元分类问题，$y_i\in\{0,1\}$，类似于逻辑回归，最后的预测值可以表达为概率值
$$
P(y=1|x)=p=\frac{1}{1+\exp[-f(x)]}
$$
和逻辑回归一样，损失函数为对数似然损失函数即交叉熵损失函数
$$
\begin{aligned}
L(y,f(x))&=-y\ln p-(1-y)\ln(1-p)\\
&=y\ln(1+\exp[-f(x)])-(1-y)\ln(1-\frac{1}{1+\exp[-f(x)]})\\
&=y\ln(1+\exp[-f(x)])+(1-y)(f(x)+\ln(1+\exp[-f(x)]))
\end{aligned}
$$
那么在第$t$轮，损失函数在$f_{t-1}(x)$上的负梯度
$$
\begin{aligned}
-\nabla L(y,f_{t-1}(x))&=-\frac{\partial L(y,f(x))}{\partial f(x)}\bigg|_{f(x)=f_{t-1}(x)}\\
&=y-\frac{1}{1+\exp[-f_{t-1}(x)]}\\
&=y-p_{t-1}
\end{aligned}
$$
即负梯度就是真实类标记与预测概率值的差值，所以每个基回归树都是在拟合真实类别与预测概率之差，与以平方误差为损失函数的回归问题是类似的
对于多分类问题，就是把交叉熵损失函数改写为多分类时的形式
$$
\begin{aligned}
L(y,f(x))&=-y_1\ln p_1-y_2\ln p_2-\cdots-y_K\log p_K\\
&=-\sum_{k=1}^Ky_k\ln p_k
\end{aligned}
$$
其中$k$代表一种分类，共有$K$种类别，当样本属于第$k$类时$y_k=1$，$p_k$为softmax归一化的概率
$$
p_k=\frac{\exp(f_k(x))}{\sum\limits_{k=1}^K\exp(f_k(x))}
$$
##### 常用损失函数
分类问题：
1. 指数损失函数
   $$
   L(y,f(x))=\exp[-yf(x)]
   $$
2. 对数损失函数（交叉熵损失函数）

回归问题：
1. 均方损失函数
   $$
   L(y,f(x))=[y-f(x)]^2
   $$
2. 绝对损失函数
   $$
   L(y,f(x))=|y-f(x)|
   $$
   其负梯度为
   $$
   \nabla L(y,f(x))=\text{sign}(y-f(x))
   $$
3. Huber损失函数
   它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。损失函数如下：
   $$
   L(y,f(x))=
   \begin{cases}
   {1\over2}(y-f(x))^2&|y-f(x)|\le\delta\\
   \delta(|y-f(x)|-{\delta\over2})&|y-f(x)|\gt\delta
   \end{cases}
   $$
   其负梯度为
   $$
   \nabla L(y,f(x))=
   \begin{cases}
   f(x)-y&|y-f(x)|\le\delta\\
   \delta\text{sign}(y-f(x))&|y-f(x)|\gt\delta
   \end{cases}
   $$
4. 分位数(Quantile)损失函数
   $$
   L(y,f(x))=
   \begin{cases}
   \theta|y-f(x)|&y\ge f(x)\\
   (1-\theta)|y-f(x)|&y\lt f(x)
   \end{cases}
   $$
   其中$\theta$为分位数，需要我们在回归前指定。对应的负梯度为
   $$
   \nabla L(y,f(x))=
   \begin{cases}
   \theta&y\ge f(x)\\
   \theta-1&y\lt f(x)
   \end{cases}
   $$
##### GBDT的正则化
为了降低过拟合风险，GBDT也需要加入正则化处理
1. 在迭代式加入步长$0\lt\eta\lt1$，即
   $$
   f_t(x)=f_{t-1}(x)+\eta T(x;\theta_t)
   $$
   显然加入步长后迭代次数要增加，所以需要调试不同的$\eta$和迭代次数
2. 按比例子采样(subsample)
   在训练数据中作不放回抽样，选择一定比例的子样本进行学习。但比例太小会增加模型的偏差，因此一般比例取$[0.5,0.8]$之间
3. 对基决策树进行剪枝
4. 提前停止(early stopping)
   选择一部分样本作为验证集，在迭代拟合训练集的过程中，如果模型在验证集里错误率不再下降，但还没有达到最大迭代次数就提前停止训练

##### GBDT总结
优点：
1. 可以灵活处理各种类型的数据，包括连续值和离散值
2. 使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如Huber损失函数和Quantile损失函数

缺点：
1. 由于弱学习器之间存在依赖关系，难以并行训练数据
2. 与其他基于树的模型相同，它通常不适用于高维稀疏数据
3. 调参比较耗时，训练也比较耗时

#### XGboost
XGboost是现在效果最好，应用最广泛的boosting方法。相比GBDT它加入了正则化项、对缺失数据做处理等优点，让它非常流行
XGboost算法的思想跟一般的boosting算法相同，也是一个利用前向分步算法的加法模型，每一轮迭代生成一棵回归树使当前损失函数最小，经过多次迭代最小化整体损失函数。与GBDT不同的是，XGboost在最小化目标中添加了正则化项。设共经历$T$轮迭代，集成分类器表达为
$$
f(x)=f_T(x)=\sum_{t=1}^TT(x;\theta_t)
$$
可以看到，某个样本$x_i$的预测值是其在每一轮决策树$T(x_i;\theta_t)$上预测值的总和，所以XGboost也相当于每一轮去减少预测值与真实目标值之间的偏差，通过一轮一轮的迭代趋近于真实值
目标函数定义为损失函数加上正则化项，于是在每一轮
$$
\begin{aligned}
Obj(t)&=L(y,f_t(x))+\Omega(f_t(x))\\
&=\sum_{i=1}^NL(y_i,f_t(x_i))+\Omega(f_t(x))
\end{aligned}
$$
直接最小化目标函数当然是可以的，但是这样就和非集成学习的方法无异。而我们要用boosting思想来解决问题，也就是要找一个方法把之前的迭代与当前轮的迭代联系起来，XGboost采用将损失函数进行二阶泰勒展开的方式。根据泰勒公式有
$$
f(x+\Delta x)\approx f(x)+f^{'}(x)\Delta x+{1\over2}f^{''}(x)\Delta x^2
$$
于是损失函数可以展开为
$$
\begin{aligned}
L(y_i,f_t(x_i))&=L(y_i,f_{t-1}(x_i)+T(x_i;\theta_t))\\
&\approx L(y_i,f_{t-1}(x_i))+\frac{\partial L(y,f(x))}{\partial f(x)}\bigg|_{f(x)=f_{t-1}(x_i)}T(x_i;\theta_t)+{1\over2}\frac{\partial^2L(y,f(x))}{\partial^2f(x)}\bigg|_{f(x)=f_{t-1}(x_i)}T^2(x_i;\theta_t)
\end{aligned}
$$
令
$$
\begin{gathered}
g_{ti}=\frac{\partial L(y,f(x))}{\partial f(x)}\bigg|_{f(x)=f_{t-1}(x_i)}\\
h_{ti}=\frac{\partial^2L(y,f(x))}{\partial^2f(x)}\bigg|_{f(x)=f_{t-1}(x_i)}
\end{gathered}
$$
目标函数可写为
$$
Obj(t)=\sum_{i=1}^NL(y_i,f_{t-1}(x_i))+\sum_{i=1}^Ng_{ti}T(x_i;\theta_t)+\sum_{i=1}^N{1\over2}h_{ti}T^2(x_i;\theta_t)+\Omega(f_t(x))
$$
对于正则项，它是当前已经生成的所有树正则化项的总和，即
$$
\begin{aligned}
\Omega(f_t(x))&=\sum_{s=1}^t\Omega(T(x;\theta_s))\\
&=\sum_{s=1}^{t-1}\Omega(T(x;\theta_s))+\Omega(T(x;\theta_t))
\end{aligned}
$$
对于第$t$轮训练完成的回归树，设其有$M$个叶结点，每个叶结点记作$R_{tm}$，对应的取值为$c_{tm}$，即
$$
T(x;\theta_t)=\sum_{m=1}^Mc_{tm}I(x\in R_{tm})
$$
正则化项就是树的复杂度，用以降低过拟合，我们希望树的结构越简单越好。那么一方面希望叶结点个数$M$少；一方面希望叶结点对应的取值$c_{tm}$小。为什么$c_{tm}$要尽量小呢？其一，泰勒展开式近似的条件就是$\Delta x$足够小，反应到损失函数中就是$c_{tm}$足够小；其二，$c_{tm}$小本身就能一定程度降低过拟合，比如某个样本目标值为1，那么可以前一棵树拟合值为10001，后一棵树拟合值为-10000，按这种方法理论上任意值都可以找到完美拟合方式，于是可以学习到在训练集上准确率100%的集成学习器，显然过拟合严重，而限制$c_{tm}$大小可以降低这种风险。因此一棵树的正则化项可以表达为
$$
\Omega(T(x;\theta_t))={1\over2}\lambda\sum_{m=1}^Mc_{tm}^2+\gamma M
$$
$0\lt\lambda,\gamma\lt1$，是用于控制我们对树复杂度的容忍程度的参数
因为前$t-1$棵树已经训练完成了，因此其损失以及其正则化项都是常数，因此第$t$轮的目标函数为
$$
\begin{aligned}
Obj(t)&=\sum_{i=1}^Ng_{ti}T(x_i;\theta_t)+\sum_{i=1}^N{1\over2}h_{ti}T^2(x_i;\theta_t)+{1\over2}\lambda\sum_{m=1}^Mc_{tm}^2+\gamma M+\text{constant}\\
&=\sum_{i=1}^Ng_{ti}\sum_{m=1}^Mc_{tm}I(x_i\in R_{tm})+\sum_{i=1}^N{1\over2}h_{ti}\sum_{m=1}^Mc_{tm}^2I(x_i\in R_{tm})+{1\over2}\lambda\sum_{m=1}^Mc_{tm}^2+\gamma M+\text{constant}\\
&=\sum_{m=1}^M\sum_{i=1}^Ng_{ti}c_{tm}I(x_i\in R_{tm})+\sum_{m=1}^M\sum_{i=1}^N{1\over2}h_{ti}c_{tm}^2I(x_i\in R_{tm})+{1\over2}\lambda\sum_{m=1}^Mc_{tm}^2+\gamma M+\text{constant}\\
&=\sum_{m=1}^M\sum_{x_i\in R_{tm}}(g_{ti}c_{tm}+{1\over2}h_{ti}c_{tm}^2)+{1\over2}\lambda\sum_{m=1}^Mc_{tm}^2+\gamma M+\text{constant}
\end{aligned}
$$
令$G_{tm}=\sum\limits_{x_i\in R_{tm}}g_{ti},H_{tm}=\sum\limits_{x_i\in R_{tm}}h_{ti}$，即回归树某个叶结点样本的一阶导数或二阶导数之和。因为是最小化，所以可省去常数项，则
$$
\begin{aligned}
Obj(t)&=\sum_{m=1}^M(G_{tm}c_{tm}+{1\over2}H_{tm}c_{tm}^2)+{1\over2}\lambda\sum_{m=1}^Mc_{tm}^2+\gamma M\\
&=\sum_{m=1}^M[G_{tm}c_{tm}+({1\over2}H_{tm}+{1\over2}\lambda)c_{tm}^2]+\gamma M
\end{aligned}
$$
要求使目标函数最小的$c_{tm}$，那么对$c_{tm}$求导令其为零
$$
\begin{gathered}
\frac{\partial Obj(t)}{\partial c_{tm}}=G_{tm}+(H_{tm}+\lambda)c_{tm}=0\\
\Rightarrow c_{tm}=-\frac{G_{tm}}{H_{tm}+\lambda}
\end{gathered}
$$
于是我们得到了回归树每个叶结点的拟合值，这就是XGboost每一轮的回归树拟合目标。将其带入目标函数得
$$
Obj(t)=-{1\over2}\sum_{m=1}^M\frac{G_{tm}^2}{H_{tm}+\lambda}+\gamma M
$$
至此我们已经得到了回归树最终叶结点的结果
##### 回归树分裂算法
因为建树的目的是最小化目标函数，显然回归树的结果会影响目标函数值，上述推导已经找到了能使目标函数最小的叶结点结果，但是还有一个因素能够影响目标函数值，即回归树的结构，回归树的结构决定了会如何就将样本分裂为叶结点。于是下面需要思考如何确定树结构即如何建树。又因为目标函数上式已经给出了计算方式，那么自然想到学习回归树以优化目标函数的方式进行，即每次分裂结点目的都是使目标函数值减小，所以上式又称为回归树的结构分数，反映了回归树结构的好坏
具体来讲，对于回归树中某一个结点$R$，我们要确定分不分裂，传统回归树的分裂指标一般是平方损失函数，在XGboost算法中分裂指标即结构分数，即分裂前后结构分数的变化，如果结构分数下降则分裂，否则不分裂。类似传统回归树，对于数据的每一个特征按取值排序，然后对每一个特征的每一个取值都做如下计算：如果以该特征的该取值为分裂点，设分为左子结点$R_l$和右子结点$R_r$，因为只考虑这一个结点的分裂情况，其余结点都不变，所以分裂前后结构分数的变化也只跟这一个结点有关。则分裂前结构分数为
$$
Obj_1=-{1\over2}\frac{(G_l+G_r)^2}{H_l+H_r+\lambda}+\gamma
$$
分裂后结构分数为
$$
Obj_2=-{1\over2}\left[\frac{G_l^2}{H_l+\lambda}+\frac{G_r^2}{H_r+\lambda}\right]+2\gamma
$$
所以分裂前后结构分数的变化为
$$
\begin{aligned}
\text{Gain}&=Obj_1-Obj_2\\
&={1\over2}\left[\frac{G_l^2}{H_l+\lambda}+\frac{G_r^2}{H_r+\lambda}-\frac{(G_l+G_r)^2}{H_l+H_r+\lambda}\right]-\gamma
\end{aligned}
$$
遍历所有特征的所有取值，选择使$\text{Gain}$最大的特征及其对应取值作为分裂条件。可以看到结构分数也可以作为特征重要性的指标，$\text{Gain}$越大的特征对样本的影响更大。另外，并不是每次都需要分裂，如果$\text{Gain}\gt0$，说明分裂这个结点使目标函数变小，那么可以分裂，反之若$\text{Gain}\lt0$则不分裂。实际使用中一般设定一个$\text{Gain}$的阈值，大于这个阈值就分裂结点，反之不作分裂
##### 回归树分裂近似算法
当数据量非常大的时候，采用刚才的建树方法需要的内存非常大，因为需要存储太多一阶和二阶导数值，因此改进出了近似算法。对于每个特征，只考察分位点可以减少计算复杂度。首先根据特征分布的分位数提出候选划分点，然后将连续型特征映射到由这些候选点划分的桶中，然后再找到所有区间的最佳分裂点
对于特征某个特征，根据其分布确定$l$个候选切分点$S=\{s_1,s_2,\cdots,s_l\}$，然后将样本对应到各切分区间中，再对$S$中的每个切分点贪心搜索最低结构分数
分位点的选取有两种策略：
1. global：学习每棵树前就提出候选切分点，并在每次分裂时都采用这种分割
2. local：每次分裂前将重新提出候选切分点

显然local策略需要的计算复杂度更高，但是global的精度不如local。但是当global分位点取得较为精细时也可以达到和local相似的精度。根据实践经验，在分位数取值合理的情况下，分位数策略可以获得与贪心算法相同的精度
##### 稀疏感知
实际工程中一般会出现输入值稀疏的情况。比如数据的缺失、one-hot编码都会造成输入数据稀疏。XGBoost在构建树的结点过程中只考虑非缺失值的数据遍历，而为每个结点增加了一个缺省方向，当样本相应的特征值缺失时，可以被归类到缺省方向上，最优的缺省方向可以从数据中学到
至于如何学到缺省值的分支，其实很简单，在利用非缺失值分裂结点之后，分别枚举特征缺失的样本归为左右分支后的增益，选择增益最大的方向作为最优缺省方向。如果在训练时特征没有缺失，而预测数据中缺失，那么默认归为右子树
##### 并行学习
XGboost算法总体来说是无法并行的，因为前向分步算法的缘故，导致必须进行串行计算。但是可以进行局部并行化，在树生成过程中，最耗时的一个步骤就是在每次寻找最佳分裂点时都需要对特征的值进行排序。而XGBoost在训练之前会根据特征对数据进行排序，然后保存排序后的索引
通过顺序访问排序后特征的索引，方便进行切分点的查找。多个特征之间互不干涉，可以使用多线程同时对不同的特征进行切分点查找，即特征的并行化处理。在对节点进行分裂时需要选择增益最大的特征作为分裂，这时各个特征的增益计算可以同时进行，这样就能够局部实现分布式或者多线程计算