# S03-提升方法
提升(boosting)方法是一种集成学习的方法，例如对于决策树这种弱分类器，单个决策树效果可能很差，但是可以对多个决策树进行线性组合，对弱学习器的结果进行整合，以此来提高分类性能
对于分类问题，用比较粗糙的分类规则（弱学习器）分类效果不好，提升方法就从弱学习器出发，把弱学习器学习到的结果进行反复的学习，得到一系列的弱分类器，然后组合起来，具体来说，大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布训练一系列的弱学习器，所以对于提升方法来说主要解决两个基本问题
1. 如何改变训练数据的权值或概率分布
2. 如何将弱分类器组合成强分类器

boosting方法的原理可以用下图表达
![boosting](https://s1.ax1x.com/2020/07/01/NTersU.png)
boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器
显然boosting算法的个体学习器是同质的，而且个体学习器之间存在强依赖关系，一系列个体学习器基本都需要串行生成
boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法
## Adaboost算法
Adaboost算法全称Adaptive boosting（自适应增强），Adaboost解决两个基本问题的思想如下：
1. 每一轮迭代时提高前一轮分类错误的样本权值，降低分类正确的样本权值。这样那些没有被正确分类的样本由于权值增大，会引起更多重视
2. 采用加权多数表决方法确定最终样本的预测分类。加大分类错误率小的弱分类器权值，让其表决时作用增大；减小分类错误率高的分类器权值，让其表决时作用减小

以二分类问题为例。设一个二分类问题的样本集为
$$
D=\{(X,Y)\}=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}
$$
其中$y_i\in\{-1,+1\}$
在第$t$轮迭代也就是第$t$个学习器上的样本权重为
$$
W(t)=(w_{t1},w_{t2},\cdots,w_{tN})
$$
一般初始化第一轮样本权重为$w_{1i}={1\over N},i=1,2,\cdots,N$
$$
W(1)=({1\over N},{1\over N},\cdots,{1\over N})
$$
在第$t$轮用带权值$W(t-1)$的样本训练出一个弱分类器$f_t(x)$，根据迭代的新分类器可以更新样本权重$W(t)$，并且计算该弱学习器的权重$\alpha_t$，最终集成分类器为
$$
f(x)=\alpha_1f_1(x)+\alpha_2f_2(x)+\cdots+\alpha_Tf_T(x)
$$
根据加权多数表决的方法，最终分类结果就观察$f(x)$值的正负，为正那么属于正类，为负那么属于负类，即
$$
g(x)=\text{sign}(f(x))
$$
### 算法原理
从Adaboost算法的基本思想看，算法的关键就在于如何更新样本权重和计算弱分类器权重
设在第$t$轮迭代时得到的集成分类器为$F_t(x)$，则
$$
F_t(x)=\sum_{s=1}^tf_t(x)
$$
显然有如下递推式
$$
F_{t+1}(x)=F_t(x)+f_{t+1}(x)
$$
最终的集成分类器
$$
f(x)=F_T(x)
$$
Adaboost算法的每一轮迭代都是为了最小化分类错误率，那么可以将分类错误率作为损失函数，但是这个损失函数数学性质差，很难求解最优化问题，因此选用其他的损失函数。Adaboost算法采用的损失函数为指数损失函数，即
$$
L(y,f(x))=\exp(-yf(x))
$$
因此最终得到的强分类器平均损失为
$$
\begin{aligned}
L&={1\over N}\sum_{i=1}^N\exp(-y_if(x_i))\\
&={1\over N}\sum_{i=1}^N\exp\left[-y_i\sum_{t=1}^T\alpha_tf_t(x)\right]\\
&={1\over N}\sum_{i=1}^N\exp\left[-\sum_{t=1}^T\alpha_ty_if_t(x)\right]\\
&={1\over N}\sum_{i=1}^N\prod_{t=1}^T\exp[-\alpha_ty_if_t(x)]
\end{aligned}
$$

但是由于Adaboost算法是个迭代的过程，并没有直接得到$f(x)$，于是算法并没有直接去最小化整体的损失函数，而是采用一种巧妙的思想：在迭代的每一轮，优化当前这一轮集成分类器的损失函数，然后一轮一轮迭代下去，逐步逼近目标。这种方法称为前向分步算法
具体来说，假设在第$t$轮迭代，目标是优化得到集成分类器
$$
F_t(x)=F_{t-1}(x)+f_t(x)=F_{t-1}(x)+\alpha_tf_t(x)
$$
其中$F_{t-1}(x)$是上一轮优化得到的，为已知量，需要求解的即$\alpha_t,f_t(x)$。所以我们的目标就是优化当前集成分类器在样本上的指数损失函数
$$
\begin{aligned}
(\alpha_t,f_t(x))&=\arg\min_{\alpha_t,f_t(x)}\sum_{i=1}^N\exp(-y_iF_t(x_i))\\
&=\arg\min_{\alpha_t,f_t(x)}\sum_{i=1}^N\exp[-y_i(F_{t-1}(x_i)+\alpha_tf_t(x_i))]\\
&=\arg\min_{\alpha_t,f_t(x)}\sum_{i=1}^N\exp[-y_iF_{t-1}(x_i)]\exp[-\alpha_ty_if_t(x_i)]
\end{aligned}
$$
其中$\exp[-y_iF_{t-1}(x_i)]$是上一轮得到的集成学习器对每个样本的损失函数，令$\beta_{ti}=\exp[-y_iF_{t-1}(x_i)]$，则
$$
(\alpha_t,f_t(x))=\arg\min_{\alpha_t,f_t(x)}\sum_{i=1}^N\beta_{ti}\exp[-\alpha_ty_if_t(x_i)]
$$
令$L_t=\sum\limits_{i=1}^N\beta_{ti}\exp[-\alpha_ty_if_t(x_i)],t\ge2$；特别地在$t=1$时有$L_1=\sum\limits_{i=1}^N\exp[-\alpha_1y_if_1(x_i)]$
因为$\beta_{ti}=\exp[-y_iF_{t-1}(x_i)]$是上一轮得到的集成学习器对每个样本的损失函数，于是有如果样本$x_i$在前一轮得到的集成分类器上分类错误那么$\beta_{ti}$就大；如果样本在前一轮得到的集成分类器上分类正确那么$\beta_{ti}$就小，因此$\beta_{ti}$完全可以当作更新样本权重的方法。于是据此我们得到了每一轮样本权重的更新思想：将样本在上一轮集成分类器上的指数损失函数作为本轮新的样本权值
按照Adaboost算法的思想，$f_t(x_i)$是按当前带权重的样本训练得到的个体学习器，因此自然其对应的加权指数损失函数就是最小的，即
$$
\sum_{i=1}^N\beta_{ti}\exp(-y_if_t(x_i))
$$
是最小的。显然在给定$\alpha_t\gt0$时，此式最小与$L_t$最小是等价的
于是综合以上就得出如下结论：以$\beta_{ti}$为第$t$轮的样本权重时，在该权重下的样本上训练得到的个体分类器$f_t(x)$就是使$L_t$最小的$f_t(x)$。这样就求解出$f_t(x)$，并且得到了每一轮样本权重的计算方法
再来求解$\alpha_t$，先对$L_t$做一些变换
$$
\begin{aligned}
L_t&=\sum_{i=1}^N\beta_{ti}\exp[-\alpha_ty_if_t(x_i)]\\
&=\sum_{y_i=f_t(x_i)}\beta_{ti}e^{-\alpha_t}+\sum_{y_i\ne f_t(x_i)}\beta_{ti}e^{\alpha_t}\\
&=e^{-\alpha_t}\sum_{i=1}^N\beta_{ti}-e^{-\alpha_t}\sum_{i=1}^N\beta_{ti}I(y_i\ne f_t(x_i))+e^{\alpha_t}\sum_{i=1}^N\beta_{ti}I(y_i\ne f_t(x_i))\\
&=(e^{\alpha_t}-e^{-\alpha_t})\sum_{i=1}^N\beta_{ti}I(y_i\ne f_t(x_i))+e^{-\alpha_t}\sum_{i=1}^N\beta_{ti}
\end{aligned}
$$
求导并令导数为零可得
$$
\begin{gathered}
\frac{\partial L_t}{\partial\alpha_t}=(e^{\alpha_t}+e^{-\alpha_t})\sum_{i=1}^N\beta_{ti}I(y_i\ne f_t(x_i))-e^{-\alpha_t}\sum_{i=1}^N\beta_{ti}=0\\
\Rightarrow \frac{e^{-\alpha_t}}{e^{\alpha_t}+e^{-\alpha_t}}=\frac{\sum\limits_{i=1}^N\beta_{ti}I(y_i\ne f_t(x_i))}{\sum\limits_{i=1}^N\beta_{ti}}
\end{gathered}
$$
等式右边就相当于以$\beta_{ti}$为样本权重时的分类误差率，令其为$e_t$
$$
e_t=\frac{\sum\limits_{i=1}^N\beta_{ti}I(y_i\ne f_t(x_i))}{\sum\limits_{i=1}^N\beta_{ti}}
$$
则可求得
$$
\alpha_t={1\over2}\ln\frac{1-e_t}{e_t}
$$
这样就得到了第$t$轮弱分类器的权重计算公式
下面再看样本权重的更新
$$
\begin{aligned}
\beta_{ti}&=\exp[-y_iF_{t-1}(x_i)]\\
&=\exp[-y_i(F_{t-2}(x_i)+\alpha_{t-1}f_{t-1}(x_i))]\\
&=\exp[-y_iF_{t-2}(x_i)]\exp(-\alpha_{t-1}y_if_{t-1}(x_i))\\
&=\beta_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]
\end{aligned}
$$
因为样本的权重是为了改变样本的概率分布，所以要对权值做归一化才能作为概率值，于是有
$$
\begin{aligned}
w_{ti}&=\frac{\beta_{ti}}{\sum\limits_{i=1}^N\beta_{ti}}\\
&=\frac{\beta_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}{\sum\limits_{i=1}^N\beta_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}\\
&=\frac{\cfrac{1}{\sum_{j=1}^N\beta_{(t-1)j}}\beta_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}{\cfrac{1}{\sum_{j=1}^N\beta_{(t-1)j}}\sum\limits_{i=1}^N\beta_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}\\
&=\frac{w_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}{\sum\limits_{i=1}^Nw_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}
\end{aligned}
$$
令$Z_t=\sum\limits_{i=1}^Nw_{ti}\exp[-\alpha_ty_if_t(x_i)]$为归一化因子，则样本权重更新公式为
$$
w_{ti}=\frac{w_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]}{Z_{t-1}}
$$
初始值在$t=1$时$w_{1i}=\cfrac{1}{N}$
分类误差率$e_t$可以改写为
$$
e_t=\sum\limits_{i=1}^N\frac{\beta_{ti}}{\sum_{j=1}^N\beta_{tj}}I(y_i\ne f_t(x_i))=\sum\limits_{i=1}^Nw_{ti}I(y_i\ne f_t(x_i))
$$
下面证明前向分步算法的有效性，即每一步优化局部损失函数$L_t$就是在优化最终的损失函数$L$。由样本权重更新公式可得
$$
w_{ti}Z_{t-1}=w_{(t-1)i}\exp[-\alpha_{t-1}y_if_{t-1}(x_i)]
$$
对损失函数$L$做一些变换，并将上式带入可得
$$
\begin{aligned}
L&={1\over N}\sum_{i=1}^N\prod_{t=1}^T\exp[-\alpha_ty_if_t(x)]\\
&=\sum_{i=1}^Nw_{1i}\prod_{t=1}^T\exp[-\alpha_ty_if_t(x)]\\
&=\sum_{i=1}^Nw_{1i}\exp[-\alpha_1y_if_1(x)]\prod_{t=2}^T\exp[-\alpha_ty_if_t(x)]\\
&=\sum_{i=1}^Nw_{2i}Z_1\prod_{t=2}^T\exp[-\alpha_ty_if_t(x)]\\
&=Z_1\sum_{i=1}^Nw_{2i}\exp[-\alpha_2y_if_2(x)]\prod_{t=3}^T\exp[-\alpha_ty_if_t(x)]\\
&=\cdots\\
&=\prod_{t=1}^TZ_t
\end{aligned}
$$
显然，要优化损失函数$L$，就是最优化$Z_t$。观察局部损失函数$L_t$和归一化因子$Z_t$，可以发现二者表达式只相差对样本权重的归一化，所以局部损失函数$L_t$和归一化因子$Z_t$其实是等价的，因此最优化$Z_t$等价于最优化$L_t$，于是优化损失函数$L$实际就是在每一轮局部优化$L_t$，因此Adaboost的前向分步算法是有效的
### 算法步骤
根据算法原理的推导总结出二分类Adaboost算法的步骤
> 输入：训练数据$D=\{(x_i,y_i)\}_{i=1}^N$，其中$y_i\in\{-1,+1\}$；个体分类器个数$T$
> 输出：分类器$g(x)=\text{sign}(f(x))$
> (1)初始化样本权重$W(1)=(w_{11},w_{12},\cdots,w_{1N})$，$w_{1i}=\cfrac{1}{N}$
> (2)对$t=1,2,\cdots,T$进行如下迭代
> 1. 根据带权重$W(t)$的训练数据进行训练得到个体分类器$f_t(x)$
> 2. 计算个体分类器$f_t(x)$在带权重$W(t)$的训练数据上的分类误差率
> $$
> e_t=\sum_{i=1}^Nw_{ti}I(y_i\ne f_t(x_i))
> $$
> 3. 利用分类误差率计算$f_t(x)$的权重
> $$
> \alpha_t=\frac{1}{2}\ln\frac{1-e_t}{e_t}
> $$
> 4. 更新样本权重$W(t+1)=(w_{(t+1)1},w_{(t+1)2},\cdots,w_{(t+1)N})$
> $$
> w_{(t+1)i}=\frac{w_{ti}\exp[-\alpha_ty_if_t(x_i)]}{Z_t}
> $$
> 其中$Z_t=\sum\limits_{i=1}^Nw_{ti}\exp[-\alpha_ty_if_t(x_i)]$，为归一化因子
> 
> (3)构建基分类器线性组合
> $$
> f(x)=\sum_{t=1}^T\alpha_tf_t(x)
> $$
> 最终分类器为
> $$
> g(x)=\text{sign}(f(x))=\text{sign}\left(\sum_{t=1}^T\alpha_tf_t(x)\right)
> $$