# 模型融合
模型融合是在基学习器的基础上对学习结果进一步处理，将多个基学习器的结果结合起来得到最终预测模型的方法
广义上讲，boosting、bagging等集成模型都可以属于模型融合，除此以外还有投票、stacking、blending等方法
## 投票模型
就是将多个模型对样本的预测结果进行投票或者平均作为最终结果
## stacking
### 基本方法
stacking是一种分层模型集成框架。以两层为例，第一层由多个基学习器组成，其输入为原始训练集，第二层的模型则是以第一层基学习器的输出作为特征加入训练集进行再训练，从而得到完整的stacking模型
以5折划分为例，我们将原始训练集分为5折，分别记为fold1、fold2、fold3、fold4和fold5。此时我们使用fold2-fold5的数据来训练基模型1，并对fold1进行预测，该预测值即作为基模型1对fold1生成的元特征；同样地，使用fold1、fold3-fold5的数据来训练基模型1，并对fold2进行预测，该预测值即作为基模型1对fold2生成的元特征；以此类推，得到基模型1对整个原始训练集生成的元特征。同样地，对其他基模型也采用相同的方法生成元特征，从而构成用于第二层模型（下记为元模型，meta model）训练的完整元特征集。对于测试集，我们可以在每次基模型训练好时预测，再将预测值做均值处理；也可以将基模型拟合全部的训练集之后再对测试集进行预测
需要注意的是，在生成第二层特征的时候，各个基模型要采用相同的Kfold，这样得到的元特征的每一折（对应于之前的K折划分）都将不会泄露进该折数据的目标值信息 ，从而尽可能的降低过拟合的风险。虽然如此，实际上我们得到的元特征还是存在一定程度上的信息泄露，比如我们在预测第二折的时候，是利用了第一折的目标值信息用于训练基模型的，也就是说第一折的目标值信息杂糅在对第二折进行预测的基模型里。但是，实践中，这种程度的信息泄露所造成的过拟合程度很小
### 特点
优点：
1. 提升性能
2. 将集成的知识迁移到到简单的分类器上
3. 自动化的大型集成策略可以通过添加正则项有效的对抗过拟合，而且并不需要太多的调参和特征选择
4. 这是目前提升机器学习效果最好的方法，或者说是最效率的方法
   
缺点：
1. 数据信息泄露，因为交叉验证会用预测的部分数据去训练另外一个交叉验证的样本
2. 实现复杂。计算复杂度很高

## blending
### 基本方法
Blending与Stacking大致相同，只是Blending的主要区别在于训练集不是通过K-Fold的CV策略来获得预测值从而生成第二阶段模型的特征，而是建立一个Holdout集，例如10%的训练数据，第二阶段的stacker模型就基于第一阶段模型对这10%训练数据的预测值进行拟合。其实就是把Stacking流程中的K-Fold CV 改成 HoldOut CV
### 特点
优点：
1. 实现简单
2. 避免信息泄露问题

缺点：
1. 使用了很少的数据
2. 容易过拟合，没有stacking稳健

## 总结
工业使用中这两种方法都很少直接使用，主要用于竞赛得到更好的效果，且两种方法的效果实际差距并不明显