# 前馈神经网络
又称为深度前馈网络、多层感知机，是最典型的深度学习模型
## 非线性扩展
对于线性模型，由于局限性比较大，为了适应更多非线性问题，需要非线性扩展。为了扩展到非线性，我们可以对样本$x$进行一个非线性变换$\phi(x)$，一般对于这个映射$\phi$有三种构造方法
1. 手动设计$\phi$：对于专门的问题用专门的$\phi$来进行非线性变换。这种方法扩展性较低，对于不同的问题都要重新设计，而且手动构造对于复杂问题难度很大
2. 核方法：运用核方法隐式定义$\phi$，拓展到高维空间甚至无限维。这种方法可扩展但是找到合适的核函数也较难，而且泛化效果往往不佳
3. 学习$\phi$：这就是深度学习采用的方法，给出一个模型$y=f(x;\boldsymbol{\theta},\boldsymbol{w})=\phi(x,\boldsymbol{\theta})^T\boldsymbol{w}$，然后去学习参数$\boldsymbol{\theta},\boldsymbol{w}$。这个方法放弃了训练问题的凸性，因此需要用梯度下降的方法来寻找最优

## 前馈神经网络
前馈神经网络就是将多个感知机连接起来组成的多层次网络结构，因为这种结构类似神经元的连接，因此每个感知机结点又被称为神经元。前馈神经网络中同一层的神经元没有相互连接，跨层的神经元也没有相互连接，只有相邻层的神经元有连接，而且每个神经元与其相邻层的每一个神经元都连接，形成一个有向无环图
![MLP](https://s1.ax1x.com/2020/07/24/UXLFL6.png)
如图所示初始层称为输入层，一般当作第零层；最后一层称为输出层，中间的层都称为隐藏层。当然输出层也可以有多个神经元。显然每个隐藏层以及输出层的神经元都相当于一个感知机，因此又被称为多层感知机，在前馈神经网络中信号由输入层向输出层单向传播。每个神经元都有一个激活函数，将它的输入作为激活函数的输入，计算得到的值作为神经元输出值
### 代价函数
对于深度神经网络，其代价函数大部分与普通的参数模型是相同的，但是因为神经网络的非线性，大多数的代价函数都变得非凸，因此并不能保证利用梯度下降法一定会得到全局收敛，因此初始值的设定、梯度的大小变得很重要
主要的代价函数有两种
#### 最大似然函数
大多数的现代神经网络都运用最大似然来进行训练，这意味着代价函数为负的对数似然，也即交叉熵损失函数。期望损失表示为
$$
J(\theta)=-E_{\boldsymbol{x},\boldsymbol{y}\sim\hat{p}_{data}}[\log p(\boldsymbol{y}|\boldsymbol{x})]
$$
使用最大似然来导出代价函数的好处是减轻了为每个模型设计代价函数的负担，对于每个模型是通用的
#### 条件统计量
有时候并不是学习一个分布$p(\boldsymbol{y}|\boldsymbol{x})$，而是学习给定$\boldsymbol{x}$时$\boldsymbol{y}$的某个条件统计量。比如用一个预测器$f(\boldsymbol{x};\boldsymbol{\theta})$来预测$\boldsymbol{y}$的均值，可以认为神经网络可以表示一大类函数中的任意一个函数$f$，这类函数不是形式相同的函数，这样就把代价函数看作了泛函，神经网络的学习目的是选择最优的函数，而不仅仅是选择参数。解决这个问题主要要用到变分法
### 输出单元
输出单元就是输出层的神经元，输出单元激活函数的选择与代价函数密切相关，比如在选择交叉熵损失函数时，最终输出层如何表示直接决定了交叉熵损失函数的形式。当然可以用作输出单元的激活函数一般也可以用作隐藏层的激活函数
假设前馈神经网络已经提供了一组定义为$\boldsymbol{h}=f(\boldsymbol{x};\boldsymbol{\theta})$的隐藏特征。输出层就是把这些隐藏特征转化为最终的输出，下面讨论几种激活函数
#### 线性单元
线性单元即简单的仿射变换，不具有非线性，因此被称为线性单元，形式如下
$$
\hat{y}=\boldsymbol{W}^\top\boldsymbol{h}+b
$$
线性输出层一般用于输出条件高斯分布的均值
$$
p(y|\boldsymbol{x})=N(y;\hat{y},\boldsymbol{\Sigma})
$$
最大化其对数似然等价于最小化均方误差
线性单元不会饱和，因此易于采用基于梯度的优化方法，甚至可以采用其他的优化方法
#### sigmoid单元
对于预测二值型变量的任务，即二分类任务，此时最大似然的定义是Bernoulli分布，因此激活函数就和Logistic Regression相同，为sigmoid函数
$$
\hat{y}=\frac{1}{1+\exp[-(\boldsymbol{W}^\top\boldsymbol{h}+b)]}
$$
一般也采用极大似然的方法学习参数，而且常用与logistic回归不同的另一种形式
$$
\begin{aligned}
J(\theta)&=-\log P(y|\boldsymbol{x})\\
&=-\log \text{sigmoid}[(2y-1)(\boldsymbol{W}^\top\boldsymbol{h}+b)]\\
&=\log[1+\exp(1-2y)(\boldsymbol{W}^\top\boldsymbol{h}+b))]
\end{aligned}
$$
一般把$\zeta(x)=\log(1+e^x)$形式的函数称为softplus函数。softplus函数只有在分类正确且$|\boldsymbol{W}^\top\boldsymbol{h}+b|$很大时才会趋于饱和，而在分类错误时不饱和，完全不会收缩梯度，因此很适合梯度下降法
#### softmax单元