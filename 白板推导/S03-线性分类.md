# S03-线性分类
## 背景简介
线性回归作为统计机器学习里的基本模型，根据对其不同的方面进行拓展出现了很多不同的模型
* 线性：线性回归拓展为非线性模型，又分为三种
  1. 属性非线性：线性回归的回归方程是一次的，非线性可变为高次，例如多项式回归
  2. 全局非线性：让整个回归方程转换为非线性方程，例如加入非线性激活函数，称之为线性分类
  3. 系数非线性：线性回归权重是不变化的，权值可变时就为非线性的，例如感知机、山景网络等
* 全局性：线性回归未对样本分层进行回归，实际可以划分为几块子样本分别建模，例如线性样条回归，决策树
* 数据未加工：线性回归的数据本身没有经过任何处理，如果经过特殊处理就可以变为其他模型，例如PCA、数据流行
## 线性分类概述
这一部分主要讨论线性分类，线性分类主要分为两种
1. 硬输出：即$y\in\{0,1\}$，输出为明确的类型。典型的有感知机、线性判别分析
2. 软输出：即$y\in[0,1]$，输出为一个概率值，也可表示为$P(y=0|X)=p,P(y=1|X)=1-p$。软输出又可分为两种
   * 判别模型：直接输出$P(Y|X)$，例如Logistic Regression
   * 生成模型：假定一个先验$P(Y)$，先求解似然$P(X|Y)$，再根据贝叶斯估计去计算$P(Y|X)$，类似于MAP。例如GDA(Gaussian Discriminant Analysis)
## 感知机
### 引入
给定样本$Data=\{(X_i,y_i)\}_{i=1}^N,X_i\in\mathbb{R}^p$，假设这个样本分为两类并可以被一个超平面分开，这种情况称为线性可分。二维情况下如图，红点和蓝点分别表示两类
![感知机](https://s1.ax1x.com/2020/04/15/JCOhvQ.png)
如果我们把两个类别分别用+1和-1来表示，于是可建立如下模型
$$
\begin{gathered}
f(X)=sign(W^TX),X\in\mathbb{R}^p,W\in\mathbb{R}^p\\
sign(x)=
\begin{cases}
+1,&x\ge0\\
-1,&x\lt0
\end{cases}
\end{gathered}
$$
我们的目的就是找到空间中一个超平面可将样本切分为两类。如图可看出，有些超平面并不能完全切分样本，会有样本分类错误，于是我们自然想到建立一个loss function为分类错误的样本数量
* 先来看对样本$X_i$分类错误怎么表示
$$
\begin{aligned}
&\begin{cases}
W^TX_i<0,y_i>0&实际类别为+1但是分类错误\\
W^TX_i>0,y_i<0&实际类别为-1但是分类错误
\end{cases}\\
&\Rightarrow 分类错误可表示为y_iW^TX_i\lt0
\end{aligned}
$$
于是loss function可以写为
$$
\begin{gathered}
L(W)=\sum_{i=1}^NI(-y_iW^TX_i)\\
I(x)=
\begin{cases}
1,&x\ge0\\
0,&x\lt0
\end{cases}
\end{gathered}
$$
这个损失函数虽然很简单很直观，但是它不连续不可导，数学性质较差，不方便我们求解，所以需要寻找其他loss function。
### 误分类驱动
感知机的目的是找到一个超平面将样本分为两类，如果一个超平面将所有点都分类错误，那么误分类的点到超平面的距离最大，在误分类点减少时，误分类的点到超平面的距离也会减少，找到满足条件的超平面时这个距离为零，因此可以选择将误分类的样本点到超平面的距离总和作为loss function
![感知机损失函数](https://s1.ax1x.com/2020/04/15/JCON4K.png)
设误分类样本集合为$D$，loss function为
$$
\begin{aligned}
L(W)&=\sum_{X_i\in D}\frac{W^TX_i}{||W||}\\
&=\sum_{X_i\in D}\frac{-y_iW^TX_i}{||W||}
\end{aligned}
$$
这个损失虽然可以使用得到结果，但是对其求导计算十分复杂，我们还可以进一步简化。考虑到我们是为了减少误分类的点，在最终找到超平面没有误分类点时损失函数为零，那么其实分母$||W||$对我们的目的是没有影响的，因为只取分子的情况下，在误分类点减少时函数值也是一直减小的直到零，因此可以采用如下loss function
$$
L(W)=\sum_{X_i\in D}-y_iW^TX_i
$$
### 学习算法
现在我们将问题简化为了最小化loss function。具体的方法是采用随机梯度下降法，梯度如下计算
$$
\nabla_WL(W)=-\sum_{X_i\in D}-y_iX_i
$$
那么针对某一个错误样本$(X_i,y_i)$，按如下方式对$W$进行更新
$$
W\leftarrow W+\eta y_iX_i\tag{*}
$$
其中$\eta\in(0,1]$为步长，也称为学习率(learning rate)
迭代算法如下
> (1)给定任意初始$W_0$
> (2)在训练数据集中选取数据($X_i$,$y_i$)
> (3)如果$y_iWX_i\le0$则根据式(*)更新权值
> (4)检查是否还有误分类样本点，若有转至(2)，若无算法结束

### 收敛性
上述算法一定能够成功结束吗，我们会不会进入一个无限循环迭代呢？即是不是在有限迭代次数内一定能够找到一个超平面将样本完全正确的的划分。下面证明算法的收敛性
由于数据集线性可分，则一定可以找到一个$W_{opt}^TX=0$将数据集完美划分，不妨设$||W_{opt}||=1$，对于所有的样本有
$$
y_iW_{opt}^TX_i>0
$$
那么有如下结论
$$
\begin{gathered}
\exist\gamma=\min_{1\le i\le N}(y_iW_{opt}^TX_i)\gt0\\
s.t.\quad y_iW_{opt}^TX_i\ge\gamma
\end{gathered}
$$
假设初始的$W_0=0$，对于迭代算法中的第$t$个误分类点，则$y^{(t)}(W^{(t)})^TX^{(t)}\lt0$，根据式$(*)$有
$$
\begin{gathered}
\begin{aligned}
W^{(t)}&=W^{(t-1)}+\eta y^{(t-1)}X^{(t-1)}\\
&=W^{(t-2)}+\eta y^{(t-2)}X^{(t-2)}+\eta y^{(t-1)}X^{(t-1)}\\
&=W^{(0)}+\eta y^{(0)}X^{(0)}+\eta y^{(1)}X^{(1)}+\cdots+\eta y^{(t-1)}X^{(t-1)}
\end{aligned}\\
\\
\begin{aligned}
\Rightarrow W_{opt}^TW^{(t)}&=W_{opt}^TW^{(0)}+\eta y^{(0)}W_{opt}^TX^{(0)}+\eta y^{(1)}W_{opt}^TX^{(1)}+\cdots+\eta y^{(t-1)}W_{opt}^TX^{(t-1)}\\
&\ge\eta\gamma+\ge\eta\gamma+\cdots+\eta\gamma\\
&=t\eta\gamma
\end{aligned}
\end{gathered}
$$
令$R=\max\limits_{1\le i\le N}||X_i||\gt 0$，则有
$$
\begin{aligned}
||W^{(t)}||^2&=||W^{(t-1)}||^2+2\eta y^{(t-1)}W^{(t-1)}\cdot X^{(t-1)}+\eta^2(y^{(t-1)})^2||X^{(t-1)}||^2\\
&=||W^{(t-1)}||^2+2\eta y^{(t-1)}(W^{(t-1)})^TX^{(t-1)}+\eta^2||X^{(t-1)}||^2\\
&\lt||W^{(t-1)}||^2+\eta^2||X^{(t-1)}||^2\\
&\lt||W^{(t-1)}||^2+\eta^2R^2\\
&\lt||W^{(0)}||^2+\eta^2R^2+\cdots+\eta^2R^2\\
&=t\eta^2R^2
\end{aligned}
$$
由以上推导最终可得
$$
\begin{gathered}
t\eta\gamma\le W_{opt}^TW^{(t)}\le||W_{opt}||\cdot||W^{(t)}||=||W^{(t)}||\lt\sqrt{t}\eta R\\
\Rightarrow t\lt\left({R\over\gamma}\right)^2
\end{gathered}
$$
因为在给定样本集后$R,\gamma$均为常数，则迭代次数$t$是有上界的，即数据集线性可分的情况下，在有限迭代次数内一定能够找到一个超平面将样本完全正确的的划分，感知机学习算法是收敛的。但当数据集不是线性可分时，感知机算法不收敛，迭代结果会发生震荡
### 几何解释
还有另外一种理解感知机的方式：对于空间中的超平面$W^TX=0$，其法向量为$\vec{W}$，那么对于一个样本点$X_i$有$W^TX_i=\vec{W}\cdot \vec{X_i}$，显然决定$W^TX_i$符号的关键就是向量$\vec{W}$与向量$\vec{X_i}$的夹角
![感知机对偶](https://s1.ax1x.com/2020/04/16/JFmpdI.png)
如图左边所示，假设红点为正类蓝点为负类。在$W^{(t)}$时，我们随机找到一个误分类点如负类点M，从图中容易看到M本应该分类为-1，即$(W^{(t)})^TX_m=\vec{W}^{(t)}\cdot \vec{X}_m\lt0$，那么$\vec{W}^{(t)}$与$\vec{X}_m$夹角应该大于90度，于是我们要想办法让$\vec{W}^{(t+1)}$远离$\vec{X}_m$，增大夹角。于是我们令$\vec{W}^{(t+1)}=\vec{W}^{(t)}-\vec{X}_m$即可达到目的，又$y_m=-1$则
$$
\begin{aligned}
\vec{W}^{(t+1)}&=\vec{W}^{(t)}-\vec{X}_m\\
&=W^{(t)}+y_mX_m
\end{aligned}
$$
如图右边所示，在$W^{(t+1)}$时仍有误分类点，因此需要继续优化。我们随机找到一个误分类点如正类点N，从图中容易看到N本应该分类为+1，即$(W^{(t+1)})^TX_n=\vec{W}^{(t+1)}\cdot \vec{X}_n\gt0$，那么$\vec{W}^{(t+1)}$与$\vec{X}_n$夹角应该小于90度，于是我们要想办法让$\vec{W}^{(t+2)}$接近$\vec{X}_n$，减小夹角。于是我们令$\vec{W}^{(t+2)}=\vec{W}^{(t+1)}+\vec{X}_n$即可达到目的，又$y_n=+1$则
$$
\begin{aligned}
\vec{W}^{(t+2)}&=\vec{W}^{(t+1)}+\vec{X}_n\\
&=W^{(t+1)}+y_nX_n
\end{aligned}
$$
于是可以对于一个误分类点$(X_i,y_i)$统一为如下迭代公式
$$
W\leftarrow W+y_iX_i
$$
实际在迭代过程中不用每次加上整个向量$y_i\vec{X}_i$，可以令$\eta\in(0,1]$，每次迭代加上向量$\eta y_i\vec{X}_i$，即
$$
W\leftarrow W+\eta y_iX_i
$$
这个形式与感知机算法等价

## 线性判别分析(LDA)
设二分类样本$D=\{(X_i,y_i)\}_{i=1}^N,X_i\in\mathbb{R}^p,y_i\in\{-1,+1\}$
$$
\begin{gathered}
X=\begin{pmatrix}X_1&X_2&\cdots&X_N\end{pmatrix}\\
Y=\begin{pmatrix}y_1&y_2&\cdots&y_N\end{pmatrix}^T
\end{gathered}
$$
设$D_1=\{X_i|y_i=+1\}$为正类样本的集合，$D_2=\{X_i|y_i=-1\}$为负类样本的集合，$|D_1|=N_1,|D_2|=N_2,N=N_1+N_2$
### 基本思想
对于二分类的问题，在高维空间中直接找样本的分界比较困难，因此考虑将高维的数据点降维，映射到一条一维的直线上，在直线上可以很容易的找到一个threshold来区分不同类别。但是映射的过程会遇到一些问题，以二维为例如图
![线性判别分析](https://s1.ax1x.com/2020/04/16/JFs2Yq.png)
可以映射的直线有很多，比如图中的$L_1,L_2$。但是很明显将样本映射到$L_2$并不能很好的区分两类，但是将样本映射到$L_1$可以达到很好的效果
### 构造目标函数
要想办法解决上述问题。我们从图中可看到，对于效果更好的$L_1$，同一类的样本点在其上投影点分布密集，但是不同类别在其上投影点分散得很开。于是我们对于这条映射直线的要求即：类内的度量小，类间的度量大
一般地，设映射的直线为$W^TX=0,||W||=1$，我们先定义一些量
1. 投影前的量
   * 正样本的均值$\overline{X_+}=\sum\limits_{i=1}^{N_1}X_i$，负样本的均值$\overline{X_-}=\sum\limits_{i=1}^{N_2}X_i$
   * 正负样本的协方差矩阵分别为$\Sigma_+,\Sigma_-$
2. 投影后的量
   * 样本$X_i$在直线$W^TX=0$上的投影为$\mu_i=W^TX_i$
   * 所有样本的投影均值为
   $$
   \mu=\frac{\sum_{i=1}^{N}W^TX_i}{N}
   $$
   * 正负样本点的投影均值分别为
   $$
   \begin{aligned}
   \mu_+=\sum_{X_i\in D_1}\mu_i=\frac{\sum_{i=1}^{N_1}W^TX_i}{N_1}\\
   \mu_-=\sum_{X_i\in D_2}\mu_i=\frac{\sum_{i=1}^{N_2}W^TX_i}{N_2}
   \end{aligned}
   $$
#### 类内度量
我们可以用样本点映射到直线后类内投影点的方差作为类内度量
* 对于正类和负类分别有
$$
\begin{aligned}
S_+&=\frac{\sum\limits_{i=1}^{N_1}(\mu_i-\mu_+)^2}{N_1}=\frac{\sum\limits_{i=1}^{N_1}(\mu_i-\mu_+)(\mu_i-\mu_+)^T}{N_1}\\
&={1\over N_1}\sum_{i=1}^{N_1}\left(W^TX_i-{1\over N_1}\sum_{j=1}^{N_1}W^TX_j\right)\left(W^TX_i-{1\over N_1}\sum_{j=1}^{N_1}W^TX_j\right)^T\\
&={1\over N_1}\sum_{i=1}^{N_1}\left[W^T\left(X_i-{1\over N_1}\sum_{j=1}^{N_1}X_j\right)\right]\left[W^T\left(X_i-{1\over N_1}\sum_{j=1}^{N_1}X_j\right)\right]^T\\
&={1\over N_1}\sum_{i=1}^{N_1}W^T(X_i-\overline{X_+})(X_i-\overline{X_+})^TW\\
&=W^T\left[{1\over N_1}\sum_{i=1}^{N_1}(X_i-\overline{X_+})(X_i-\overline{X_+})^T\right]W\\
&=W^T\Sigma_+W\\
\\
S_-&=W^T\Sigma_-W
\end{aligned}
$$
则总体的类内度量为
$$
\begin{aligned}
S&=S_++S_-\\
&=W^T(\Sigma_++\Sigma_-)W
\end{aligned}
$$
#### 类间度量
我们可以用样本点映射到直线后正类的投影均值与负类投影均值间的距离作为类间度量
$$
\begin{aligned}
d^2&=||\mu_+-\mu_-||^2=(\mu_+-\mu_-)^2\\
&=\left(\frac{\sum_{i=1}^{N_1}W^TX_i}{N_1}-\frac{\sum_{i=1}^{N_2}W^TX_i}{N_2}\right)^2\\
&=[W^T(\overline{X_+}-\overline{X_-})]^2\\
&=W^T(\overline{X_+}-\overline{X_-})(\overline{X_+}-\overline{X_-})^TW
\end{aligned}
$$
#### 目标函数
我们的目的是类间距离尽量大而类内的距离尽量小，那么根据类内度量和类间度量即可构造一个目标函数
$$
J(W)=\frac{W^T(\overline{X_+}-\overline{X_-})(\overline{X_+}-\overline{X_-})^TW}{W^T(\Sigma_++\Sigma_-)W}
$$
### 求解方法
令$S_w=\Sigma_++\Sigma_-$，称为类内散度矩阵(within-class scatter matrix)
令$S_b=(\overline{X_+}-\overline{X_-})(\overline{X_+}-\overline{X_-})^T$，称为类间散度矩阵(between-class scatter matrix)
则有
$$
\begin{gathered}
J(W)=\frac{W^TS_bW}{W^TS_wW}=(W^TS_bW)(W^TS_wW)^{-1}\\
\frac{\partial J(W)}{\partial W}=2S_bW(W^TS_wW)^{-1}+(W^TS_bW)[-2(W^TS_wW)^{-2}S_wW]=0\\
\Rightarrow S_bW(W^TS_wW)-(W^TS_bW)S_wW=0\\
\Rightarrow S_wW=\frac{W^TS_wW}{W^TS_bW}S_bW\\
\Rightarrow W=\frac{W^TS_wW}{W^TS_bW}S_w^{-1}S_bW
\end{gathered}
$$
因为确定这条直线只需要确定法向量的方向即$W$的方向即可，因此不需要确定$W$长度，那么可以用如下方法求解
$$
\begin{gathered}
\begin{aligned}
W&\propto S_w^{-1}S_bW\\
&\propto S_w^{-1}(\overline{X_+}-\overline{X_-})(\overline{X_+}-\overline{X_-})^TW\\
&\propto S_w^{-1}(\overline{X_+}-\overline{X_-})
\end{aligned}\\
\Rightarrow W=S_w^{-1}(\overline{X_+}-\overline{X_-})
\end{gathered}
$$
