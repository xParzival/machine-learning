# S03P02-线性判别分析
## 线性判别分析(LDA)
设二分类样本$D=\{(X_i,y_i)\}_{i=1}^N,X_i\in\mathbb{R}^p,y_i\in\{-1,+1\}$
$$
\begin{gathered}
X=\begin{pmatrix}X_1&X_2&\cdots&X_N\end{pmatrix}\\
Y=\begin{pmatrix}y_1&y_2&\cdots&y_N\end{pmatrix}^T
\end{gathered}
$$
设$D_1=\{X_i|y_i=+1\}$为正类样本的集合，$D_2=\{X_i|y_i=-1\}$为负类样本的集合，$|D_1|=N_1,|D_2|=N_2,N=N_1+N_2$
### 基本思想
对于二分类的问题，在高维空间中直接找样本的分界比较困难，因此考虑将高维的数据点降维，映射到一条一维的直线上，在直线上可以很容易的找到一个threshold来区分不同类别。但是映射的过程会遇到一些问题，以二维为例如图
![线性判别分析](https://s1.ax1x.com/2020/04/16/JFs2Yq.png)
可以映射的直线有很多，比如图中的$L_1,L_2$。但是很明显将样本映射到$L_2$并不能很好的区分两类，但是将样本映射到$L_1$可以达到很好的效果
### 构造目标函数
要想办法解决上述问题。我们从图中可看到，对于效果更好的$L_1$，同一类的样本点在其上投影点分布密集，但是不同类别在其上投影点分散得很开。于是我们对于这条映射直线的要求即：类内的度量小，类间的度量大
一般地，设映射的直线为$W^TX=0,||W||=1$，我们先定义一些量
1. 投影前的量
   * 正样本的均值$\overline{X_+}=\sum\limits_{i=1}^{N_1}X_i$，负样本的均值$\overline{X_-}=\sum\limits_{i=1}^{N_2}X_i$
   * 正负样本的协方差矩阵分别为$\Sigma_+,\Sigma_-$
2. 投影后的量
   * 样本$X_i$在直线$W^TX=0$上的投影为$\mu_i=W^TX_i$
   * 所有样本的投影均值为
   $$
   \mu=\frac{\sum_{i=1}^{N}W^TX_i}{N}
   $$
   * 正负样本点的投影均值分别为
   $$
   \begin{aligned}
   \mu_+=\sum_{X_i\in D_1}\mu_i=\frac{\sum_{i=1}^{N_1}W^TX_i}{N_1}\\
   \mu_-=\sum_{X_i\in D_2}\mu_i=\frac{\sum_{i=1}^{N_2}W^TX_i}{N_2}
   \end{aligned}
   $$
#### 类内度量
我们可以用样本点映射到直线后类内投影点的方差作为类内度量
* 对于正类和负类分别有
$$
\begin{aligned}
S_+&=\frac{\sum\limits_{i=1}^{N_1}(\mu_i-\mu_+)^2}{N_1}=\frac{\sum\limits_{i=1}^{N_1}(\mu_i-\mu_+)(\mu_i-\mu_+)^T}{N_1}\\
&={1\over N_1}\sum_{i=1}^{N_1}\left(W^TX_i-{1\over N_1}\sum_{j=1}^{N_1}W^TX_j\right)\left(W^TX_i-{1\over N_1}\sum_{j=1}^{N_1}W^TX_j\right)^T\\
&={1\over N_1}\sum_{i=1}^{N_1}\left[W^T\left(X_i-{1\over N_1}\sum_{j=1}^{N_1}X_j\right)\right]\left[W^T\left(X_i-{1\over N_1}\sum_{j=1}^{N_1}X_j\right)\right]^T\\
&={1\over N_1}\sum_{i=1}^{N_1}W^T(X_i-\overline{X_+})(X_i-\overline{X_+})^TW\\
&=W^T\left[{1\over N_1}\sum_{i=1}^{N_1}(X_i-\overline{X_+})(X_i-\overline{X_+})^T\right]W\\
&=W^T\Sigma_+W\\
\\
S_-&=W^T\Sigma_-W
\end{aligned}
$$
则总体的类内度量为
$$
\begin{aligned}
S&=S_++S_-\\
&=W^T(\Sigma_++\Sigma_-)W
\end{aligned}
$$
#### 类间度量
我们可以用样本点映射到直线后正类的投影均值与负类投影均值间的距离作为类间度量
$$
\begin{aligned}
d^2&=||\mu_+-\mu_-||^2=(\mu_+-\mu_-)^2\\
&=\left(\frac{\sum_{i=1}^{N_1}W^TX_i}{N_1}-\frac{\sum_{i=1}^{N_2}W^TX_i}{N_2}\right)^2\\
&=[W^T(\overline{X_+}-\overline{X_-})]^2\\
&=W^T(\overline{X_+}-\overline{X_-})(\overline{X_+}-\overline{X_-})^TW
\end{aligned}
$$
#### 目标函数
我们的目的是类间距离尽量大而类内的距离尽量小，那么根据类内度量和类间度量即可构造一个目标函数
$$
J(W)=\frac{W^T(\overline{X_+}-\overline{X_-})(\overline{X_+}-\overline{X_-})^TW}{W^T(\Sigma_++\Sigma_-)W}
$$
### 求解方法
令$S_w=\Sigma_++\Sigma_-$，称为类内散度矩阵(within-class scatter matrix)
令$S_b=(\overline{X_+}-\overline{X_-})(\overline{X_+}-\overline{X_-})^T$，称为类间散度矩阵(between-class scatter matrix)
则有
$$
\begin{gathered}
J(W)=\frac{W^TS_bW}{W^TS_wW}=(W^TS_bW)(W^TS_wW)^{-1}\\
\frac{\partial J(W)}{\partial W}=2S_bW(W^TS_wW)^{-1}+(W^TS_bW)[-2(W^TS_wW)^{-2}S_wW]=0\\
\Rightarrow S_bW(W^TS_wW)-(W^TS_bW)S_wW=0\\
\Rightarrow S_wW=\frac{W^TS_wW}{W^TS_bW}S_bW\\
\Rightarrow W=\frac{W^TS_wW}{W^TS_bW}S_w^{-1}S_bW
\end{gathered}
$$
因为确定这条直线只需要确定法向量的方向即$W$的方向即可，因此不需要确定$W$长度，那么可以用如下方法求解
$$
\begin{gathered}
\begin{aligned}
W&\propto S_w^{-1}S_bW\\
&\propto S_w^{-1}(\overline{X_+}-\overline{X_-})(\overline{X_+}-\overline{X_-})^TW\\
&\propto S_w^{-1}(\overline{X_+}-\overline{X_-})
\end{aligned}\\
\Rightarrow W=S_w^{-1}(\overline{X_+}-\overline{X_-})
\end{gathered}
$$
