# S03P05-朴素贝叶斯
## 朴素贝叶斯法
设有K类的分类样本$D=\{(X_i,y_i)\}_{i=1}^N,X_i\in\mathbb{R}^p,y_i\in\{1,2,\cdots,K\}$
$$
\begin{gathered}
X=\begin{pmatrix}X_1&X_2&\cdots&X_N\end{pmatrix}\\
Y=\begin{pmatrix}y_1&y_2&\cdots&y_N\end{pmatrix}^T
\end{gathered}
$$
设第k类的样本集合为$D_k$且$|D_k|=N_k$，$\sum_{k=1}^KN_i=N$
### 朴素贝叶斯假设
根据贝叶斯定理，对于一个新数据$X=(X^{(1)},X^{(2)},\cdots,X^{(p)})^T$
$$
P(y|X)=\frac{P(X,y)}{P(X)}=\frac{P(X|y)P(y)}{P(X)}
$$
我们的目的是从样本学习到$P(y=k|X)$，属于哪一类的概率最大，就将样本归于哪一类。根据上式，判断$P(y=k|X)$大小实际只跟分母$P(X|y=k)P(y=k)$有关，因此只需要学习$P(X|y=k)$和$P(y=k)$即可
一般假设$y$服从分类分布(Categorical Distribution)即
$$
P(y=k)=\lambda_k\quad\sum_{k=1}^K\lambda_k=1
$$
对于$P(X|y=k)$
$$
\begin{aligned}
P(X|y=k)&=P(x_1,x_2,\cdots,x_p|y=k)\\
&=P(x_1|y=k)P(x_2,\cdots,x_p|x_1,y=k)\\
&=P(x_1|y=k)P(x_2|x_p,y=k)\cdots P(x_p|x_1,x_2,\cdots,x_{p-1},y=k)
\end{aligned}
$$
如果$x_j$即第j维可能取值有$S_j$个，则理论上$P(X|y=k)$需要的参数有$K\prod\limits_{j=1}^pS_j$个，这显然是无法计算的，所以需要进行简化。于是我们假设随机变量$X$的每一维都是条件独立的，称为朴素贝叶斯假设，又叫条件独立性假设
$$
P(X|y=k)=\prod_{j=1}^pP(x_j|y=k)
$$
最终分类器可表示为
$$
\hat{y}=\arg\max_y P(X|y=k)\prod_{j=1}^pP(x_j|y=k)
$$
### 极大似然估计
1. 求$P(y)$，即$\lambda_k$
   令对数似然$L(\lambda)=\log P(Y)$
   $$
   \begin{aligned}
   L(\lambda)&=\log\prod_{k=1}^K[P(y=k)]^{N_k}\\
   &=\sum_{k=1}^KN_k\log\lambda_k
   \end{aligned}
   $$
   于是转化为如下优化问题
   $$
   \begin{gathered}
   \max_\lambda L(\lambda)=\max_\lambda\sum_{k=1}^KN_k\log\lambda_k\\
   s.t.\quad\sum_{k=1}^K\lambda_k=1       
   \end{gathered}
   $$
   利用拉格朗日乘子法即可求得
   $$
   \lambda_k=\frac{N_k}{N}
   $$
   这个式子表明，实际就是用样本中每一类出现的频率来估计类的分布
2. 求$P(X|y=k)$
   此时有两种情况，即$X$是离散随机变量还是连续随机变量
   * $X$是离散随机变量
     假设$X$的每一维都是Categorical Distribution，且第j维的取值集合为$\{m_{j1},m_{j2},\cdots,m_{jS_j}\}$。令第$k$类样本第$j$维取值为$m_{jl}$的样本数为$N_{jl|k}$，则$\sum\limits_{l=1}^{S_j}N_{jl|k}=N_k$，对数似然函数为$L(\lambda)=\log P(X|y=k)$
     $$
     \begin{gathered}
     P(x_j=m_{jl}|y=k)=\lambda_{jl|k}\quad\sum_{l=1}^{S_j}\lambda_{jl|k}=1\\
     \begin{aligned}
     L(\lambda)&=\log\prod_{j=1}^pP(x_j|y=k)\\
     &=\sum_{j=1}^p\log P(x_j|y=k)\\
     &=\sum_{j=1}^p\log\left(\prod_{l=1}^{S_j}[P(x_j=m_{jl}|y=k)]^{N_{jl|k}}\right)\\
     &=\sum_{j=1}^p\sum_{l=1}^{S_j}N_{jl|k}\log\lambda_{jl|k}
     \end{aligned}
     \end{gathered}
     $$
     于是转化为如下优化问题
     $$
     \begin{gathered}
     \max_\lambda L(\lambda)=\max_\lambda\sum_{j=1}^p\sum_{l=1}^{S_j}N_{jl|k}\log\lambda_{jl|k}\\
     s.t.\quad\sum_{l=1}^{S_j}\lambda_{jl|k}=1,j=1,2,\cdots,p
     \end{gathered}
     $$
     利用拉格朗日乘子法即可求得
     $$
     \lambda_{jl|k}=\frac{N_{jl|k}}{N_k}
     $$
     这个式子表明，实际就是用指定类的样本中该维取值出现的频率来估计该维的分布
   * $X$是连续随机变量
     假设$X$的每一维都是Gaussian Distribution，再按与离散相同的方法求解即可
### 贝叶斯估计
在使用极大似然估计法的时候，可能会出现样本中某一类的数量为零的情况，于是可以用贝叶斯估计的方法，具体结果如下
$$
\begin{gathered}
P(y=k)=\frac{N_k+\lambda}{N+K\lambda}\\
\\
P(x_j=m_{jl}|y=k)=\frac{N_{jl|k}+\lambda}{N_k+S_j\lambda}
\end{gathered}
$$
其中$\lambda\ge0$，这个结果等价于在随机变量各个取值的频数上加上一个$\lambda$
$\lambda=0$时即极大似然估计，$\lambda=1$时称为拉普拉斯平滑