# S04P01-主成分分析
## 主成分分析(PCA)
考虑一个问题：正交属性空间中的样本点，如何用一个超平面映射对所有样本进行恰当的表达？
![PCA](https://s1.ax1x.com/2020/04/21/JGkZZR.png)
以二维为例如图，$u_1$，$u_2$分别为映射为新的平面后的坐标轴方向，显然$u_1$方向上样本的新坐标分布更分散更均匀，而$u_2$方向上样本新坐标趋于相同，那么我们可以忽略$u_2$方向只用$u_1$方向，这样就达到了降维的目的。在高维空间中可能不止一个这样的方向，我们将这样的方向称为主成分。实际上在我们还希望将样本映射到超平面的代价是最小的，那么还应该满足原始空间的样本点到映射超平面的距离最短。对应到图中，如果降维到$u_1$方向，那么尽量使黑点和红点间的距离小
综上得到两个基本原则：
* 最大可分性：样本点在这个新坐标系某方向的投影尽量分开
* 最近重构性：样本点到这个新坐标系距离足够近

从上面的分析我们知道PCA实际上是对原始的高维空间样本做一个坐标变换到低维空间，并且这个低维空间满足上述两个基本原则，从而达到降维的目的
给定一个的样本$D=\{X_i\}_{i=1}^N,X_i\in\mathbb{R}^p$
$$
X=\begin{pmatrix}X_1&X_2&\cdots&X_N\end{pmatrix}^T_{N\times p}
$$
$\mu$为样本的均值向量，$\Sigma$为样本的协方差矩阵
$$
\begin{gathered}
\mu=\sum_{i=1}^NX_i\\
\Sigma={1\over N}\sum_{i=1}^N(X_i-\mu)(X_i-\mu)^T
\end{gathered}
$$
### 最大可分性
如果希望样本点在新坐标系主成分的投影最大，显然就是要求样本在主成分方向投影点的方差最大，所以又称为”最大投影方差“
若将均值向量$\mu$所在的点作为坐标原点即将数据中心化，设要在某个主成分方向上的基向量在样本原始空间表示为$W=(w_1,w_2,\cdots,w_p)^T$，$||W||^2=W^TW=1$。则在该方向投影后的方差为
$$
\begin{aligned}
S&={1\over N}\sum_{i=1}^N[W^T(X_i-\mu)]^2\\
&={1\over N}\sum_{i=1}^NW^T(X_i-\mu)(X_i-\mu)^TW\\
&=W^T\left({1\over N}\sum_{i=1}^N(X_i-\mu)(X_i-\mu)^T\right)W\\
&=W^T\Sigma W
\end{aligned}
$$
于是转化为如下优化问题
$$
\begin{gathered}
\max_WW^T\Sigma W\\
s.t.\quad W^TW=1
\end{gathered}
$$
利用拉格朗日乘子法令$L=W^T\Sigma W+\lambda(1-W^TW)$
$$
\begin{gathered}
\frac{\partial L}{\partial W}=2\Sigma W-2\lambda W=0\\
\Rightarrow\Sigma W=\lambda W
\end{gathered}
$$
上式即可看出$\lambda$为协方差矩阵$\Sigma$的特征值，$W$为$\Sigma$的特征向量，于是对$\Sigma$进行特征值分解即可。将上式带入方差得
$$
S=\lambda W^TW=\lambda
$$
那么使得方差最大的主成分即$\Sigma$最大的特征值对应的特征向量，方差其次大的为第二大的特征值对应的特征向量。而矩阵的特征向量是相互正交的，可作为一组正交基，假设我们最终想要从p维降至q维，那么降维后的坐标系基向量组即为$\Sigma$的特征值从大到小排列后对应的前q个特征向量组
### 最近重构性
如果希望样本点重构到新坐标系的代价最小，就是要求样本点到新坐标系的距离最近，所以又称为”最小重构距离“
对于一个样本点$X_i$，设其在新的q维坐标系中为$\hat{X_i}$。令$W=(W_1,W_2,\cdots,W_q,W_{q+1},\cdots,W_p)$，其中$W_j$为变换坐标后每一维的基向量，$||W_j||=W_j^TW_j$。将样本用新基向量表示出来则有
$$
\begin{aligned}
X_i&=\sum\limits_{j=1}^pW_j^T(X_i-\mu)W_j\\
\hat{X_i}&=\sum\limits_{j=1}^qW_j^T(X_i-\mu)W_j
\end{aligned}
$$
原始样本点和变换后样本点的距离为$||X_i-\hat{X_i}||^2$，那么平均重构距离为$d={1\over N}\sum\limits_{i=1}^N||X_i-\hat{X_i}||^2$
$$
\begin{aligned}
d&={1\over N}\sum_{i=1}^N||X_i-\hat{X_i}||^2\\
&={1\over N}\sum_{i=1}^N\left(\sum_{j=1}^pW_j^T(X_i-\mu)W_j-\sum\limits_{j=1}^qW_j^T(X_i-\mu)W_j\right)^2\\
&={1\over N}\sum_{i=1}^N\left(\sum_{j=q+1}^pW_j^T(X_i-\mu)W_j\right)^2\\
&={1\over N}\sum_{i=1}^N\sum_{j=q+1}^p(W_j^T(X_i-\mu))^2\\
&=\sum_{j=q+1}^p{1\over N}\sum_{i=1}^NW_j^T(X_i-\mu)(X_i-\mu)^TW_j\\
&=\sum_{j=q+1}^pW_j^T\left({1\over N}\sum_{i=1}^N(X_i-\mu)(X_i-\mu)^T\right)W_j\\
&=\sum_{j=q+1}^pW_j^T\Sigma W_j
\end{aligned}
$$
于是转化为如下优化问题
$$
\begin{gathered}
\min_W\sum_{j=q+1}^pW_j^T\Sigma W_j\\
s.t.\quad W_j^TW_j=1
\end{gathered}
$$
根据拉格朗日乘子法令$L=\sum_{j=q+1}^pW_j^T\Sigma W_j+\sum_{j=q+1}^p\lambda_j(1-W_j^TW_j)$
$$
\begin{gathered}
\frac{\partial L}{\partial W_j}=2\Sigma W_j-2\lambda_j W_j=0\\
\Rightarrow\Sigma W_j=\lambda_jW_j
\end{gathered}
$$
显然$\lambda_j$为$\Sigma$的特征值，$W_j$为对应的特征向量，代入原式
$$
d=\sum_{j=q+1}^p\lambda_jW_j^TW_j=\sum_{j=q+1}^p\lambda_j
$$
若要使其最小化，就要取$\Sigma$最小的$p-q$个特征值。因此将数据从p维降至q维需先求$\Sigma$的特征值，为了满足最小重构距离，必须让$p-q$个特征值最小的特征向量不在作为新坐标系基向量的q个特征向量向量之中。这本质上与用最大投影方差的方法结果是完全等价的
### 奇异值分解(SVD)
我们知道对于一个中心矩阵$H=I-{1\over N}EE^T$，矩阵即$X$的中心化以后的矩阵，协方差矩阵$\Sigma={1\over N}X^THX$。对$HX$做奇异值分解有
$$
\begin{gathered}
HX=U\Lambda V^T\\
U^TU=I,V^TV=I,\Lambda为除了主对角线外均为零的矩阵\\
\begin{aligned}
\Sigma&={1\over N}X^THX={1\over N}X^TH^THX\\
&={1\over N}V\Lambda^TU^TU\Lambda V^T\\
&={1\over N}V\Lambda^T\Lambda V^T\\
&={1\over N}V\Delta V^T
\end{aligned}\\
\end{gathered}
$$
$\Delta=\Lambda^T\Lambda$显然为一个对角阵，因此对$\Sigma$做特征值分解可以转化为对$X$中心化后的矩阵做奇异值分解