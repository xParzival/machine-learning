# S01-高斯分布
## 高斯分布
### 1. 高斯分布与极大似然估计
#### 数学推导部分
* 给定包含N个d维样本的数据集
  $$
  Data:X=(X_1,X_2,\ldots,X_N)^T=
  \begin{pmatrix}
  X_1^T\\
  X_2^T\\
  \vdots\\
  X_N^T
  \end{pmatrix}_{N\times d}
  $$
  其中$X_i\in\mathbb{R}^d$且$X_i\mathop{\sim}\limits^{iid}N(\mu,\Sigma)$，则参数$\theta=(\mu,\Sigma)$
* 极大似然估计为$\theta=\arg\max\limits_{\theta}P(X|\theta)$
##### 1. 一维情况
* 讨论$d=1$的简单情况，即$\theta=(\mu,\sigma^2)$，此时$p(X_i|\theta)={1\over \sqrt{2\pi\sigma^2}}\exp[-{1\over 2\sigma^2}(X_i-\mu)^2]$  
  而$P(X|\theta)=\prod\limits_{i=1}^{N}p(X_i|\theta)$，为简化计算，对$P$取对数，则有
  $$
  \begin{aligned}
  \log{P(X|\theta)}&=\log\prod\limits_{i=1}^N{1\over \sqrt{2\pi\sigma^2}}\exp[-{1\over 2\sigma^2}(X_i-\mu)^2]\\
  &=\sum\limits_{i=1}^N\log{1\over \sqrt{2\pi}\sigma}\exp[-{1\over 2\sigma^2}(X_i-\mu)^2]\\
  &=\sum\limits_{i=1}^N[\log{1\over \sqrt{2\pi}}+\log{1\over \sigma}-\frac{(X_i-\mu)^2}{2\sigma^2}]\\
  \end{aligned}
  $$
  * 求$\mu_{MLE}$和$E(\mu_{MLE})$
  $$
  \begin{aligned}
  \mu_{MLE}&=\arg\max_\mu\log{P(X|\theta)}\\
  &=\arg\min_\mu\sum\limits_{i=1}^N(X_i-\mu)^2
  \end{aligned}
  $$
  $$
  \begin{aligned}
  &{\partial\over \partial\mu}\sum\limits_{i=1}^N(X_i-\mu)^2=0\\
  &\Rightarrow\sum\limits_{i=1}^N2(\mu-X_i)=0\\
  &\Rightarrow\mu_{MLE}=\frac{\sum\limits_{i=1}^NX_i}{N}\\
  \\
  E(\mu_{MLE})&=E\left(\frac{\sum_{i=1}^NX_i}{N}\right)\\
  &=\frac{\sum_{i=1}^NE\left(X_i\right)}{N}\\
  &=\mu
  \end{aligned}
  $$
  显然$\mu_{MLE}$是$\mu$的无偏估计量
  * 求$\sigma_{MLE}^2$和$E(\sigma_{MLE}^2)$
    $$
    \begin{aligned}
    \sigma_{MLE}&=\arg\max_\sigma\log{P(X|\theta)}\\
    &=\arg\max_\sigma\sum\limits_{i=1}^N[-\log\sigma-\frac{(X_i-\mu)^2}{2\sigma^2}]\\
    &=\arg\max_\sigma{L(\sigma)}\\
    \\
    \frac{\partial{L(\sigma)}}{\partial\sigma}&=0\\
    &\Rightarrow\sum\limits_{i=1}^N[\frac{(X_i-\mu)^2}{\sigma^3}-{1\over\sigma}]=0\\
    &\Rightarrow\sigma_{MLE}^2=\sum\limits_{i=1}^N\frac{(X_i-\mu)^2}{N}\\
    &\Rightarrow\sigma_{MLE}^2=\frac{\sum\limits_{i=1}^N(X_i-\mu_{MLE})^2}{N}\\
    \\
    \sigma_{MLE}^2&=\frac{\sum\limits_{i=1}^N(X_i-\mu_{MLE})^2}{N}\\
    &=\frac{\sum\limits_{i=1}^N(X_i^2-2\mu_{MLE}X_i+\mu_{MLE}^2)}{N}\\
    &=\frac{\sum\limits_{i=1}^NX_i^2}{N}-2\mu_{MLE}\frac{\sum\limits_{i=1}^NX_i}{N}+\frac{\sum\limits_{i=1}^N\mu_{MLE}^2}{N}\\
    &=\frac{\sum\limits_{i=1}^NX_i^2}{N}-\mu_{MLE}^2\\
    \\
    E(\sigma_{MLE}^2)&=E\left(\frac{\sum_{i=1}^NX_i^2}{N}-\mu_{MLE}^2\right)\\
    &=E\left((\frac{\sum_{i=1}^NX_i^2}{N}-\mu^2)-(\mu_{MLE}^2-\mu^2)\right)\\
    &=E\left(\frac{\sum_{i=1}^N(X_i^2-\mu^2)}{N}\right)-E(\mu_{MLE}^2-\mu^2)\\
    &=\frac{\sum_{i=1}^NE(X_i^2-\mu^2)}{N}-[E(\mu_{MLE}^2)-E(\mu^2)]\\
    &=\frac{\sum_{i=1}^N[E(X_i^2)-E^2(X_i)]}{N}-[E(\mu_{MLE}^2)-E^2(\mu_{MLE})]\\
    &=\frac{\sum_{i=1}^NVar(X_i)}{N}-Var(\mu_{MLE})\\
    &=\sigma^2-Var\left(\frac{\sum_{i=1}^NX_i}{N}\right)\\
    &=\sigma^2-\frac{Var(\sum_{i=1}^NX_i)}{N^2}\\
    &=\sigma^2-\frac{\sum_{i=1}^NVar(X_i)}{N^2}\\
    &=\frac{N-1}{N}\sigma^2
    \end{aligned}
    $$
    因此$\sigma_{MLE}^2$是$\sigma^2$的有偏估计量，$\sigma^2$无偏估计量为
    $$
    {\hat\sigma}^2=\frac{N}{N-1}\sigma_{MLE}^2=\frac{\sum\limits_{i=1}^N(X_i-\mu_{MLE})^2}{N-1}
    $$
##### 2. 高维情况  
  * 对于一个$X_i=X=(x_1,x_2,\ldots,x_d)^T$有
    $$
    X\sim{N(\mu,\Sigma)}={1\over(2\pi)^{d\over2}{|\Sigma|^{1\over2}}}\exp[-{1\over2}(X-\mu)^T\Sigma^{-1}(X-\mu)]
    $$
    其中
    $$
    \begin{aligned}
    \mu&=(\mu_1,\mu_2,\ldots,\mu_d)^T\\
    \\
    \Sigma_{d\times d}&=
    \begin{bmatrix}
      \sigma_{11}^2&\sigma_{12}^2&\cdots&\sigma_{1d}^2 \\
      \sigma_{21}^2&\sigma_{22}^2&\cdots&\sigma_{2d}^2 \\
      \vdots&\vdots&\ddots&\vdots \\
      \sigma_{d1}^2&\sigma_{d2}^2&\cdots&\sigma_{dd}^2 \\
    \end{bmatrix}
    \end{aligned}
    $$
    $(X-\mu)^T\Sigma^{-1}(X-\mu)$又称为$X$和$\mu$间的马氏距离
  * 假设$\Sigma$为正定矩阵，对其进行特征值分解$\Sigma=U\Lambda{U^T}$，其中$U=(u_1,u_2,\ldots,u_d)_{p\times p}$，$UU^T=U^TU=I$，$\Lambda=diag(\lambda_1,\lambda_2,\ldots,\lambda_d)$
    $$
    \begin{aligned}
    \Sigma&=(u_1,u_2,\ldots,u_d)
    \begin{pmatrix}
    \lambda_1&0&\cdots&0\\
    0&\lambda_2&\cdots&0\\
    \vdots&\vdots&\ddots&\vdots\\
    0&0&\cdots&\lambda_d
    \end{pmatrix}
    \begin{pmatrix}u_1^T\\u_2^T\\\vdots\\u_d^T\end{pmatrix}\\
    &=(\lambda_1u_1,\lambda_2u_2,\cdots,\lambda_du_d)
    \begin{pmatrix}u_1^T\\u_2^T\\\vdots\\u_d^T\end{pmatrix}\\
    &=\sum\limits_{i=1}^d\lambda_iu_iu_i^T\\
    \\
    \Sigma^{-1}&=(U\Lambda{U^T})^{-1}=(U^T)^{-1}\Lambda^{-1}U^{-1}=U\Lambda^{-1}U^T\\
    &=\sum\limits_{i=1}^d{1\over\lambda_i}u_iu_i^T\\
    \\
    (X-\mu)^T\Sigma^{-1}(X-\mu)&=(X-\mu)^T\left(\sum\limits_{i=1}^d{1\over\lambda_i}u_iu_i^T\right)(X-\mu)\\
    &=\sum\limits_{i=1}^d(X-\mu)^Tu_i{1\over\lambda_i}u_i^T(X-\mu)
    \end{aligned}
    $$
    令$Y=(y_1,y_2,\ldots,y_d)^T$，其中$y_i=(X-\mu)^Tu_i$，$\Delta=(X-\mu)^T\Sigma^{-1}(X-\mu)$，则
    $$
    \Delta=\sum\limits_{i=1}^dy_i{1\over\lambda_i}y_i^T=\sum\limits_{i=1}^d{y_i^2\over\lambda_i}
    $$
  * 在$d=2$即2维情况下，$\Delta={y_1^2\over\lambda_1}+{y_2^2\over\lambda_2}$。在给定$\Delta$值时，马氏距离即为一个在$y_1,y_2$坐标轴下的椭圆。显然$y_i$为$X-\mu$在$u_i$上的投影，那么特征值分解相当于对X做了坐标转换到Y坐标系。当$\Delta$值变化时，马氏距离即为在平面上的一簇椭圆，也即高斯分布三维图像平行于X平面的横截面的投影
  ![高斯分布](https://s1.ax1x.com/2020/04/08/GWZiY6.png)
#### 高斯分布局限性
  1. 参数很多，d维高斯分布需要参数个数为$\frac{d(d+1)}{2}=O(d^2)$。因此实际使用中会对协方差矩阵$\Sigma$进行简化，假设$\Sigma$为对角矩阵，那么就不需要进行特征值分解，此时图像为两个轴与X坐标系平行的椭圆。进一步如果假设$\Sigma$对角矩阵的$\lambda_i$都相等，则图像为一个圆，此时又称X为各向同性的高斯分布
     ![GWlRfA.png](https://s1.ax1x.com/2020/04/08/GWlRfA.png)
  2. 实际的数据分布可能不符合高斯分布，因此需要对单纯的高斯分布模型进行变化，例如高斯混合模型(GMM)

### 2. 高斯分布相关概率分布求解
#### 求边缘概率及条件概率
  令$X=\begin{pmatrix}X_a\\X_b\end{pmatrix},X_a\in\mathbb{R}^m,X_b\in\mathbb{R}^n,m+n=d,P(X)$即为$X_a$和$X_b$的联合概率分布
  $令\mu=\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix},\mu_a\in\mathbb{R}^m,\mu_b\in\mathbb{R}^n$
  $令\Sigma=\begin{pmatrix}\Sigma_{aa}&\Sigma_{ab}\\\Sigma_{ba}&\Sigma_{bb}\end{pmatrix},\Sigma_{aa}\in\mathbb{R}^{m\times m},\Sigma_{ab}\in\mathbb{R}^{m\times n},\Sigma_{ba}\in\mathbb{R}^{n\times m},\Sigma_{bb}\in\mathbb{R}^{n\times n}$
##### 边缘概率
  $$
  \begin{aligned}
  X_a&=
  \begin{pmatrix}I_m&0\end{pmatrix}
  \begin{pmatrix}X_a\\X_b\end{pmatrix}\\
  \Rightarrow E(X_a)&=
  \begin{pmatrix}I_m&0\end{pmatrix}
  \begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}
  =\mu_a\\
  \Sigma(X_a)&=
  \begin{pmatrix}I_m&0\end{pmatrix}
  \begin{pmatrix}\Sigma_{aa}&\Sigma_{ab}\\\Sigma_{ba}&\Sigma_{bb}\end{pmatrix}
  \begin{pmatrix}I_m\\0\end{pmatrix}
  =\Sigma_{aa}
  \end{aligned}
  $$
  $故X_a\sim{N(\mu_a,\Sigma_{aa})}，同理X_b\sim{N(\mu_b,\Sigma_{bb})}$
##### 条件概率
  $构造随机变量X_{b\cdot a}=X_b-\Sigma_{ba}\Sigma_{aa}^{-1}X_a,其与X_a相互独立。则X_b=X_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}X_a$
  $$
  \begin{aligned}
  X_{b\cdot a}&=
  \begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&I_n\end{pmatrix}
  \begin{pmatrix}X_a\\X_b\end{pmatrix}\\
  \Rightarrow E(X_{b\cdot a})&=
  \begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&I_n\end{pmatrix}
  \begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}\\
  &=\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a\\
  记作\mu_{b\cdot a}\\
  \\
  \Rightarrow Var(X_{b\cdot a})&=
  \begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&I_n\end{pmatrix}
  \begin{pmatrix}\Sigma_{aa}&\Sigma_{ab}\\\Sigma_{ba}&\Sigma_{bb}\end{pmatrix}
  \begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&I_n\end{pmatrix}^T\\
  &=\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}\\
  记作\Sigma_{bb\cdot a}\\
  \\
  \Rightarrow E(X_b|X_a)&=E(X_b)\\
  &=E(X_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}X_a)\\
  &=E(X_{b\cdot a})+\Sigma_{ba}\Sigma_{aa}^{-1}X_a\\
  &=\mu_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}X_a\\
  \\
  \Sigma(X_b|X_a)&=\Sigma(X_b)\\
  &=\Sigma(X_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}X_a)\\
  &=\Sigma(X_{b\cdot a})\\
  &=\Sigma_{bb\cdot a}
  \end{aligned}
  $$
  $故X_b|X_a\sim N(\mu_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}X_a,\Sigma_{bb\cdot a})，同理X_a|X_b\sim N(\mu_{a\cdot b}+\Sigma_{ab}\Sigma_{bb}^{-1}X_b,\Sigma_{aa\cdot b})$

#### 求联合高斯概率分布
设随机变量$X,Y$的概率分布$P(X),P(Y|X)$满足
$$
\begin{aligned}
P(X)&=N(X|\mu,\Lambda^{-1})\\
P(Y|X)&=N(Y|AX+b,L^{-1})
\end{aligned}
$$
那么$Y=AX+b+\epsilon,\epsilon\sim N(0,L^{-1})$即可满足上述分布
##### Y的概率分布
$$
\begin{aligned}
E(Y)&=E(AX+b+\epsilon)\\
&=AE(X)+b+E(\epsilon)\\
&=A\mu+b\\
\\
\Sigma(Y)&=\Sigma(AX+B+\epsilon)\\
&=\Sigma(AX)+\Sigma(\epsilon)\\
&=A\Lambda^{-1}A^T+L^{-1}
\end{aligned}
$$
故$Y\sim N(A\mu+b,A\Lambda^{-1}A^T+L^{-1})$
##### 联合概率分布
令$Z=\begin{pmatrix}X\\Y\end{pmatrix}$，$P(Z)$即为$X$和$Y$的联合概率分布，则有
$$
\begin{aligned}
Z=\begin{pmatrix}X\\Y\end{pmatrix}\sim N\left(
\begin{bmatrix}\mu\\A\mu+b\end{bmatrix},
\begin{bmatrix}
\Lambda^{-1}&\Delta\\
\Delta^T&A\Lambda^{-1}A^T+L^{-1}
\end{bmatrix}
\right)
\end{aligned}
$$
于是转化为求$\Delta$矩阵
$$
\begin{aligned}
\Delta&=Cov(X,Y)\\
&=E([X-E(X)][Y-E(Y)]^T)\\
&=E[(X-\mu)(AX+b+\epsilon-A\mu-b)^T]\\
&=E[(X-\mu)(A(X-\mu)+\epsilon)^T]\\
&=E[(X-\mu)(X-\mu)^TA^T+(X-\mu)\epsilon^T]\\
&=E[(X-\mu)(X-\mu)^TA^T+E[(X-\mu)\epsilon^T]\\
&=E[(X-\mu)(X-\mu)^TA^T]+E(X-\mu)E(\epsilon^T)\\
&=E[(X-\mu)(X-\mu)^T]A^T\\
&=\Lambda^{-1}A^T\\
\Rightarrow Z&\sim N\left(
\begin{bmatrix}\mu\\A\mu+b\end{bmatrix},
\begin{bmatrix}
\Lambda^{-1}&\Lambda^{-1}A^T\\
A\Lambda^{-1}&A\Lambda^{-1}A^T+L^{-1}
\end{bmatrix}\right)
\end{aligned}
$$
