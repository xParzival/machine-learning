# 最小二乘法
给定数据集$D=\{(X_1,y_1),(X_2,y_2),\cdots,(X_N,y_N)\}$，其中$X_i\in\mathbb{R}^p,y_i\in\mathbb{R},i=1,2,3,\ldots,N$
$$
\begin{aligned}
X&=(X_1,X_2,\cdots,X_N)^T=
\begin{pmatrix}
1&x_{11}&x_{12}&\cdots&x_{1p}\\
1&x_{21}&x_{22}&\cdots&x_{2p}\\
\vdots&\vdots&\ddots&\vdots&\vdots\\
1&x_{N1}&x_{N2}&\cdots&x_{Np}
\end{pmatrix}_{N\times(p+1)}\\
Y&=\begin{pmatrix}y_1\\y_2\\\vdots\\y_N\end{pmatrix}\\
\end{aligned}
$$
最小二乘法就是要拟合函数$f(W)=W_x^TX^{'}+b=W_x^TX^{'}+w_0\cdot 1=W^TX$，其中$W=(w_0,w_1,w_2,\cdots,w_p)^T$
## 最小二乘估计(Least Square Estiamte,LSE)
损失函数为
$$
\begin{aligned}
&\begin{aligned}
L(W)&=\sum_{i=1}^N{||W^TX_i-y_i||}^2\\
&=\sum_{i=1}^N(W^TX_i-y_i)^2\\
&=\begin{pmatrix}W^TX_1-y_1&W^TX_2-y_2&\cdots&W^TX_N-y_N\end{pmatrix}
\begin{pmatrix}W^TX_1-y_1\\W^TX_2-y_2\\\vdots\\W^TX_N-y_N\end{pmatrix}\\
&=(W^TX^T-Y^T)(XW-Y)\\
&=W^TX^TXW-W^TX^TY-Y^TXW+Y^TY\\
&=W^TX^TXW-2W^TX^TY+Y^TY\\
\end{aligned}\\
\\
&\begin{aligned}
&\Rightarrow\hat{W}=\arg\min_WL(W)\\
\\
&\Rightarrow\frac{\partial L(W)}{\partial W}=2X^TXW-2X^TY=0\\
\\
&\Rightarrow\hat{W}=(X^TX)^{-1}X^TY=X^{\dagger}Y
\end{aligned}
\end{aligned}
$$
## 几何解释
设$X=(X_0,X_1,\cdots,X_p)$，其中$X_i$为列向量，那么$f(W)=XW$，即$f(W)$为$X$张成的p维空间中的向量，$f(W)$为$X$列向量的线性组合
$$
f(W)=w_0X_0+w_1X_1+\cdots+w_pX_p
$$
$Y$为$X$所张成的空间外的一个向量，若要用一个合适的$f(W)$拟合$Y$，那么只需要找到$X$所张成的空间中距离$Y$最近的向量，显然为$Y$在$X$张成的空间中的投影，则有
$$
\begin{aligned}
&X^T(Y-f(W))=0\\
&\Rightarrow X^T(Y-XW)=0\\
&\Rightarrow \hat{W}=(X^TX)^{-1}X^TY
\end{aligned}
$$
## 概率角度解释
设$\epsilon\sim N(0,\sigma^2)$，则有$Y=f(W)+\epsilon=W^TX+\epsilon$，显然
$$
\begin{aligned}
(y_i|X_i;W)&\sim N(W^TX_i,\sigma^2)\\
P(y_i|X_i;W)&={1\over\sqrt{2\pi\sigma^2}}\exp(-{1\over{2\sigma^2}}(y_i-W^TX_i)^2)\\
P(Y|X;W)&=\prod_{i=1}^NP(y_i|X_i;W)
\end{aligned}
$$
根据极大似然估计有
$$
\begin{aligned}
L(W)&=\log{P(Y|X;W)}\\
&=\log\prod_{i=1}^NP(y_i|X_i;W)\\
&=\log\prod_{i=1}^N{1\over\sqrt{2\pi\sigma^2}}\exp\left(-{1\over{2\sigma^2}}(y_i-W^TX_i)^2\right)\\
&=\sum_{i=1}^N\left(\log{1\over\sqrt{2\pi\sigma^2}}-{1\over{2\sigma^2}}(y_i-W^TX_i)^2\right)\\
\\
\hat{W}&=\arg\max_WL(W)\\
&=\arg\max_W\sum_{i=1}^N\left(-{1\over{2\sigma^2}}(y_i-W^TX_i)^2\right)\\
&=\arg\min_W\sum_{i=1}^N(y_i-W^TX_i)^2
\end{aligned}
$$
故最小二乘估计等价于噪声为高斯分布的极大似然估计

# 线性回归的正则化
$$
\begin{aligned}
L(W)&=\sum_{i=1}^N(W^TX_i-y_i)^2\\
\hat{W}&=(X^TX)^{-1}X^TY
\end{aligned}
$$
## 过拟合
* 实际数据集观测数少或维数过多，此时造成$X^TX$不可逆，那么$W$没有解析解。具体表现为模型过拟合
* 解决方法：
  1. 增加数据量
  2. 特征选择/特征提取（降维）
  3. 正则化
## 正则化
$$
\arg\max_W[L(W)+\lambda P(W)]
$$
其中$L(W)$称为损失函数(Loss Function)，$P(W)$称为惩罚项(Penalty)
* 正则化一般有两种
  1. L1正则化，又叫Lasso：$P(W)=||W||_1$
  2. L2正则化，又叫岭回归(Ridge Regression)：$P(W)=||W||_2^2=W^TW$
### Lasso
### Ridge Regression
令$J(W)=L(W)+\lambda P(W)$，则
$$
\begin{aligned}
&\begin{aligned}
J(W)&=\sum_{i=1}^N(W^TX_i-y_i)^2+\lambda W^TW\\
&=W^TX^TXW-2W^TX^TY+Y^TY+\lambda W^TW\\
&=W^T(X^TX+\lambda I)W-2W^TX^TY+Y^TY
\end{aligned}\\
\\
&\begin{aligned}
&\Rightarrow\frac{\partial J(W)}{\partial W}=2(X^TX+\lambda I)W-2X^TY=0\\
\\
&\Rightarrow\hat{W}=(X^TX+\lambda I)^{-1}X^TY
\end{aligned}
\end{aligned}
$$
因为$X^TX$为半正定矩阵，则$X^TX+\lambda I$一定为正定矩阵，故一定可逆，有解析解。因此岭回归有抑制过拟合的效果