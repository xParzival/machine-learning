# S02-线性回归
## 最小二乘法
给定数据集$D=\{(X_1,y_1),(X_2,y_2),\cdots,(X_N,y_N)\}$，其中$X_i\in\mathbb{R}^p,y_i\in\mathbb{R},i=1,2,3,\ldots,N$
$$
\begin{aligned}
X&=(X_1,X_2,\cdots,X_N)^T=
\begin{pmatrix}
1&x_{11}&x_{12}&\cdots&x_{1p}\\
1&x_{21}&x_{22}&\cdots&x_{2p}\\
\vdots&\vdots&\ddots&\vdots&\vdots\\
1&x_{N1}&x_{N2}&\cdots&x_{Np}
\end{pmatrix}_{N\times(p+1)}\\
Y&=\begin{pmatrix}y_1\\y_2\\\vdots\\y_N\end{pmatrix}\\
\end{aligned}
$$
最小二乘法就是要拟合函数$f(W)=W_x^TX^{'}+b=W_x^TX^{'}+w_0\cdot 1=W^TX$，其中$W=(w_0,w_1,w_2,\cdots,w_p)^T$
### 最小二乘估计(Least Square Estimate,LSE)
损失函数为
$$
\begin{aligned}
&\begin{aligned}
L(W)&=\sum_{i=1}^N{||W^TX_i-y_i||}^2\\
&=\sum_{i=1}^N(W^TX_i-y_i)^2\\
&=\begin{pmatrix}W^TX_1-y_1&W^TX_2-y_2&\cdots&W^TX_N-y_N\end{pmatrix}
\begin{pmatrix}W^TX_1-y_1\\W^TX_2-y_2\\\vdots\\W^TX_N-y_N\end{pmatrix}\\
&=(W^TX^T-Y^T)(XW-Y)\\
&=W^TX^TXW-W^TX^TY-Y^TXW+Y^TY\\
&=W^TX^TXW-2W^TX^TY+Y^TY\\
\end{aligned}\\
\\
&\begin{aligned}
&\Rightarrow\hat{W}=\arg\min_WL(W)\\
\\
&\Rightarrow\frac{\partial L(W)}{\partial W}=2X^TXW-2X^TY=0\\
\\
&\Rightarrow\hat{W}=(X^TX)^{-1}X^TY=X^{\dagger}Y
\end{aligned}
\end{aligned}
$$
### 几何解释
设$X=(X_0,X_1,\cdots,X_p)$，其中$X_i$为列向量，那么$f(W)=XW$，即$f(W)$为$X$张成的p维空间中的向量，$f(W)$为$X$列向量的线性组合
$$
f(W)=w_0X_0+w_1X_1+\cdots+w_pX_p
$$
$Y$为$X$所张成的空间外的一个向量，若要用一个合适的$f(W)$拟合$Y$，那么只需要找到$X$所张成的空间中距离$Y$最近的向量，显然为$Y$在$X$张成的空间中的投影，则有
$$
\begin{aligned}
&X^T(Y-f(W))=0\\
&\Rightarrow X^T(Y-XW)=0\\
&\Rightarrow \hat{W}=(X^TX)^{-1}X^TY
\end{aligned}
$$
### 概率角度解释
设$\epsilon\sim N(0,\sigma^2)$，则有$Y=f(W)+\epsilon=W^TX+\epsilon$，显然
$$
\begin{aligned}
(y_i|X_i;W)&\sim N(W^TX_i,\sigma^2)\\
P(y_i|X_i;W)&={1\over\sqrt{2\pi\sigma^2}}\exp(-{1\over{2\sigma^2}}(y_i-W^TX_i)^2)\\
P(Y|X;W)&=\prod_{i=1}^NP(y_i|X_i;W)
\end{aligned}
$$
根据极大似然估计有
$$
\begin{aligned}
L(W)&=\log{P(Y|X;W)}\\
&=\log\prod_{i=1}^NP(y_i|X_i;W)\\
&=\log\prod_{i=1}^N{1\over\sqrt{2\pi\sigma^2}}\exp\left(-{1\over{2\sigma^2}}(y_i-W^TX_i)^2\right)\\
&=\sum_{i=1}^N\left(\log{1\over\sqrt{2\pi\sigma^2}}-{1\over{2\sigma^2}}(y_i-W^TX_i)^2\right)\\
\\
\hat{W}&=\arg\max_WL(W)\\
&=\arg\max_W\sum_{i=1}^N\left(-{1\over{2\sigma^2}}(y_i-W^TX_i)^2\right)\\
&=\arg\min_W\sum_{i=1}^N(y_i-W^TX_i)^2
\end{aligned}
$$
故最小二乘估计等价于噪声为高斯分布的极大似然估计

## 线性回归的正则化
$$
\begin{aligned}
L(W)&=\sum_{i=1}^N(W^TX_i-y_i)^2\\
\hat{W}&=(X^TX)^{-1}X^TY
\end{aligned}
$$
### 过拟合
* 实际数据集观测数少或维数过多，此时造成$X^TX$不可逆，那么$W$没有解析解。具体表现为模型过拟合
* 解决方法：
  1. 增加数据量
  2. 特征选择/特征提取（降维）
  3. 正则化
### 正则化
$$
\arg\min_W[L(W)+\lambda P(W)]
$$
其中$L(W)$称为损失函数(Loss Function)，$P(W)$称为惩罚项(Penalty)
* 正则化一般有两种
  1. L1正则化，又叫Lasso：$P(W)=||W||_1$
  2. L2正则化，又叫岭回归(Ridge Regression)：$P(W)=||W||_2^2=W^TW$
#### L2正则化
##### 岭回归推导
令$J(W)=L(W)+\lambda P(W)$，则
$$
\begin{aligned}
&\begin{aligned}
J(W)&=\sum_{i=1}^N(W^TX_i-y_i)^2+\lambda W^TW\\
&=W^TX^TXW-2W^TX^TY+Y^TY+\lambda W^TW\\
&=W^T(X^TX+\lambda I)W-2W^TX^TY+Y^TY
\end{aligned}\\
\\
&\begin{aligned}
&\Rightarrow\frac{\partial J(W)}{\partial W}=2(X^TX+\lambda I)W-2X^TY=0\\
\\
&\Rightarrow\hat{W}=(X^TX+\lambda I)^{-1}X^TY
\end{aligned}
\end{aligned}
$$
因为$X^TX$为半正定矩阵，则$X^TX+\lambda I$一定为正定矩阵，故一定可逆，有解析解。因此岭回归有抑制过拟合的效果
##### 贝叶斯角度理解
根据贝叶斯学派观点，参数$W$服从一个先验分布，假设该先验分布为$w_i\sim N(0,\sigma_0^2)$，注意这里指的是单个参数的分布。因为每个参数$w_i$相互独立，则
$$
\begin{aligned}
P(W)&=\prod_{i=1}^pP(w_i)\\
&=\prod_{i=1}^p{1\over\sqrt{2\pi\sigma_0^2}}\exp\left(-{1\over{2\sigma_0^2}}w_i^2\right)\\
&={1\over(2\pi\sigma_0)^{p\over2}}\exp\left(-{1\over{2\sigma_0^2}}\sum_{i=1}^pw_i^2\right)\\
&={1\over(2\pi\sigma_0)^{p\over2}}\exp\left(-{1\over{2\sigma_0^2}}||W||^2\right)
\end{aligned}
$$
设$\epsilon\sim N(0,\sigma^2)$，则有$Y=f(W)+\epsilon=W^TX+\epsilon$，显然
$$
(y_i|X_i)=(y_i|X_i;W)\sim N(W^TX_i,\sigma^2)
$$
根据贝叶斯定理
$$
\begin{aligned}
&\begin{aligned}
P(W|X,Y)&=\frac{P(Y|X,W)P(X,W)}{P(X,Y)}\\
&=\frac{P(Y|X,W)P(X)P(W)}{P(X,Y)}\\
&=\frac{P(Y|X,W)P(W)}{P(Y|X)}\\
&\propto P(Y|X,W)P(W)\\
&=P(W)\prod_{i=1}^NP(y_i|X_i,W)\\
&={1\over(2\pi\sigma_0)^{p\over2}}\exp\left(-{1\over{2\sigma_0^2}}||W||^2\right)\prod_{i=1}^N{1\over\sqrt{2\pi\sigma^2}}\exp\left(-{1\over{2\sigma^2}}(y_i-W^TX_i)^2\right)\\
&\propto\exp\left(-{1\over{2\sigma^2}}\sum_{i=1}^N(y_i-W^TX_i)^2-{1\over{2\sigma_0^2}}||W||^2\right)
\end{aligned}\\
\\
&\begin{aligned}
W_{MAP}&=\arg\max_W \log P(W|X,Y)\\
&=\arg\max_W\left(-{1\over{2\sigma^2}}\sum_{i=1}^N(y_i-W^TX_i)^2-{1\over{2\sigma_0^2}}||W||^2\right)\\
&=\arg\min_W\left(\sum_{i=1}^N(W^TX_i-y_i)^2+\frac{\sigma^2}{\sigma_0^2}W^TW\right)\\
&\Leftrightarrow\arg\min_W[L(W)+\lambda P(W)],\lambda=\frac{\sigma^2}{\sigma_0^2}
\end{aligned}
\end{aligned}
$$
故加入了L2正则化项的最小二乘估计等价于先验分布和噪声均为高斯分布的最大后验估计。注意这里先验分布和噪声分布的均值都为0
##### 几何意义
考虑如下有约束条件的优化问题
$$
\begin{gathered}
\hat{W}=\arg\min_W L(W)=\arg\min_W\sum_{i=1}^N(W^TX_i-y_i)^2\\
s.t.\quad||W||^2\le t
\end{gathered}
$$
利用拉格朗日乘子法转化为无约束条件的优化问题，即
$$
J(W,\lambda)=\sum_{i=1}^N(W^TX_i-y_i)^2+\lambda||W||^2-\lambda t
$$
在给定$\lambda$和$t$后该优化任务与加入了L2正则化的最小二乘估计等价
假设$p=2$即在二维情况下，$W=(w_1,w_2)^T$，$X_i=(x_{i1},x_{i2})^T$上述有约束条件的优化问题可以写为
$$
\begin{gathered}
\begin{aligned}
L(W)&=\sum_{i=1}^N(W^TX_i-y_i)^2\\
&=\sum_{i=1}^2\left(\begin{pmatrix}w_1&w_2\end{pmatrix}
\begin{pmatrix}x_{i1}\\x_{i2}\end{pmatrix}-y_i\right)^2\\
&=\sum_{i=1}^2(w_1x_{i1}+w_2x_{i2}-y_i)^2\\
&=aw_1^2+bw_2^2+cw_1+dw_2+e
\end{aligned}\\
其中a,b,c为x_{i1},x_{i2},y_i构成的系数(i=1,2\cdots,N),均为常数\\
s.t.\quad w_1^2+w_2^2\le t
\end{gathered}
$$
这是一个凸优化问题，显然$L(W)$在坐标$w_1,w_2$平面内的投影为一簇椭圆，在没有约束条件时椭圆圆心时$L(W)$取得最小值。约束条件将$W$限制在一个圆内，画出图像如下，$L(W)$在椭圆与圆的切点$\hat{W}$处取得最小值
![L2正则化](https://s1.ax1x.com/2020/04/13/GXrgOS.png)
假设我们用梯度下降法来求解这个优化问题，考虑某一步到达了约束条件边界的一个点$W$。如果没有约束条件，那么应该按照$\nabla J(W)$即梯度方向迭代，但是在有约束条件的情况下，如图所示，应该按蓝色箭头方向迭代，即约束圆的切线方向，最终收敛于最优解$\hat{W}$，此时梯度方向与约束圆的法线方向相同
##### 代数解释
假设用梯度下降法来求解这个优化问题时，迭代公式为
$$
\begin{aligned}
W_{t+1}&=W_t-\eta\nabla J(W_t)\\
&=W_t-\eta\nabla(L(W_t)+\lambda W_t^TW_t)\\
&=W_t-\eta\nabla L(W_t)-\lambda W_t\\
&=(1-\eta\lambda)W_t-\eta\nabla L(W_t)
\end{aligned}
$$
该式表明每次迭代后$W_t$相比$W_{t+1}$降低了一个比例，即在迭代过程中让权重趋向于按比例衰减，因此L2正则化对应权值衰减(Weight Decay)
#### L1正则化
##### Lasso推导
推导与L2正则化类似，此处省略
##### 贝叶斯角度解释
推导与L2正则化类似，此处直接给出结论：加入了L1正则化项的最小二乘估计等价于先验分布为拉普拉斯分布和噪声为高斯分布的最大后验估计。注意这里先验分布和噪声分布的均值都为0
##### 几何意义
L1正则化时$W$限制在如图的一个菱形内，显然在点$\hat{W}$时$L(W)$取得最小值。此时$w_1=0$
![L1正则化](https://s1.ax1x.com/2020/04/13/GXbm6S.png)
因此L1正则化倾向于得到稀疏的$W$，即L1正则化会使得部分参数为零。这样就达到了特征选择的效果，可以用来降维，从而抑制了过拟合
##### 代数解释
$$
\begin{aligned}
W_{t+1}&=W_t-\eta\nabla J(W_t)\\
&=W_t-\eta\nabla(L(W_t)+\lambda W_t)\\
&=W_t-\eta\nabla L(W_t)-\lambda\\
&=(W_t-\lambda)-\eta\nabla L(W_t)
\end{aligned}
$$
该式表明每次迭代后$W_t$相比$W_{t+1}$有一个偏移量，即在迭代过程中让权重趋向于减少甚至为零，因此L1正则化倾向于让权值稀疏化
#### 总结
* L1、L2正则化都能达到抑制过拟合的效果
* L1正则化趋向于得到稀疏化权值，L2正则化趋向于权值衰减但是不会稀疏化权值
* L1正则化可以用来特征选择和降维