# S06P02-EM算法
## EM算法
EM算法是用于含隐变量的概率模型参数极大似然估计或极大后验估计的方法。在概率模型不仅含有观测变量(observed variable)，还含有隐变量(latent variable)时，简单的极大似然估计或者极大后验估计不可行，这时就需要EM算法
### 三硬币模型
#### 问题
假设有三个硬币A,B,C，其正面向上的概率分别为a,b,c。有如下实验：做两步投掷，第一步先掷硬币A；第二步如果上一步出现正面则掷硬币B，如果上一步出现反面则掷硬币C，最终得到正面记为1，反面记为0。把这个两步实验做n次，得到一个实验结果序列。要求不看掷硬币的过程，仅根据实验结果推断a,b,c的值
#### 解决方法
显然这个问题中含有一个隐变量，即掷硬币A的结果，或者第二步是掷硬币B还是硬币C，这个变量是我们没法观测到的，一般记隐变量为$Z$。记最后结果为$y$，令$\theta=(a,b,c)$为模型参数，于是可以把这个问题写为
$$
\begin{aligned}
P(y|\theta)&=\sum_ZP(y,Z|\theta)=\sum_ZP(y|\theta,Z)P(Z|\theta)\\
&=ab^y(1-b)^{1-y}+(1-a)c^y(1-c)^{1-y}
\end{aligned}
$$
如果做了n次实验，得到的样本为$Y=(y_1,y_2,\cdots,y_n)$，根据极大似然估计，则要极大化下式
$$
\begin{aligned}
P(Y|\theta)&=\prod_{i=1}^nP(y_i|\theta)\\
&=\prod_{i=1}^n[ab^{y_i}(1-b)^{1-y_i}+(1-a)c^{y_i}(1-c)^{1-y_i}]
\end{aligned}
$$
这个问题是没有解析解的，只能通过迭代的方法求解。EM算法就是解决这种问题的一种迭代方法
### EM算法推导
给定观测样本$X=(X_1,X_2,\cdots,X_N)$，其对应的隐变量为$Z$。于是可写出对应的对数似然函数
$$
L(\theta)=\log P(X|\theta)=\sum_{i=1}^N\log P(X_i|\theta)
$$
#### ELBO和KL散度
对$L(\theta)$做一些变换
$$
\begin{aligned}
L(\theta)&=\sum_{i=1}^N\log\frac{P(X_i,Z|\theta)}{P(Z|X_i,\theta)}\\
&=\sum_{i=1}^N[\log P(X_i,Z|\theta)-\log P(Z|X_i,\theta)]\\
&=\sum_{i=1}^N\left[\log\frac{P(X_i,Z|\theta)}{P_i(Z)}-\log\frac{P(Z|X_i,\theta)}{P_i(Z)}\right]
\end{aligned}
$$
其中$P_i(Z)$为对每一个样本引入的一个概率分布，满足$\int_ZP_i(Z)dZ=1$。上式两边的每一个$i$对应的项对$P_i(Z)$求期望
$$
\begin{gathered}
\begin{aligned}
left_i&=\int_ZP_i(Z)\log P(X_i|\theta)dZ\\
&=\log P(X_i|\theta)\int_ZP_i(Z)dZ\\
&=\log P(X_i|\theta)
\end{aligned}\\
\\
\begin{aligned}
right_i=\int_ZP_i(Z)\log\frac{P(X_i,Z|\theta)}{P_i(Z)}dZ-\int_ZP_i(Z)\log\frac{P(Z|X_i,\theta)}{P_i(Z)}dZ
\end{aligned}
\end{gathered}
$$
对右边部分，定义一个Evidence Lower Bound
$$
ELBO_i=\int_ZP_i(Z)\log\frac{P(X_i,Z|\theta)}{P_i(Z)}dZ
$$
显然有
$$
\begin{aligned}
KL(P_i(Z)||P(Z|X_i,\theta))&=\int_ZP_i(Z)\log\frac{P_i(Z)}{P(Z|X_i,\theta)}dZ\\
&=-\int_ZP_i(Z)\log\frac{P(Z|X_i,\theta)}{P_i(Z)}dZ
\end{aligned}
$$
则可得
$$
\begin{aligned}
right_i&=ELBO_i+KL(P_i(Z)||P(Z|X_i,\theta))\\
&\ge ELBO_i
\end{aligned}
$$
等号当且仅当$P_i(Z)=P(Z|X_i,\theta)$时成立
求和以后最终写为
$$
\begin{aligned}
L(\theta)&=\sum_{i=1}^N\log P(X_i|\theta)\\
&\ge \sum_{i=1}^N\int_ZP_i(Z)\log\frac{P(X_i,Z|\theta)}{P_i(Z)}dZ
\end{aligned}
$$
上式表明说$L(\theta)$有一个下界，下界由$P_i(Z)$决定，极大化这个下界就相当于在极大化$L(\theta)$。显然在取等号的时候极大化下界等价于极大化$L(\theta)$，于是在第$t$次迭代对于$\theta^{(t)}$，令$P_i(Z)=P(Z|X_i,\theta^{(t)})$等号成立，即用$Z$关于给定$X_i,\theta^{(t)}$的后验概率来估计它，可以令
$$
\begin{aligned}
B(\theta,\theta^{(t)})&=\sum_{i=1}^N\int_ZP_i(Z)\log\frac{P(X_i,Z|\theta)}{P_i(Z)}dZ\\
&=\sum_{i=1}^N\int_ZP(Z|X_i,\theta^{(t)})\log\frac{P(X_i,Z|\theta)}{P(Z|X_i,\theta^{(t)})}dZ\\
&=\sum_{i=1}^N\left[\int_ZP(Z|X_i,\theta^{(t)})\log P(X_i,Z|\theta)dZ-\int_ZP(Z|X_i,\theta^{(t)})\log P(Z|X_i,\theta^{(t)})dZ\right]\\
&=\sum_{i=1}^N\int_ZP(Z|X_i,\theta^{(t)})\log P(X_i,Z|\theta)dZ-\sum_{i=1}^N\int_ZP(Z|X_i,\theta^{(t)})\log P(Z|X_i,\theta^{(t)})dZ
\end{aligned}
$$
在第$t$轮迭代时，$B(\theta,\theta^{(t)})$在$\theta=\theta^{(t)}$时对应等号成立的情况，即$B(\theta^{(t)},\theta^{(t)})=L(\theta^{(t)})$；而在$\theta\ne\theta^{(t)}$时，等号不成立，但是仍然满足$B(\theta,\theta^{(t)})\lt L(\theta)$。最大化$L(\theta)$就是在每一轮迭代都对$B(\theta,\theta^{(t)})$求最大，然后迭代至收敛即可
$$
\begin{aligned}
\theta^{(t+1)}&=\arg\max_{\theta}\sum_{i=1}^N\int_ZP(Z|X_i,\theta^{(t)})\log P(X_i,Z|\theta)dZ-\sum_{i=1}^N\int_ZP(Z|X_i,\theta^{(t)})\log P(Z|X_i,\theta^{(t)})dZ\\
&=\arg\max_{\theta}\sum_{i=1}^N\int_ZP(Z|X_i,\theta^{(t)})\log P(X_i,Z|\theta)dZ\\
&=\arg\max_{\theta}Q(\theta,\theta^{(t)})
\end{aligned}
$$
于是最终转化为每一轮最大化$Q(\theta,\theta^{(t)})=\sum_{i=1}^NQ_i(\theta,\theta^{(t)})$
$$
Q_i(\theta,\theta^{(t)})=\int_ZP(Z|X_i,\theta^{(t)})\log P(X_i,Z|\theta)dZ=E_{P(Z|X_i,\theta^{(t)})}[\log P(X_i,Z|\theta)]
$$
这就是EM算法的来源，E代表求期望$E_{P(Z|X_i,\theta^{(t)})}[\log P(X_i,Z|\theta)]$，M代表最大化这个期望
#### ELBO和Jensen不等式
还可以从另外一个角度来理解，还是从对数似然函数角度出发
$$
\begin{aligned}
L(\theta)&=\sum_{i=1}^N\log P(X_i|\theta)\\
&=\sum_{i=1}^N\log\int_ZP(X_i,Z|\theta)dZ\\
&=\sum_{i=1}^N\log\int_Z\frac{P(X_i,Z|\theta)}{P_i(Z)}P_i(Z)dZ\\
&=\sum_{i=1}^N\log E_{P_i(Z)}\left[\frac{P(X_i,Z|\theta)}{P_i(Z)}\right]
\end{aligned}
$$
其中$P_i(Z)$为关于$Z$的一个概率分布，满足$\int_ZP_i(Z)dZ=1$。根据Jensen不等式有
$$
\begin{aligned}
\log E_{P_i(Z)}\left[\frac{P(X_i,Z|\theta)}{P_i(Z)}\right]&\ge E_{P_i(Z)}\left[\log\frac{P(X_i,Z|\theta)}{P_i(Z)}\right]\\
&=\int_ZP_i(Z)\log\frac{P(X_i,Z|\theta)}{P_i(Z)}dZ\\
&=ELBO_i
\end{aligned}
$$
等号在$\frac{P(X_i,Z|\theta)}{P_i(Z)}$为常数时成立
$$
\begin{gathered}
\frac{P(X_i,Z|\theta)}{P_i(Z)}=C\\
\begin{aligned}
&\Rightarrow P_i(Z)={1\over C}P(X_i,Z|\theta)\\
&\Rightarrow\int_ZP_i(Z)dZ=\int_Z{1\over C}P(X_i,Z|\theta)dZ\\
&\Rightarrow C=P(X_i|\theta)\\
\end{aligned}
\end{gathered}
$$
将$C=P(X_i|\theta)$代入即得
$$
P_i(Z)=\frac{P(X_i,Z|\theta)}{P(X_i|\theta)}=P(Z|X_i,\theta)
$$
这和用KL散度方法得到的结果一致，也是在每一轮用后验概率$P(Z|X_i,\theta)$来估计$P_i(Z)$，后续推导也与前一种方法等价
#### EM算法
一般地，用$X$表示观测到的样本，用$Z$表示隐变量，模型参数为$\theta$，$X$和$Z$联合数据称为完全数据，$X$可称为不完全数据。在给定观测数据$X$时，不完全数据的对数似然函数为$\log P(X|\theta)$，$X$和$Z$的联合概率分布为$P(X,Z|\theta)$，完全数据的对数似然函数为$\log P(X,Z|\theta)$
EM算法通过迭代求解$\log P(X|\theta)$的极大似然估计，每次迭代都包含两步：E步，求期望(Expectation)；M步，求极大化(Maximization)。下面先给出EM算法的具体流程
* 算法流程：
  > (1)选择参数初值$\theta^{(0)}$
  > (2)E步：记$\theta^{(i)}$为第$i$次迭代参数的估计值，在第$i+1$次迭代计算下式
  > $$
  > \begin{aligned}
  > Q(\theta,\theta^{(i)})&=E_{Z|X,\theta^{(i)}}[\log P(X,Z|\theta)]\\
  > &=\int_Z\log P(X,Z|\theta)P(Z|X,\theta^{(i)})dZ
  > \end{aligned}
  > $$
  > $P(Z|X,\theta^{(i)})$是给定观测数据$Y$和当前参数估计$\theta^{(i)}$下隐变量$Z$的条件分布
  > (3)M步：求使$Q(\theta,\theta^{(i)})$极大化的$\theta$，即第$i+1$次迭代的参数估计值$\theta^{(i+1)}$
  > $$
  > \theta^{(i+1)}=\arg\max_\theta Q(\theta,\theta^{(i)})
  > $$
  > (4)重复步骤(2)、(3)，直到达到收敛条件
### EM算法理解
通过推导我们可以看出，在每一轮迭代实际都是在最大化$L(\theta)=\log P(X|\theta)$的一个下界，通过不断的最大化这个下界，最终使$L(\theta)$本身达到最大，如下图的过程
<div align=center>
<img src="https://s1.ax1x.com/2020/05/09/YQZ3hd.png" />
</div>

### EM算法的收敛性
EM的收敛性包含两层：关于对数似然函数序列$L(\theta^{(t)})$的收敛性和参数序列$\theta^{(t)}$的收敛性。而且算法的收敛性只能保证参数序列一定能收敛于对数似然函数序列的稳定点，不保证收敛到对数似然函数序列的极大值点，因此初始值的设定很重要。对于凸优化问题肯定能收敛至最优解，对于非凸优化问题，可能收敛至局部最优点
* 下面只证明对数似然函数序列的$L(\theta^{(t)})$收敛性：
  如果对数似然函数有极大值，要证明对数似然函数序列的$L(\theta^{(t)})$收敛，即要证明$L(\theta^{(t)})$是单调递增的，即证$L(\theta^{(t)})\le L(\theta^{(t+1)})$
  根据EM算法的推导可得
  $$
  \begin{gathered}
  L(\theta)=\sum_{i=1}^Nleft_i=\sum_{i=1}^NQ_i(\theta,\theta^{(t)})-\sum_{i=1}^N\int_ZP(Z|X_i,\theta^{(t)})\log P(Z|X_i,\theta)dZ
  \end{gathered}
  $$
  令
  $$
  H_i(\theta,\theta^{(t)})=\int_ZP(Z|X_i,\theta^{(t)})\log P(Z|X_i,\theta)dZ
  $$
  将变量$\theta$分别取$\theta^{(t)},\theta^{(t+1)}$，考虑下式
  $$
  \begin{aligned}
  &H_i(\theta^{(t+1)},\theta^{(t)})-H_i(\theta^{(t)},\theta^{(t)})\\
  =&\int_ZP(Z|X_i,\theta^{(t)})\log P(Z|X_i,\theta^{(t+1)})dZ-\int_ZP(Z|X_i,\theta^{(t)})\log P(Z|X_i,\theta^{(t)})dZ\\
  =&\int_ZP(Z|X_i,\theta^{(t)})\log\frac{P(Z|X_i,\theta^{(t+1)})}{P(Z|X_i,\theta^{(t)})}dZ\\
  =&-KL\left[Z|X_i,\theta^{(t)}||Z|X_i,\theta^{(t+1)})\right]\\
  \le&0
  \end{aligned}
  $$
  因为每次迭代要对$Q_i(\theta,\theta^{(t)})$极大化，所以显然$Q_i(\theta^{(t)},\theta^{(t)})\le Q_i(\theta^{(t+1)},\theta^{(t)})$
  $$
  \begin{aligned}
  L(\theta^{(t+1)})-L(\theta^{(t)})&=\left[\sum_{i=1}^NQ_i(\theta^{(t+1)},\theta^{(t)})-\sum_{i=1}^NH_i(\theta^{(t+1)},\theta^{(t)})\right]-\left[\sum_{i=1}^NQ_i(\theta^{(t)},\theta^{(t)})-\sum_{i=1}^NH_i(\theta^{(t)},\theta^{(t)})\right]\\
  &=\sum_{i=1}^N\left[Q_i(\theta^{(t+1)},\theta^{(t)})-Q_i(\theta^{(t)},\theta^{(t)})\right]-\sum_{i=1}^N\left[H_i(\theta^{(t+1)},\theta^{(t)})-H_i(\theta^{(t)},\theta^{(t)})\right]\\
  &\ge0
  \end{aligned}
  $$
  因此对数似然函数序列$L(\theta^{(t)})$是单调递增的，只要$L(\theta)$有极大值则EM算法一定可以收敛
### 广义EM算法
上面介绍的EM算法中，在E步计算期望时，我们令$P_i(Z)=P_i(Z|X_i,\theta)$，但是在实际情况中，$P_i(Z|X_i,\theta)$也有可能计算不出来，这样等号就不能成立，这种情况下就要考虑新的方法来极大化$L(\theta)$。根据之前的推导
$$
\begin{aligned}
L(\theta)&=ELBO+KL\\
&=\sum_{i=1}^NELBO_i+\sum_{i=1}^NKL(P_i(Z)||P(Z|X_i,\theta))\\
&=\sum_{i=1}^N\int_ZP_i(Z)\log\frac{P(X_i,Z|\theta)}{P_i(Z)}dZ-\sum_{i=1}^N\int_ZP_i(Z)\log\frac{P(Z|X_i,\theta)}{P_i(Z)}dZ
\end{aligned}
$$
根据上式，如果固定$\theta$，那么$L(\theta)$不变，按照之前的想法要让$KL=0$，但是在$P_i(Z|X_i,\theta)$计算不出来时只能作为一个优化问题来求解，也就是最小化$KL$的问题，等价于最大化$ELBO$，此时对应的$P_i(Z)=\hat{P_i}(Z)$；在$\hat{P_i}(Z)$下，对$\theta$极大化$L(\theta)$，如此迭代就可以达到极大化$L(\theta)$的目的。这就是广义EM算法的基本思想
#### Generalized EM
广义EM算法(GEM)流程
> (1)选择参数初值$\theta^{(0)}$
> (2)E步：记$P_i^{(t)}(Z)$为第$t$次迭代$P_i(Z)$的估计值，$\theta^{(t)}$为第$t$次迭代参数的估计值，在第$t+1$次迭代计算固定$\theta=\theta^{(t)}$时的优化问题
> $$
> \begin{aligned}
> P_i^{(t+1)}(Z)&=\arg\max_{P_i(Z)}ELBO_i\\
> &=\arg\max_{P_i(Z)}E_{P_i(Z)}\left[\log\frac{P(X_i,Z|\theta^{(t)})}{P_i(Z)}\right]\\
> &=\arg\max_{P_i(Z)}\int_ZP_i(Z)\log\frac{P(X_i,Z|\theta^{(t)})}{P_i(Z)}dZ
> \end{aligned}
> $$
> (3)M步：极大化在$P_i(Z)=P_i^{(t+1)}(Z)$时的$ELBO_i$，即求第$t+1$次迭代的参数估计值$\theta^{(t+1)}$
> $$
> \begin{aligned}
> \theta^{(t+1)}&=\arg\max_\theta ELBO_i\\
> &=\arg\max_\theta\int_ZP_i^{(t+1)}(Z)\log\frac{P(X_i,Z|\theta)}{P_i^{(t+1)}(Z)}dZ
> \end{aligned}
> $$
> (4)重复步骤(2)、(3)，直到达到收敛条件

可以看到实际上GEM就是一种坐标上升法
我们研究一下此时的$ELBO_i$
$$
\begin{aligned}
ELBO_i&=\int_ZP_i(Z)\log\frac{P(X_i,Z|\theta)}{P_i(Z)}dZ\\
&=\int_ZP_i(Z)\log P(X_i,Z|\theta)dZ-\int_ZP_i(Z)\log P_i(Z)dZ\\
&=E_{P_i(Z)}[\log P(X_i,Z|\theta)]+H[P_i(Z)]
\end{aligned}
$$
其中$H[P_i(Z)]=-\int_ZP_i(Z)\log P_i(Z)dZ$为$P_i(Z)$的熵
一般地，令F函数
$$
F(P_i(Z),\theta)=ELBO_i
$$
此时的GEM算法又称为F-MM算法
#### GEM的变种
实际在GEM算法中，在E-Step和M-Step分别求解优化问题时，可能都很难求解，因此针对这两步优化运用不同的方法，又可以衍生出更广泛的EM算法，主要有以下两种
1. VEM/VBEM：在求解E-Step和M-Step分别求解优化问题时运用变分推断(VI)的方法
2. MCEM：在求解E-Step和M-Step分别求解优化问题时运用马尔可夫链蒙特卡罗方法(MCMC)