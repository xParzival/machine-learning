# 特征工程
特征工程是机器学习等建模之前的重要一步，对模型准确率和实用性有极大影响
## 业务流程
一般来说建模流程如下：
1. 将业务抽象为分类或回归问题
2. 定义目标值或标签
3. 选取合适的样本，匹配出全部信息作为特征来源
4. 特征工程+模型训练+模型评价与调优（实际这三个步骤是有交叉的）
5. 输出模型报告
6. 上线与监控

## 特征工程
在实际业务处理中，数据通常是脏数据。所谓的脏，指数据可能存在以下几种问题（主要问题）:
1. 数据缺失(Incomplete)：属性值为空
2. 数据噪声(Noisy)：变量的随机误差和方差，是观测点和真实点之间的误差
3. 数据不一致(Inconsistent)：数据前后存在矛盾，如 Age = 42而Birthday = "01/09/1985"
4. 数据冗余(Redundant)：数据量或者属性数目超出数据分析需要
5. 数据集不均衡(Imbalance)：各个类别或取值的数据量相差悬殊
6. 离群点/异常值(Outliers)：离群点指值远离数据集中部分的数据；异常值指数据值不合常理，如 Salary = -100
7. 数据重复(Duplicate)：在数据集中出现多次的数据

特征工程就是对数据进行处理，得到干净的数据。特征工程一般包含的内容：
* 基础特征构造
* 数据预处理
* 特征衍生
* 特征变换
* 特征筛选
  
特征工程是一个完整且复杂的流程，其每一个过程都是有可能交叉的，并不是完全按顺序的
### 数据预处理
实际上数据处理相关的工作时间占据了整个项目的70%以上。数据的质量，直接决定了模型的预测和泛化能力的好坏。它涉及很多因素，包括：准确性、完整性、一致性、时效性、可信性和解释性。而在真实数据中，我们拿到的数据可能包含了大量的缺失值，可能包含大量的噪音，也可能因为人工录入错误导致有异常点存在，非常不利于算法模型的训练。数据清洗的结果是对各种脏数据进行对应方式的处理，得到标准的、干净的、连续的数据，提供给数据统计、数据挖掘等使用
数据预处理的主要步骤分为：数据清理、数据集成、和数据变换
#### 数据清理
数据清理含义就是将数据变得干净，主要包括以下几个方面
1. 缺失值处理：
   获取信息和数据的过程中，会存在各类的原因导致数据丢失和空缺。基于变量的分布特性和变量的重要性（信息量和预测能力）采用不同的方法来处理
   * 删除特征：当某个特征缺失率达到一定限度(一般80%)时，覆盖率较低，且重要性较低，可以直接将该特征删除
   * 定值填充：某些变量的缺失是代表一定含义的，例如信用卡信息的缺失，就代表这个样本没有办过信用卡，这时可以赋一个特殊值作为标记，比如0，-9999等
   * 统计量填充：若缺失率较低（小于5-10%）且重要性较低，则根据数据分布的情况进行填充。对于数据符合均匀分布，用该特征的均值填补缺失，对于数据存在倾斜分布的情况，采用中位数进行填补
   * 插值法填充：包括随机插值，多重差补法，热平台插补，拉格朗日插值，牛顿插值等
   * 模型填充：针对连续型特征使用回归、贝叶斯、随机森林、决策树等模型对缺失数据进行预测
   * 哑变量填充：若特征是离散型，且不同值较少，可转换成哑变量，例如性别SEX特征，存在male,fameal,NA三个不同的值，可将该列转换成 IS_SEX_MALE, IS_SEX_FEMALE, IS_SEX_NA。若某个变量存在十几个不同的值，可根据每个值的频数，将频数较小的值归为一类'other'，降低维度。此做法可最大化保留变量的信息
   
   总结来看，常用的做法是：先检测出特征的缺失比例，考虑删除或者填充，若需要填充的特征是连续型，一般采用均值法和随机差值进行填充，若特征是离散型，通常采用中位数或哑变量进行填充。当然有时候还可以对连续特征进行离散化，例如将年龄离散化为青年、中年、老年，这时一般将缺失值当作离散化后特征的额外标记，例如标记为'other'
2. 离群点/异常值处理：
   异常分为两种：“伪异常”，由于特定的业务运营动作产生，是正常反应业务的状态，而不是数据本身的异常；“真异常”，不是由于特定的业务运营动作产生，而是数据本身分布异常，即离群点。离群点的检测方法如下
   * 简单统计分析：箱线图、分位点判断
   * $3\sigma$原则：如果数据符合正态分布，那么偏离均值$3\sigma$之外的通常为异常点。通常判断标准为$P(|x-\mu|\gt\sigma)\lt0.03$
   * 基于绝对离差中位数(MAD)：这是一种稳健对抗离群数据的距离值方法，采用计算各观测值与平均值的距离总和的方法，放大了离群值的影响
   * 基于距离：通过定义对象之间的临近性度量，根据距离判断异常对象是否远离其他对象，缺点是计算复杂度较高，不适用于大数据集和存在不同密度区域的数据集
   * 基于密度：离群点的局部密度显著低于大部分近邻点，适用于非均匀的数据集
   * 基于聚类：利用聚类算法，丢弃远离其他簇的小簇
   
   具体的处理方法
   * 根据异常点的数量和影响，考虑是否将该条记录删除，这样做信息损失较大
   * 若对数据做了对数变换后消除了异常值，则此方法生效，且不损失信息
   * 平均值或中位数替代异常点，对于极值点一般用最接近极值点的正常数据替代极值。简单高效，信息的损失较少
   * 根据使用的模型来评估，比如在训练树模型时，树模型对离群点的鲁棒性较高，无信息损失，不影响模型训练效果
3. 噪声处理：
   噪声是特征采集过程由随机误差和方差产生的，是观测点和真实点之间的误差。一般有如下处理方法
   * 对数据进行离散化，等频或等宽分箱，然后用每个箱的平均数，中位数或者边界值（不同数据分布，处理方法不同）代替箱中所有的数，起到平滑数据的作用，可以降低噪声的影响
   * 建立该变量和预测变量的回归模型，根据回归系数和预测变量，反解出自变量的近似值
   * 滤波方法，常用语图像处理、语音处理等领域，对数据本身精度有影响

#### 数据集成
数据集成就是指把各个渠道得到的数据整合到一起，作去重处理、去冗余等
#### 数据变换
数据变换包括对数据进行规范化，离散化，稀疏化等处理，达到适用于挖掘的目的
1. 规范化：数据中不同特征的量纲可能不一致，数值间的差别可能很大，不进行处理可能会影响到数据分析的结果，因此，需要对数据按照一定比例进行缩放，使之落在一个特定的区域，便于进行综合分析。特别是基于距离的挖掘方法，聚类，KNN，SVM一定要做规范化处理
   规范化主要有以下方法。这里只简单介绍三种，更多的方法可以到scikit-learn的数据预处理包中查看
   * 最大最小规范化：
     $$
     x_{new}=\frac{x-x_{min}}{x_{max}-x_{min}}
     $$
     这样就把特征映射到$[0,1]$之间
   * Z-Score标准化：
     $$
     x_{new}=\frac{x-\mu}{\sigma}
     $$
     其中$\mu$为特征的均值，$\sigma$为方差。这样标准化之后新的特征均值为0，方差为1
   * Log变换：
     $$
     x_{new}=\log x
     $$
     作对数变换后可以有效降低数据量级，而且基本不丢失信息

2. 离散化：数据离散化是指将连续的数据进行分段，使其变为一个个离散化的区间。分段的原则有基于等距离、等频率或优化的方法
   离散化的原因有以下几个
   * 模型原因：某些模型是基于离散特征的，必须做离散化才能进行训练，例如ID3决策树，朴素贝叶斯等
   * 有效的离散化能减小算法的时间和空间开销，提高系统对样本的分类聚类能力和抗噪声能力
   * 离散化特征比连续型特征更容易理解
   * 离散化有助于克服特征的缺陷，使模型更稳定，例如数据缺失、异常值、噪声等影响

   离散化的主要方法
   * 等频或等宽离散化
   * 聚类法：根据聚类出来的簇，每个簇中的数据为一个箱，簇的数量模型给定

3. 稀疏化：针对离散型且标称变量，无法进行有序的LabelEncoder时，通常考虑将变量做0，1哑变量的稀疏化处理。例如动物类型变量中含有猫，狗，猪，羊四个不同值，将该变量转换成is_猪，is_猫，is_狗，is_羊四个哑变量。若是变量的不同值较多，则根据频数，将出现次数较少的值统一归为一类。稀疏化处理既有利于模型快速收敛，又能提升模型的抗噪能力

### 特征衍生
在实际业务中，通常我们只拥有几个到几十个不等的基础特征，例如用户信息、用户基本行为数据等，而多数特征没有实际含义，不适合直接建模，如用户地址（多种属性值的分类特征）、用户日消费金额（弱数值特征）。而此类特征在做一定的变换或者组合后，往往具有较强的信息价值。所以我们需要对基础特征做一些衍生类的工作，也就是生成更多有价值的特征
特征构建工作并不完全依赖于技术，它要求我们具备相关领域丰富的知识或者实践经验，基于业务，花时间去观察和分析原始数据，思考问题的潜在形式和数据结构，从原始数据中找出一些具有真实意义的特征
特征衍生的方法十分灵活，并没有特定的规则和流程，主要是依靠业务经验和逻辑，在多个原始特征的基础上进行组合交叉，得到很多新特征。在实际使用中，无论使用什么方法，特征衍生都是十分耗时且繁重的过程，对于模型效果的影响也很大
下面介绍几种主要思路：
1. 时间序列特征：对某些有时间累积性的特征会做时间序列处理，比如某段时间内的总和、某段时间内的均值等
2. 特征交叉：对多个特征进行交叉组合，或做交，并，补，笛卡尔集等运算。最直接就是暴力交叉，即所有特征进行交叉，但是不仅耗时而且会带来稀疏性等问题
3. 自动衍生：可以使用sklearn.preprocessing.PolynomialFeatures、featuretools这样的工具来进行特征衍生

### 特征选择
在训练机器学习模型之前，特征选择是一个很重要的过程，之所以进行特征选择，有以下几点很重要的原因：
1. 现实任务中经常遇到维数灾难问题，如果能选择出重要特征，再进行后续学习过程，则维数灾难可以大为减轻
2. 去除不相关的特征往往会降低学习任务的难度，使模型更易理解（比如，使决策树的规则变得更加清晰）
3. 去除不相关的变量还可以尽量减少过拟合的风险，尤其是在使用人工神经网络或者回归分析等方法时，额外的输入变量会增加模型本身的额外自由度，这些额外的自由度对于模型记住某些细节信息会有所帮助，但对于创建一个稳定性良好、泛化性能强的模型却没有好处，也就是说增加额外的不相关变量会增大过拟合的风险

需要注意的是，虽然特征选择和降维都是为了减少特征的数量，但是特征选择不同于降维。降维是创造特征的新组合，比如PCA和SVD；特征选择则只是从原有特征中进行选择或排除，不涉及原有特征的转变
特征选择宏观上主要有以下三大类
1. filter：过滤式方法先对数据集进行特征选择，然后再训练模型
   * 剔除方差过低的特征
   * 单变量特征选择：IV值，信息增益等
2. wrapper：包裹式方法直接把最终将要使用的模型的性能作为特征子集的评价标准，也就是说，包裹式特征选择的目的就是为给定的模型选择最有利于其性能的特征子集，它是一个在模型构建中的特征选择框架
   * 递归特征消除(RFE)
3. Embedding：嵌入式特征选择是将特征选择过程与模型训练过程融为一体，两者在同一个优化过程中完成，即在模型训练的过程中自动进行特征选择。嵌入式方法的实例有 LASSO回归、岭回归、决策树等
   * sklearn.SelectFromModel
   * 将特征选择过程融入pipeline

通常来说，从两个方面考虑来选择特征：
* 特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用
* 特征与目标的相关性：与目标相关性高的特征，应当优选选择

下面结合Scikit-learn给出一些具体的特征选择方法：
1. 移除低方差的特征：feature_selection.VarianceThreshold
   不管是对于离散变量还是连续变量，一般来说如果某个取值的样本占所有样本数目达到95%以上，那么这个特征就是没有什么用处的，可以删除
2. 单变量特征选择：对应sklearn.feature_selection中一系列方法
   分类问题：
   * 卡方检验 feature_selection.chi2
   * F检验 feature_selection.f_classif
   * 互信息 feature_selection.mutual_info_classif
   
   回归问题：
   * 相关系数 feature_selection.f_regression
   * 互信息 feature_selection.mutual_info_regression
   * 最大信息系数(MIC)

   需要注意的是，相关系数、卡方检验等主要对线性相关性有效，对于非线性关系并不好用。互信息的方法可以捕获任何类型的统计依赖关系，但是作为一个非参数方法，估计准确需要更多的样本
3. 递归特征消除(RFE)：针对含有特征权重的预测模型，RFE通过递归的方式，不断减少特征集的规模来选择需要的特征
   * 采用预测模型在这些原始的特征上进行训练
   * 在获取到特征的权重值后，对这些权重值取绝对值，把最小绝对值剔除掉
   * 不断循环递归，直至剩余的特征数量达到所需的特征数量
   
   还可以通过交叉验证的方式执行RFE，以此来选择最佳数量的特征：对于指定的模型，通过计算该模型对应数量特征的validation error做交叉验证。选择error最小的那个子集作为所挑选的特征。这种方法计算量非常大
4. 使用带L1或L2正则化的模型来做特征筛选

#### 实际问题
在业务中的模型会遇到如下问题：
* 模型效果不好
* 训练集效果好，跨时间测试效果不好
* 跨时间测试效果也好，上线之后效果不好
* 上线之后效果还好，几周之后分数分布开始下滑
* 一两个月内都比较稳定，突然分数分布骤降
* 没有明显问题，但模型每个月逐步失效

业务所需要的特征必须具备的条件：
* 变量必须对模型有贡献，也就是说必须能对客群加以区分
* 逻辑回归要求变量之间线性无关
* 针对逻辑回归评分卡，希望变量呈现单调趋势（有一部分也是业务原因，但从模型角度来看，单调变量未必一定比有转折的变量好）
* 客群在每个变量上的分布稳定，分布迁移无可避免，但不能波动太大

因此在实际业务中常用考虑以下问题：
1. 特征重要性
   * IV值：用于分类问题，IV值越大特征与目标越相关
     对于离散特征直接按离散的取值计算，对于连续特征则需要进行离散化。假设一个特征被分成了$I$个组，令$i_+$表示第$i$组正例的数目，$i_-$表示第$i$组反例的数目，$n_+,n_-$分别表示样本中正例和反例的总数，则可以先计算出每组的woe值(Weight of Evidence)
     $$
     \text{woe}_i=\ln\frac{i_+/n_+}{i_-/n_-}=\ln\frac{i_+/i_-}{n_+/n_-}
     $$
     显然woe值反映了某组正例占总正例的比率与反例占总反例比率之间的差距。每一组的IV值为
     $$
     \text{iv}_i=(\frac{i_+}{n_+}-\frac{i_-}{n_-})\text{woe}_i=(\frac{i_+}{n_+}-\frac{i_-}{n_-})\ln\frac{i_+/n_+}{i_-/n_-}
     $$
     则该特征的IV值为
     $$
     \text{IV}=\sum_{i=1}^I\text{iv}_i
     $$
   * 模型筛选：采用可以输出特征重要性的模型来筛选特征，比如随机森林、GBDT等

2. 特征共线性
   * 相关系数：此时为不同特征之间的相关系数，而不是特征和目标值之间的相关系数
   * 方差膨胀系数(VIF)：VIF可以检验回归模型是否存在严重的多重共线性问题，它表示回归系数估计量的方差与假设自变量间不线性相关时方差相比的比值
     $$
     \text{VIF}=\frac{1}{1-R^2}
     $$
     其中$0\le R^2\le1$是以其他特征对某特征做回归分析时的复相关系数，复相关系数越大表明共线性越严重，但是复相关系数不够稳定，因此取$1-R^2$的倒数作为评判标准称为方差膨胀系数，越大表明共线性越严重，一般VIF大于10认为有严重的共线性

3. 特征稳定性
   * PSI(Population Stability Index)：群体稳定性指标。用来衡量实际的占比与预期占比的差异，可以用来比较特征在跨时间上的稳定性。具体来讲，假设某个特征被分为$I$个分组，如果以训练数据为基准，设训练数据每个分组的正例占比为基准比例$\text{base\_rate}_i$。如果该特征是稳定的，那么在验证数据、测试数据，以及上线后的数据上，这个比例应该是接近的，设在基准数据集以外某数据上对应分组的正例占比为$r_i$，则有
     $$
     \text{PSI}=\sum_{i=1}^I(r_i-\text{base\_rate}_i)\ln\frac{r_i}{\text{base\_rate}_i}
     $$
     显然比例分布的越接近，PSI值越小，即特征越稳定。实际上PSI也可以用来评价模型结果的稳定性。一般认为PSI小于0.1时候稳定性很高，0.1-0.25一般，大于0.25稳定性差
     在用于评价特征时，PSI高的特征不建议选用；PSI也可用于监控特征或模型的稳定性，当PSI波动较大时需要对模型做出调整
   * 跨时间交叉验证：将样本按时间切分为不同子集，然后按照交叉验证的方式去训练得到模型和进入模型的特征，最后取特征的交集。这种方法容易受特征共线性的影响，因此要么提前检验特征共线性，要么每次交叉验证都只用部分特征训练