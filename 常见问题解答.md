# 类别变量处理
## 类别数量较少
一般使用one-hot编码、哑变量或effect编码处理。大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征通过编码，转化到多维特征，降低了空间和时间的效率
但我们知道对于决策树来说并不推荐使用编码，尤其当类别特征中类别个数很多的情况下，会存在以下问题：
1. 会产生样本切分不平衡问题，导致切分增益非常小（即浪费了这个特征）。使用 one-hot编码，意味着在每一个决策节点上只能使用one vs rest（例如是不是狗，是不是猫等）的切分方式。例如，动物类别切分后，会产生是否狗，是否猫等一系列特征，这一系列特征上只有少量样本为 1，大量样本为 0，这时候切分样本会产生不平衡，这意味着切分增益也会很小。较小的那个切分样本集，它占总样本的比例太小，无论增益多大，乘以该比例之后几乎可以忽略；较大的那个拆分样本集，它几乎就是原始的样本集，增益几乎为零。比较直观的理解就是不平衡的切分和不切分没有区别
2. 会影响决策树的学习。因为就算可以对这个类别特征进行切分，独热编码也会把数据切分到很多零散的小空间上。而决策树学习时利用的是统计信息，在这些数据量小的空间上，统计信息不准确，学习效果会变差

## 类别数量特别多
类别数目特别多时，如果还是用one-hot编码的方法，会造成维数爆炸。一般会使用压缩编码的方法，下面列举几种
1. 特征哈希
2. bin-counting
3. 深度学习中会使用一种embedding方法
4. 其他编码方式

# 类别不平衡
在很多业务场景下，分类任务会出现类别十分不平等的情况。也就是说，在数据集中，有一类含有的数据要远远多于其他类的数据（类别分布不平衡），此时如果直接训练会对模型效果有很大影响
考虑一个简单的例子，10万正样本与1000个负样本，正负样本比列为100：1，如果直接带入模型中去学习，每一次梯度下降如果使用全量样本，负样本的权重只有不到1/100，即使完全不学习负样本的信息，准确率也有超过99%，所以显然我们绝不能以准确率来衡量模型的效果。但是实践下面，我们其实也知道，即使用KS或者AUC来度量模型的表现，依然没法保证模型能将负样本很好的学习。而我们实际上需要得到一个分类器，既能对于正例有很高的准确率，同时又不会影响到负例的准确率
于是针对这个问题出现了一些解决办法
## 下探
所谓下探，就是对评分较低被拒绝的人进行放款，牺牲一部分收益，来积累坏样本，供后续模型学习，这是最直接解决风控场景样本不均衡的方法
这也是所有方法中最直接有效的。但是不是每一家公司都愿意承担这部分坏账的成本
此外随着业务开展，后续模型迭代的时候，使用的样本是有偏的，下探同样可以解决这个问题
## 半监督学习
### 拒绝推断
#### 拒绝推断的目的
* 如果只用好坏用户建模，则忽略了那些授信被拒的用户，加入拒绝用户是为了让建模样本更接近总体的分布，防止样本有偏，同时也能增加样本数量
* 公司内部策略的变动，导致当前的申请者已不能代表建模时点的申请者，所以过去被拒的用户不一定现在也会被拒绝，因此，只使用审批通过的用户可能会造成误判
* 做拒绝推断可以找出之前被拒的好用户，挖掘这些用户，改善风控流程，增加公司收益
#### 拒绝推断的适用范围
高核准率不适合用拒绝推断，因为高核准率下好坏用户已接近于整体的申请用户。中低核准率适用用拒绝推断
#### 拒绝推断的常用方法
* 硬性截断法：先用好坏用户建立初始模型，然后用这个初始模型对拒绝用户进行打分，设定一个阈值分数（根据对拒绝用户的风险容忍度），低于这个阈值的为坏用户，高于这个阈值的为好用户。再将已标记好的拒绝用户放入样本中，重新建立模型
* 分配法：此方法适用于评分卡，先用好坏用户建立初始评分卡模型，再将样本跟据评分高低进行分组，计算各分组的违约率。然后对拒绝用户进行打分并按此前的步骤进行分组，以各分组的违约率为抽样比例，随机抽取该分组下的违约用户，指定其为坏用户，剩下的则是好用户。最后将已标记的拒绝用户加入样本中，重新建立模型

平常工作中主要用到以上两种方法，个人建议做申请模型最好做一下拒绝推断，这样模型上线后的得分分布和拒绝率和线下才不会有很大的差异
## 标签分裂
将客户群用某些指标或模型方式划分为更小的集合，在小样本集里面分别建立模型。比如按地区、按渠道等方式，模型方式可以用聚类等方式
## 代价敏感学习
代价敏感学习是利用不同类别的样本被误分类而产生不同的代价，使用这种方法解决数据不平衡问题。而且有很多研究表明，代价敏感学习和样本不平衡问题有很强的联系，并且使用代价敏感学习的方法解决不平衡学习问题要优于使用随机采样的方法
1. 把误分类代价作为数据集的权重，然后采用 Bootstrap 采样方法选择具有最好的数据分布的数据集
2. 以集成学习的模式来实现代价最小化的技术，这种方法可以选择很多标准的学习算法作为集成学习中的弱分类器
3. 把代价敏感函数或者特征直接合并到分类器的参数中，这样可以更好的拟合代价敏感函数。由于这类技术往往都具有特定的参数，因此这类方法没有统一的框架

## 采样方法
### 欠采样
删除一部分正例，平衡正反样本比例
### 过采样
将负样本用一些手段增加一部分，平衡正负样本比例
#### SMOTE
python包：imblearn.over_sampling.SMOTE
SMOTE是一种基于少数类样本计算样本特征空间的相似度，然后合成人工样本的过采样方法
具体采样计算方法如下：
1. 对于少数类$P\in S$中的样本$x_i\in P$，计算其K个近邻少数类样本。近邻指标一般是欧式距离，实际也可以使用其他度量
2. 从K个近邻中随机选择一个近邻$\hat{x}_i$，通过如下方法合成一个人工新样本
   $$
   x_{new}=x_i+\delta(\hat{x}_i-x_i)\quad \delta\in(0,1)
   $$

![SMOTE](https://s1.ax1x.com/2020/08/30/dqDUDP.png)
缺点：会产生重复样本，容易过拟合
为了克服SMOTE的缺点，Adaptive Synthetic Sampling方法被提出，主要包括：Borderline-SMOTE和Adaptive Synthetic Sampling（ADA-SYN）算法
#### Borderline-SMOTE1
其与SMOTE的不同是：SMOTE是对每一个少数类样本产生综合新样本，而Borderline-SMOTE仅对靠近边界的少数类样本创造新数据
设样本集为S，多数类集合为N，少数类集合为P
1. 对于少数类$P\in S$中的样本$x_i\in P$，计算其K个近邻样本，即为近邻集合$S_i$。近邻指标一般是欧式距离，实际也可以使用其他度量
2. 将少数类样本按以下标准分为三类：
   * DANGER：近邻样本中少数类样本占多数，即${K\over2}\lt|S_i\bigcap N|\lt K$，视为危险样本，即在边界容易被误分的样本
   * SAFE：近邻样本中少数类样本占少数，即$0\lt|S_i\bigcap N|\lt{K\over2}$，视为安全样本
   * NOISE：近邻样本中没有少数类样本，都是多数类样本，即$|S_i\bigcap N|=0$，视为噪声
   
   对于DANGER样本，用SMOTE的方法合成新样本即可
#### Borderline-SMOTE2
Borderline-SMOTE2是在Borderline-SMOTE1基础上进一步改进。Borderline-SMOTE2也一样将样本分为三类后对DANGER样本进行过采样，但是采样方法有所不同：
1. 对DANGER类样本$x_i$，分别找到其少数类K近邻$P_i$和多数类K近邻$N_i$
2. 设定一个比例$\alpha$，在$P_i$中取$\alpha$比例的样本点，和SMOTE一样做线性插值生成样本点
3. 在$N_i$中取$1-\alpha$比例的样本点，和SMOTE一样做线性插值生成样本点，此时为了让新生成的样本点离少数类样本更近，取$\delta\in(0,0.5)$

两种Borderline SMOTE方法都可以加强边界处模糊样本的存在感，且Borderline SMOTE-2又能在此基础上使新增样本更加靠近真实值
#### ADASYN
ADASYN是一种自适应综合过采样方法，其解决思路是根据数据分布情况为不同的小众样本生成不同数量的新样本
python包：imblearn.over_sampling.ADASYN
具体实现流程如下
1. 首先计算需要生成的新样本数量$G=(|N|-|P|)\times\beta$，其中$\beta\in[0.1]$为，平衡样本后想要达到的平衡度
2. 对少数类样本$x_i$，计算其K近邻$S_i$
3. 对少数类样本$x_i$，计算$\Gamma_i=\frac{|S_i\bigcap N|/K}{Z},i=1,2,\cdots,|P|$，其中$Z$为归一化常数，则$\sum_{i=1}^{|P|}\Gamma_i=1$
4. 对所有少数类样本$x_i$计算合成样本的数目$G_i=\Gamma_i G$
5. 在每个待合成的少数类样本周围K个邻居中随机选择1个少数类样本，利用线性插值合成新样本$x_{new}=x_i+\delta(\hat{x}_i-x_i)$
6. 重复步骤5直到每个少数类样本合成的新样本达到步骤4计算出的数目为止

ADASYN能够自适应的决定每个小众样本的合成数量，但不能抵抗噪声的干扰
# LR+离散特征优势
在工业界，很少直接将连续值作为特征喂给逻辑回归模型，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：
1. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易scalable（扩展）
2. 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰
3. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合
4. 离散化后可以进行特征交叉，由M+N个变量变为 𝑀∗𝑁 个变量，进一步引入非线性，提升表达能力
5. 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问
6. 离散后每一个特征的每一个取值对应着评分卡加减分数，业务上更容易理解

#人工特征 VS 机器特征
首先，海量离散特征＋LR是业内常见的一个做法，但并不是Holy Grail，事实上这一般而言仅仅是因为LR的优化算法更加成熟，而且可以在计算中利用稀疏特性进行更好的优化—可谓不得已而为之
事实证明GBDT和深度学习特征的加入对于CTR预测是有正面帮助的。如果这个问题思考地更深一点，其实当前深度学习网络的最后一层，如果是binary classification，其实等同于LR
所以说，通过人工／半人工的方式产生的features，跟深度神经网络（无论之前用了怎样的结构）最后学出来的representation，其实是异曲同工，区别在于深度学习一般而言会学出一个dense representation，而特征工程做出来的是一堆sparse representation。某些时候，人工特征其实跟神经网络经过几层非线性之后的结果是高度相似的
在暴力提取高阶／非线性特征的本事上，机器肯定胜过人类。但是，就算最牛的机器智能，有时候都敌不过一些“人类常识”。尤其是业务的一些逻辑，可以认为是人脑在更大的一个数据集上pre-train出来的一些特征，其包含的信息量一定是大于你用于预测的dataset的。在这种情况下，往往厉害的人工features会outperform暴力的机器方法
所以，特征离散化，从数学角度来说可以认为是增加robustness，但是更重要的，make sense of the data，将数据转变成人类可以理解、可以validate的格式。人类的业务逻辑，当然也不是完美的。在当前机器智能还未征服“常识”这个领域之前，人类的business insights还是一个有力的补充（在很多case，甚至是最重要的部分）。在机器能够完全掌握的范围内，譬如围棋，人类引以为傲的intuition已经无法抵抗机器的暴力计算了——所以在未来，我们一定会看到越来越多的机器智能开始侵入一些传统上认为必须要依靠人类的“感觉”的一些领域。风控领域当然也不能躲过这个大的趋势
# LR 适用于稀疏特征原因
假设有1w 个样本， y类别0和1，100维特征，其中10个样本都是类别1，而特征 f1的值为0，1，且刚好这10个样本的 f1特征值都为1，其余9990样本都为0(在高维稀疏的情况下这种情况很常见)，我们都知道这种情况在树模型的时候，很容易优化出含一个使用 f1为分裂节点的树直接将数据划分的很好，但是当测试的时候，却会发现效果很差，因为这个特征只是刚好偶然间跟 y拟合到了这个规律，这也是我们常说的过拟合
说线性模型在优化会产生这样一个式子：
$$
y=𝑊_1∗𝑓_1+⋯+𝑊_i∗𝑓_i+⋯
$$
1. 其中$W_1$特别大以拟合这十个样本。因为反正$f_1ß$的值只有0和1，$W_1$大对其他9990样本不会有任何影响
2. 线性模型普遍都会带着正则项，而 lr 等线性模型的正则项是对权重的惩罚，也就是$W_1$一旦过大，惩罚就会很大，进一步压缩$W_1$的值，使他不至于过大；而树模型则不一样，树模型的惩罚项通常为叶子节点数和深度等，而我们都知道，对于上面这种 case，树只需要一个节点就可以完美分割9990和10个样本，惩罚项极其之小