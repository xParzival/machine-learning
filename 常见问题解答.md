## 类别变量处理
### 类别数量较少
一般使用one-hot编码、哑变量或effect编码处理。大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征通过编码，转化到多维特征，降低了空间和时间的效率
但我们知道对于决策树来说并不推荐使用编码，尤其当类别特征中类别个数很多的情况下，会存在以下问题：
1. 会产生样本切分不平衡问题，导致切分增益非常小（即浪费了这个特征）。使用 one-hot编码，意味着在每一个决策节点上只能使用one vs rest（例如是不是狗，是不是猫等）的切分方式。例如，动物类别切分后，会产生是否狗，是否猫等一系列特征，这一系列特征上只有少量样本为 1，大量样本为 0，这时候切分样本会产生不平衡，这意味着切分增益也会很小。较小的那个切分样本集，它占总样本的比例太小，无论增益多大，乘以该比例之后几乎可以忽略；较大的那个拆分样本集，它几乎就是原始的样本集，增益几乎为零。比较直观的理解就是不平衡的切分和不切分没有区别
2. 会影响决策树的学习。因为就算可以对这个类别特征进行切分，独热编码也会把数据切分到很多零散的小空间上。而决策树学习时利用的是统计信息，在这些数据量小的空间上，统计信息不准确，学习效果会变差

### 类别数量特别多
类别数目特别多时，如果还是用one-hot编码的方法，会造成维数爆炸。一般会使用压缩编码的方法，下面列举几种
1. 特征哈希
2. bin-counting
3. 深度学习中会使用一种embedding方法
4. 其他编码方式