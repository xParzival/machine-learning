# 统计与概率
## 两大学派
对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派。设样本观测集为
$$
X_{N\times p}=(X_1,X_2,\cdots,X_N)^T,X_i=(x_{i1},x_{i2},\cdots,x_{ip})
$$
即样本有N个观测，每个观测是p维随机向量，其中每个观测都由概率分布$p(X_i|\theta)$生成
### 频率学派
频率学派认为概率分布$p(X_i|\theta)$的参数$\theta$是一个客观存在的常量。对于N个观测整个样本集出现的概率$P(X|\theta)\overset{iid}{=}\prod\limits_{i=1}^Np(X_i|\theta)$。此时估计$\theta$采用极大似然估计(MLE)的方法
$$
\theta_{MLE}=\arg\max_{\theta}P(X|\theta)
$$
### 贝叶斯学派
贝叶斯学派认为概率分布$p(X_i|\theta)$的参数$\theta$不是常量，而是一个符合$\theta\sim p(\theta)$分布的随机变量，这个分布称为先验分布。于是针对N个观测的样本集，根据贝叶斯定理可得出后验概率分布
$$
p(\theta|X)=\frac{P(X|\theta)p(\theta)}{P(X)}=\frac{P(X|\theta)p(\theta)}{\int_\theta P(X|\theta)p(\theta)d\theta}
$$
其中分别有以下几个概念：
1. $p(\theta|X)$称为后验概率分布，即考虑给出的相关证据或数据后所得到的条件概率分布
2. $p(\theta)$称为先验概率分布，即没有证据或数据的时候参数的概率分布
3. $P(X|\theta)$称为关于参数在给定证据或数据上的似然函数

为了估计参数的分布，采用最大后验估计(MAP)的方法
$$
\theta_{MAP}=\arg\max_\theta p(\theta|X)=\arg\max_\theta P(X|\theta)p(\theta)
$$
然后就可以计算后验概率分布$p(\theta|X)$。根据后验分布就可以进行贝叶斯估计来预测新样本
#### 共轭分布
在贝叶斯估计中，如果先验概率分布$p(\theta)$与后验概率分布$p(\theta|X)$是同一类分布，那么称先验分布和后验分布为共轭分布
## 协方差(covariance)
### 协方差
  * 概念:协方差在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。简单来讲，协方差就是衡量两个变量相关性的变量。当协方差为正时，两个变量呈正相关关系（同增同减）；当协方差为负时，两个变量呈负相关关系（一增一减）。而协方差矩阵是将所有变量的协方差关系用矩阵的形式表现出来，通过矩阵这一工具，可以更方便地进行数学运算。
  * 数学定义：设有两个一维随机变量$X=\{x_1,x_2,...x_n\},Y=\{y_1,y_2,...y_n\}$，则X和Y的协方差为
  $$
  Cov(X,Y)={\sum\limits_{i=1}^n(x_i-\overline{x})(y_i-\overline{y})\over n}
  $$
  * 性质：设$X,Y$的期望和方差分别为$E(X),Var(X)$和$E(Y),Var(Y)$
    1. $Cov(X,Y)=Cov(Y,X)$
    2. $Cov(X,X)=Var(X)$
    3. $Cov(aX,bY)=abCov(X,Y)\quad a,b为常数$
    4. $Cov(X_1+X_2,Y)=Cov(X_1,Y)+Cov(X_2,Y)$
    5. 根据定义自然推导
       $$
       \begin{aligned}
       Cov(X,Y)&=E[(X-E(X))(Y-E(Y))]\\
       &=E(XY)-2E(X)(Y)+E(X)E(Y)\\
       &=E(XY)-E(X)E(Y)
       \end{aligned}
       $$
    6. 如果$X$和$Y$相互独立，则$Cov(X,Y)=0$。反之不一定成立。
    7. $Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)$
    8. $Var(X-Y)=Var(X)+Var(Y)-2Cov(X,Y)$
### 协方差矩阵(covariance matrix)
  * 概念：协方差只能解决二维的问题，扩展到多维就形成了协方差矩阵的概念。协方差矩阵的每个元素是各个向量元素之间的协方差，是从标量随机变量到高维度随机向量的自然推广。
  * 数学定义：设矩阵$X=(X_1,X_2...X_d)$，其中$X_i$为列向量，代表一个维度，称矩阵
    $$
    \Sigma=(\sigma_{i,j}^2)_{d\times d}=
    \begin{bmatrix}
    \sigma_{11}^2&\sigma_{12}^2&\cdots&\sigma_{1d}^2 \\
    \sigma_{21}^2&\sigma_{22}^2&\cdots&\sigma_{2d}^2 \\
    \vdots&\vdots&\ddots&\vdots \\
    \sigma_{d1}^2&\sigma_{d2}^2&\cdots&\sigma_{dd}^2 \\
    \end{bmatrix}
    $$
    为$X$的协方差矩阵，也记为$D(X)$，其中
    $$
    \sigma_{ij}^2=Cov(X_i,X_j)\qquad i,j=1,2,3\ldots,d
    $$
  * 设矩阵$X=(X_1,X_2...X_N)^T_{N\times d}$，其中$X_i$为列向量，代表一个样本，那么可以将协方差矩阵写为如下形式
    $$
    \Sigma={1\over N}\sum_{i=1}^N(X_i-\mu)(X_i-\mu)^T=E([X-E(X)]^T[X-E(X)])
    $$
    其中$\mu=(\mu_1,\mu_2,\cdots,\mu_d)^T$为均值向量
  * 性质：
    1. $\Sigma^T=\Sigma$
    2. $\Sigma$为半正定矩阵
    3. 若矩阵$X$中所有n维随机变量两两相互独立，则其协方差矩阵为对角矩阵
### 中心矩阵
设矩阵$X=(X_1,X_2...X_N)^T_{N\times d}$，其中$X_i$为列向量，代表一个样本
令$E$为元素均为1的列向量，即$E=(1,1,\cdots,1)^T$，那么对于协方差矩阵有
 $$
 \begin{aligned}
 \Sigma&={1\over N}\sum_{i=1}^N(X_i-\mu)(X_i-\mu)^T\\
 &={1\over N}
 \begin{pmatrix}X_1-\mu&X_2-\mu&\cdots&X_N-\mu\end{pmatrix}
 \begin{pmatrix}(X_1-\mu)^T\\(X_1-\mu)^T\\\vdots\\(X_1-\mu)^T\end{pmatrix}\\
 &={1\over N}(X^T-{1\over N}X^TEE^T)(X^T-{1\over N}X^TEE^T)^T\\
 &={1\over N}X^T(I-{1\over N}EE^T)(I-{1\over N}EE^T)^TX
 \end{aligned}
 $$
* 令$H=I-{1\over N}EE^T$，称为中心矩阵，其作用就是将样本都向其中心做偏移，具有如下性质：
  1. $H^T=H$
  2. $H^n=H$

于是最终可以简化为
$$
\Sigma={1\over N}X^THX
$$
## 高斯分布
### 一维高斯分布
设一维随机变量$X=x$服从高斯分布，则其概率密度函数为
  $$
  N(x|\mu,\sigma^2)={1\over \sqrt{2\pi\sigma^2}}\exp[-{1\over 2\sigma^2}(x-\mu)^2]
  $$
  其中，$\mu,\sigma$分别为$X$的均值和标准差
### 多维高斯分布
设d维随机变量$X=(x_1,x_2,\ldots,x_d)^T$，则其概率密度函数为
  $$
  N(X|\mu,\Sigma)={1\over(2\pi)^{d/2}|\Sigma|^{1/2}}\exp[-{1\over 2}(X-\mu)^T\Sigma^{-1}(X-\mu)]
  $$
  其中，$\mu$为$X$的均值向量，$\Sigma$为$X$的协方差矩阵
### 高斯分布推论
设随机变量$X\sim{N(\mu,\Sigma)},X\in\mathbb{R}^p$，随机变量$Y=AX+B,Y\in\mathbb{R}^q$，则有
  $$
  Y\sim{N(A_{q\times p}\mu+B,A\Sigma{A^T})}
  $$
## 极大似然估计(Maximum Likelihood Estimate)
  * 概念：极大似然估计，通俗理解来说，就是利用已知的样本结果信息，反推最具有最大可能（最大概率）导致这些样本结果出现的模型参数值。极大似然估计中采样需满足一个重要的假设，就是所有的采样都是独立同分布的
  * 似然函数：设有一个样本$X$符合参数为$\theta$的分布，则其似然函数为$P(X|\theta)$。这里样本$X$是确定的，参数$\theta$为变量，它描述对于不同的模型参数，出现样本$X$这种样本的概率是多少
  * 极大似然估计参数
    $$
    \hat{\theta}=\arg\max_{\theta}P(X|\theta)
    $$
    实际应用中可能会对$P(X|\theta)$取对数以简化计算，即
    $$
    \hat{\theta}=\arg\max_{\theta}\log{P(X|\theta)}
    $$
## 熵
### 信息量
在信息论中，若一个事件发生的概率为$p(x)$，那么这个时间含的信息量为$-\log p(x)$
### 信息熵
信息量度量的是一个具体事件发生了所带来的信息，而熵则是在结果出来之前对可能产生的信息量的期望——考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望。即
$$
H(X)=-\sum_{i=1}^Np(x_i)\log p(x_i)
$$
### 条件熵
定义为X给定条件下，Y的条件概率分布的熵对X的数学期望
设$X,Y$的联合概率分布为$p(x,y)$，则根据定义随机变量$X$给定的条件下随机变量$Y$的条件熵为
$$
\begin{aligned}
H(Y|X)&=-\sum_xp(x)\sum_yp(y|x)\log p(y|x)\\
&=-\sum_x\sum_yp(x,y)\log p(y|x)
\end{aligned}
$$
条件熵表示在已知随机变量$X$的条件下随机变量$Y$的不确定性
### 相对熵
相对熵又称为KL散度。设有关于样本的两个概率分布$p(x),q(x)$，则$p(x)$对$q(x)$的相对熵定义为
$$
\begin{aligned}
KL(p||q)&=\sum_xp(x)\log\frac{p(x)}{q(x)}\\
&=\left[-\sum_xp(x)\log q(x)\right]-\left[-\sum_xp(x)\log p(x)\right]
\end{aligned}
$$
可以看出相对熵可以看做用$q(x)$去模拟$p(x)$时，模拟分布的信息熵与真实信息熵的差值，因此相对熵可以衡量两个分布之间的不相似性，即两个分布越相似，相对熵越小，否则越大
性质：
1. 不对称性：$KL(p||q)\ne KL(q||p)$
2. 非负性：$KL(p||q)\ge0$
### 交叉熵
设有关于样本的两个概率分布$p(x),q(x)$，$p(x)$对$q(x)$交叉熵为
$$
\begin{aligned}
H(p,q)&=\sum_xp(x)\log q(x)
\end{aligned}
$$
显然交叉熵就是用$q(x)$去模拟$p(x)$时的信息熵
根据相对熵公式有
$$
KL(p||q)=H(p,q)-H(p)
$$
即相对熵为用$q(x)$去模拟$p(x)$时，交叉熵与实际熵的差值。也就是用$q(x)$去模拟$p(x)$时多余的信息量
## 指数族分布
概率密度函数形如以下式子的分布称为指数族分布
$$
p(x|\eta)=h(x)\exp(\eta^T\phi(x)-A(\eta))
$$
其中，$\eta$为自然参数，可以为实数也可以为向量；$\phi(x)$为充分统计量；$A(\eta)$是一个对数配分函数
### 对数配分函数
对于一个概率分布函数来说需要积分为1，$\exp(-A(\eta))$在式子中就起到归一化的作用，保证概率密度函数在随机变量$x$上的积分为1。对指数族分布概率密度函数两边对$x$求积分得
$$
\begin{gathered}
p(x|\eta)=\frac{h(x)\exp(\eta^T\phi(x))}{\exp(A(\eta))}\\
\Rightarrow1=\frac{\int h(x)\exp(\eta^T\phi(x))dx}{\exp(A(\eta))}\\
\Rightarrow A(\eta)=\log\int h(x)\exp(\eta^T\phi(x))dx
\end{gathered}
$$
因此称为对数变分函数
### 充分统计量
若给定样本统计量的值，样本联合密度的条件分布与未知参数无关，则这个统计量为充分统计量
例如对于高斯分布，计算出样本的均值和方差就可以确定样本的高斯分布，那么样本的均值和方差就叫做这个样本的充分统计量
### 性质
1. 对数配分函数和充分统计量的关系
   对$A(\eta)$求一阶导数
   $$
   \begin{aligned}
   A^{'}(\eta)&=\frac{{d\over d\eta}\int h(x)\exp(\eta^T\phi(x))dx}
   {\int h(x)\exp(\eta^T\phi(x))dx}\\
   &=\frac{\int{d\over d\eta}h(x)\exp(\eta^T\phi(x))dx}
   {\exp(A(\eta))}\\
   &=\frac{\int h(x)\exp(\eta^T\phi(x))\phi(x)dx}
   {\exp(A(\eta))}\\
   &=\int h(x)\exp(\eta^T\phi(x)-A(\eta))\phi(x)dx\\
   &=\int p(x|\eta)\phi(x)dx\\
   &=E_{p(x|\eta)}[\phi(x)]
   \end{aligned}
   $$
   对$A(\eta)$求二阶导数
   $$
   \begin{gathered}
   \begin{aligned}
   A^{''}(\eta)&=\int p(x|\eta)\phi(x)dx\\
   &=\int\phi(x){d\over d\eta}p(x|\eta)dx
   \end{aligned}\\
   \\
   \begin{aligned}
   {d\over d\eta}p(x|\eta)&={d\over d\eta}h(x)\exp(\eta^T\phi(x)-A(\eta))\\
   &=h(x)\exp[\eta^T\phi(x)-A(\eta)][\phi(x)-A^{'}(\eta)]\\
   &=h(x)\exp[\eta^T\phi(x)-A(\eta)]\left[\phi(x)-\int p(x|\eta)\phi(x)dx\right]\\
   &=p(x|\eta)\left[\phi(x)-\int p(x|\eta)\phi(x)dx\right]
   \end{aligned}\\
   \\
   \begin{aligned}
   A^{''}(\eta)&=\int\phi(x)\left\{p(x|\eta)\left[\phi(x)-\int p(y|\eta)\phi(y)dy\right]\right\}dx\\
   &=\int p(x|\eta)\phi^2(x)dx-\int\phi(x)p(x|\eta)\int p(y|\eta)\phi(y)dydx\\
   &=E_{p(x|\eta)}[\phi^2(x)]-\int\phi(x)p(x|\eta)dx\int p(y|\eta)\phi(y)dy\\
   &=E_{p(x|\eta)}[\phi^2(x)]-E_{p(x|\eta)}[\phi(x)]E_{p(y|\eta)}[\phi(y)]\\
   &=E_{p(x|\eta)}[\phi^2(x)]-(E_{p(x|\eta)}[\phi(x)])^2\\
   &=Var_{p(x|\eta)}[\phi(x)]\\
   &\ge0
   \end{aligned}
   \end{gathered}
   $$
   因此还可以发现$A(\eta)$是凸函数
2. 极大似然估计与充分统计量
   给定一个符合指数族分布的数据集$D=(X_1,X_2,\cdots,X_N)$，根据极大似然估计有
   $$
   \begin{aligned}
   \eta_{MLE}&=\arg\max_\eta\log P(D|\eta)\\
   &=\arg\max_\eta\log\prod_{i=1}^Np(X_i|\eta)\\
   &=\arg\max_\eta\sum_{i=1}^N\log p(X_i|\eta)\\
   &=\arg\max_\eta\sum_{i=1}^N\log [h(x)\exp(\eta^T\phi(X_i)-A(\eta))]\\
   &=\arg\max_\eta\sum_{i=1}^N[\eta^T\phi(X_i)-A(\eta)]
   \end{aligned}
   $$
   令$L(\eta)=\sum\limits_{i=1}^N[\eta^T\phi(X_i)-A(\eta)]$
   $$
   \begin{gathered}
   \frac{\partial L(\eta)}{\partial\eta}=\sum_{i=1}^N\phi(X_i)-NA^{'}(\eta)=0\\
   \Rightarrow A^{'}(\eta_{MLE})={1\over N}\sum_{i=1}^N\phi(X_i)
   \end{gathered}
   $$
   根据以上计算可以得知，只需要知道$\phi(x)$就可以根据样本计算出$\eta_{MLE}$，也就是说$\phi(x)$就包含了样本的信息，可以用它来估计分布的参数，因此被称为充分统计量
3. 最大熵原理
   假设$X$是离散的且有$k$种取值，即
   $$
   \begin{array}{c|cccc}
   X_i&1&2&\cdots&k\\\hline
   p&p_1&p_2&\cdots&p_k
   \end{array}
   $$
   其中$\sum\limits_{i=1}^kp_i=1$，那么熵为
   $$
   H(p)=-\sum_{i=1}^kp_i\log(p_i)
   $$
   最大熵原理就是说在满足已知事实的情况下，熵最大的分布就是当前的分布。下面推导一下在没有任何约束时熵最大是什么状态，即优化问题
   $$
   \begin{gathered}
   \max_{p}H(p)=-\sum_{i=1}^kp_i\log p_i\\
   s.t.\quad\sum_{i=1}^kp_i=1
   \end{gathered}
   $$
   利用拉格朗日乘子法
   $$
   \begin{gathered}
   L(p,\lambda)=-\sum_{i=1}^kp_i\log p_i+\lambda(1-\sum_{i=1}^kp_i)\\
   \begin{cases}
   \frac{\partial L}{\partial p_i}=-\log p_i-1-\lambda=0\\
   \frac{\partial L}{\partial\lambda}=1-\sum_{i=1}^kp_i=0
   \end{cases}\\
   \Rightarrow p_1=p_2=\cdots=p_k={1\over k}
   \end{gathered}
   $$
   可以看到熵最大原理实际就对应等可能的状态，这是非常符合直观的
   在给定的数据集$D=(X_1,X_2,\cdots,X_N)$，有一个经验分布
   $$
   P(X=x)=\hat{p}(x)=\frac{count(X=x)}{N}
   $$
   $x$就是样本中$X_i$的取值。那么对于任意一个函数向量$f(x)$一定可以求得一个经验期望$E_{\hat{p}(x)}[f(x)]=C$，也就是说这是一个可以由数据集得到的定值，也就是前面提到的已知事实，如果要求样本的真实分布$p(x)$，根据最大熵原理有如下优化问题，这时的约束就应该加上这个已知事实
   $$
   \begin{gathered}
   \max_p-\sum_xp(x)\log p(x)=\min_p\sum_xp(x)\log p(x)\\
   \begin{aligned}
   s.t.\quad&\sum_xp(x)=1\\
   &E_{p(x)}[f(x)]=E_{\hat{p}(x)}[f(x)]=C
   \end{aligned}
   \end{gathered}
   $$
   显然$E_{p(x)}[f(x)]=\sum_xp(x)f(x)$，根据拉格朗日乘子法有
   $$
   \begin{gathered}
   L(p,\lambda_0,\lambda)=\sum_xp(x)\log p(x)+\lambda_0(1-\sum_xp(x))+\lambda^T(C-\sum_xp(x)f(x))\\
   \frac{\partial L}{\partial p(x)}=\log p(x)+1-\lambda_0-\lambda^Tf(x)=0\\
   \\
   \begin{aligned}
   \Rightarrow p(x)&=\exp(\lambda^Tf(x)+\lambda_0-1)\\
   &=h(x)\exp(\eta^T\phi(x)+A(\eta))
   \end{aligned}
   \end{gathered}
   $$
   其中$h(x)=1,\eta=\lambda,\phi(x)=f(x),A(\eta)=\lambda_0-1$，即$p(x)$满足指数族分布
  于是我们得到结论，对于一个数据集，根据最大熵原理它符合一个指数族分布
## Jensen不等式
对于定义在区间$[a,b]$上的凸函数$f(x)$，对任意的$x_1,x_2,\cdots,x_n\in[a,b]$有
$$
f(\sum_{i=1}^n\lambda_ix_i)\le\sum_{i=1}^n\lambda_if(x_i)
$$
其中$\lambda_i\ge0$且$\sum\limits_{i=1}^n\lambda_i=1$。注意这里的凸函数与国内的凸函数定义是相反的
上述为离散形式的Jensen不等式，推广到连续形式为
$$
f\left(\int xp(x)dx\right)\le\int f(x)p(x)dx
$$
其中$p(x)\ge0$且$\int p(x)dx=1$
在离散形式中可以将$\lambda_i$看作离散随机变量$x_i$的概率分布，在连续形式中可以将$p(x)$看作连续随机变量$x$的概率分布，那么即可写为
$$
f(E[x])\le E[f(x)]
$$
等号在$E[x]=x$时成立，即$x$为常数时成立
对于凹函数则不等号正好相反